{"meta":{"title":"求索","subtitle":"尔不必求记，却宜求个明白！","description":"","author":"Mingxu.xie","url":"https://blog.xiemx.com","root":"/"},"pages":[{"title":"commands","date":"2019-09-26T09:58:47.000Z","updated":"2019-09-27T02:30:53.818Z","comments":true,"path":"commands.html","permalink":"https://blog.xiemx.com/commands.html","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108#获取所有region的机器数量for i in $(aws ec2 describe-regions | grep RegionName| awk -F\"[\\\"]+\" '&#123;print $4&#125;');do echo -n \"$i: \"; AWS_PROFILE=jp aws--region=$i ec2 describe-instances|grep InstanceId|wc -l;done;#获取节点的机器tag.name和privateIPaws ec2 describe-instances --filters \"Name=tag:Service,Values=eos\" --query\"Reservations[*].Instances[*].[PrivateIpAddress,Tags[?Key=='Name'].Value]\" --output text#simple http serverpython -m SimperHTTPServer 12345#ssh端口转发ssh -v -N -L 0.0.0.0:60123:192.168.10.195:60123 203.175.165.66#查看rsa私钥的公钥ssh-keygen -y -f id_rsa &gt; id_rsa.pub#wget 断点续传wget -c -t 0 -O backup_2018_01_01_050000_9591273.bak http://47.88.192.116:12312/backup_2018_01_01_050000_9591273.bak#rsync+ssh proxyrsync -av -P -e \"ssh -o 'ProxyCommand ssh 123.206.197.235 nc %h %p'\" 2018-03-06_03-05.sql.gz 10.86.48.8:/data/backup#consul-template/srv/consul/consul-template -consul 192.168.10.123:8500 -template/srv/consul/my.ctmpl:/srv/consul/conf.d/my.conf:/etc/init.d/nginx reload -once#iptables forwardiptables -t nat -A PREROUTING -d 192.168.10.226 -p tcp -dport 6386 -j DNAT --to-destination 192.168.10.81:6386iptables -t nat -I POSTROUTING -j MASQUERADE#远程抓包在本地调用wireshark工具打开ssh -o 'ProxyCommand ssh 123.206.197.235 nc %h %p' 192.168.199.63 \"sudo tcpdump -vv -i eth0 port 80 -w -\" | wireshark -k-i -#git diff对比commit差异，只列出文件名git diff --name-status 128e72a28ea38d1c8691a22a28937356e7adf736^ 128e72a28ea38d1c8691a22a28937356e7adf736#docker log-opt--log-driver syslog --log-opt tag=\"eos-producer\"--log-driver json-file --log-opt max-size=500msudo docker run --log-driver=awslogs --log-opt awslogs-region=ap-southeast-1 --log-opt awslogs-group=eos-log --log-optawslogs-datetime-format='\\[%b %d, %Y %H:%M:%S\\]' -d -p 8898:80 nginxdocker run -d -p 9876:9876 -p 80:8888 --name eos --log-driver gelf --log-opt gelf-address=udp://10.188.10.145:15155 -v/data/eos/:/eos eoslaomao/eos:1.3.0 /opt/eosio/bin/nodeos --data-dir=/eos/data --config-dir=/eos/confdocker run -d -p 9876:9876 -p 8888:8888 --name eos --log-driver json-file --log-opt max-size=1g -v /data/eos/:/eosgcr.io/eosasia-testnet/eos:v1.5.3-patch /opt/eosio/bin/nodeos --data-dir=/eos/data --config-dir=/eos/conf#pigz多线程打包apt install pigztar --use-compress-program=pigz -cvpf package.tgz ./packagetar --use-compress-program=pigz -xvpf package.tgz -C ./package#aws elb reg/unreg instanceaws elbv2 register-targets--target-group-arn=arn:aws:elasticloadbalancing:ap-northeast-1:908572518:targetgroup/e-api-1/65c430c260d1556e --targetsId=i-00b7f498a742cdaws elbv2 deregister-targets--target-group-arn=arn:aws:elasticloadbalancing:ap-northeast-1:908572518:targetgroup/e-api-1/65c430c260d1556e --targetsId=i-00b7f498a742cd#pmmdocker run -d \\-p 80:81 \\--volumes-from pmm-data \\--name pmm-server \\--restart always \\percona/pmm-server:latest#git 使用指定的private key cloneGIT_SSH_COMMAND=\"ssh -i ~/.ssh/id_rsa\" git clone git@cd.i.ly.com:ly/cat.git#K8S 端口转发kubectl -n test port-forward $(kubectl -n test get endpoints prometheus -ojsonpath='&#123;.subsets[0].addresses[0].targetRef.name&#125;') 9090:9090#jp 处理数据jq '.members[]|&#123;\"id\": .id, \"Name\": .fullName&#125;' trello.jsonjq '.cards[]|if .idList == \"5c64e37e57ba1c20f62751ce\" then .idMembers else .xxx end' trello.json#获取websocket返回信息s = new WebSocket('wss://www.staging.strikingly.com/livechat-ws')s.onclose = (e) =&gt; &#123; console.log(e)&#125;# 获取postgresql schemeselect * from information_schema.schemata;#查看表字段 - postgresqlSELECT a.attname as name, a.attnotnull as notnullFROM pg_class as c,pg_attribute as awhere c.relname = 'mini_program_accounts' and a.attrelid = c.oid and a.attnum&gt;0#postgresql dump databasepg_dump -U vi -d vi -h rm-j.pg.rds.aliyuncs.com -p 3433 --no-owner --no-privileges -f vi.sql#mysqldumpmysqldump --host rm-j.mysql.rds.aliyuncs.com -u vi -p --databases via --column-statistics=0 &gt; vi.sql# 获取当前会话总数select count(*) from pg_stat_activity;# 杀死所有idle的进程select pg_terminate_backend(pid) from pg_stat_activity where state='idle';"},{"title":"分类","date":"2019-09-27T06:54:52.021Z","updated":"2019-09-27T06:54:52.017Z","comments":false,"path":"categories/index.html","permalink":"https://blog.xiemx.com/categories/index.html","excerpt":"","text":""},{"title":"About","date":"2019-09-26T09:34:08.338Z","updated":"2019-09-26T09:34:08.338Z","comments":true,"path":"about/index.html","permalink":"https://blog.xiemx.com/about/index.html","excerpt":"","text":"asdfsadf"},{"title":"Repositories","date":"2019-09-27T06:55:01.568Z","updated":"2019-09-27T06:55:01.567Z","comments":false,"path":"repository/index.html","permalink":"https://blog.xiemx.com/repository/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2019-09-27T06:55:16.106Z","updated":"2019-09-27T06:55:16.105Z","comments":true,"path":"links/index.html","permalink":"https://blog.xiemx.com/links/index.html","excerpt":"","text":""},{"title":"Tags","date":"2019-09-26T09:33:06.254Z","updated":"2019-09-26T09:33:06.254Z","comments":true,"path":"tags/index.html","permalink":"https://blog.xiemx.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"k8s ingress-nginx动态balance实现解析","slug":"2019-09-16-k8s-ingress-nginx","date":"2019-09-15T23:09:52.000Z","updated":"2019-10-19T15:30:13.452Z","comments":false,"path":"/2019/09/16/2019-09-16-k8s-ingress-nginx/","link":"","permalink":"https://blog.xiemx.com/2019/09/16/2019-09-16-k8s-ingress-nginx/","excerpt":"","text":"只节选了比较关键的代码，删除了比较多的干扰项。纯属个人理解！！！ 1. 初始化balancer.init_worker()，使用balancer.balance()动态获取123456789101112131415161718192021http &#123; lua_package_path \"/etc/nginx/lua/?.lua;/etc/nginx/lua/vendor/?.lua;/usr/local/lib/lua/?.lua;;\"; init_by_lua_block &#123; ok, res = pcall(require, \"configuration\") &#125; init_worker_by_lua_block &#123; balancer.init_worker() #####创建定时任务 ngx.timer.every(BACKENDS_SYNC_INTERVAL, sync_backends) &#125; ###upstream configureupstream upstream_balancer &#123; server 0.0.0.1; # placeholder balancer_by_lua_block &#123; balancer.balance() &#125; keepalive 32; keepalive_timeout 60s; keepalive_requests 100; &#125; 2. 获取backend信息，balancer.init_worker()12345678910111213141516171819202122232425####https://sourcegraph.com/github.com/kubernetes/ingress-nginx@dd0fe4b458cc5520f25eb8bba25bbe6f0c72ee98/-/blob/rootfs/etc/nginx/lua/balancer.lua?utm_source=share#L223local configuration = require(\"configuration\")function _M.init_worker() sync_backends() -- when worker starts, sync backends without delay local _, err = ngx.timer.every(BACKENDS_SYNC_INTERVAL, sync_backends) if err then ngx.log(ngx.ERR, string.format(\"error when setting up timer.every for sync_backends: %s\", tostring(err))) endendlocal function sync_backends() local backends_data = configuration.get_backends_data() local new_backends, err = cjson.decode(backends_data) local balancers_to_keep = &#123;&#125; for _, new_backend in ipairs(new_backends) do sync_backend(new_backend) balancers_to_keep[new_backend.name] = balancers[new_backend.name] endend 1234567891011121314151617181920212223242526272829303132333435363738394041424344####https://sourcegraph.com/github.com/kubernetes/ingress-nginx@dd0fe4b458cc5520f25eb8bba25bbe6f0c72ee98/-/blob/rootfs/etc/nginx/lua/configuration.lua?utm_source=share#L10:10local configuration_data = ngx.shared.configuration_datalocal certificate_data = ngx.shared.certificate_datalocal _M = &#123;&#125;function _M.get_backends_data() return configuration_data:get(\"backends\")endfunction _M.call() if ngx.var.request_method ~= \"POST\" and ngx.var.request_method ~= \"GET\" then ngx.status = ngx.HTTP_BAD_REQUEST ngx.print(\"Only POST and GET requests are allowed!\") return end if ngx.var.request_uri == \"/configuration/servers\" then handle_servers() return end if ngx.var.request_uri == \"/configuration/general\" then handle_general() return end if ngx.var.uri == \"/configuration/certs\" then handle_certs() return end if ngx.var.request_uri ~= \"/configuration/backends\" then ####只接受以上4类型URL ngx.status = ngx.HTTP_NOT_FOUND ngx.print(\"Not found!\") return end local backends = fetch_request_body() local success, err = configuration_data:set(\"backends\", backends)end 12345678910111213141516171819### fetch_request_body()，从此函数可以看出此函数是一个外部调用，可以得出原始的数据来源为外部触发的POST，可以查询Call()函数的引用位置local function fetch_request_body() ngx.req.read_body() ###防止ngx.req.get_body_data()返回nil,显示执行一下 local body = ngx.req.get_body_data() if not body then local file_name = ngx.req.get_body_file() ###读取cache file local file = io.open(file_name, \"rb\") if not file then return nil end body = file:read(\"*all\") file:close() end return bodyend 1234567891011121314151617181920####nginx.conf 查看nginx配置文件中显示调用call()函数的位置为当前server切url /configuration 符合函数要求，在查找外部调用的代码（基本可以定位为控制器的逻辑控制） server &#123; listen unix:/tmp/nginx-status-server.sock; set $proxy_upstream_name \"internal\"; keepalive_timeout 0; gzip off; access_log off; location /configuration &#123; # this should be equals to configuration_data dict client_max_body_size 10m; client_body_buffer_size 10m; proxy_buffering off; content_by_lua_block &#123; configuration.call() &#125; &#125; 3. 通过上面的信息检索，在Ingress中监听pod变化信息，动态调用/configuration/backends, 函数为configureBackends()12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061####https://github.com/kubernetes/ingress-nginx/blob/ce3e3d51c397ff6a0cd6731cc64360ecdb69ea54/internal/ingress/controller/nginx.go#L982func configureBackends(rawBackends []*ingress.Backend) error &#123; backends := make([]*ingress.Backend, len(rawBackends)) for i, backend := range rawBackends &#123; var service *apiv1.Service if backend.Service != nil &#123; service = &amp;apiv1.Service&#123;Spec: backend.Service.Spec&#125; &#125; luaBackend := &amp;ingress.Backend&#123; Name: backend.Name, Port: backend.Port, SSLPassthrough: backend.SSLPassthrough, SessionAffinity: backend.SessionAffinity, UpstreamHashBy: backend.UpstreamHashBy, LoadBalancing: backend.LoadBalancing, Service: service, NoServer: backend.NoServer, TrafficShapingPolicy: backend.TrafficShapingPolicy, AlternativeBackends: backend.AlternativeBackends, &#125; var endpoints []ingress.Endpoint for _, endpoint := range backend.Endpoints &#123; endpoints = append(endpoints, ingress.Endpoint&#123; Address: endpoint.Address, Port: endpoint.Port, &#125;) &#125; luaBackend.Endpoints = endpoints backends[i] = luaBackend &#125; statusCode, _, err := nginx.NewPostStatusRequest(\"/configuration/backends\", \"application/json\", backends) ####backends 为request.body,却内容为IP/PORT，以下给出了backend的struct if err != nil &#123; return err &#125; if statusCode != http.StatusCreated &#123; return fmt.Errorf(\"unexpected error code: %d\", statusCode) &#125; return nil&#125;####Backend structtype Backend struct &#123; Name string `json:\"name\"` Service *apiv1.Service `json:\"service,omitempty\"` Port intstr.IntOrString `json:\"port\"` SecureCACert resolver.AuthSSLCert `json:\"secureCACert\"` SSLPassthrough bool `json:\"sslPassthrough\"` Endpoints []Endpoint `json:\"endpoints,omitempty\"` SessionAffinity SessionAffinityConfig `json:\"sessionAffinityConfig\"` UpstreamHashBy UpstreamHashByConfig `json:\"upstreamHashByConfig,omitempty\"` LoadBalancing string `json:\"load-balance,omitempty\"` NoServer bool `json:\"noServer\"` TrafficShapingPolicy TrafficShapingPolicy `json:\"trafficShapingPolicy,omitempty\"` AlternativeBackends []string `json:\"alternativeBackends,omitempty\"`&#125; 4. ngx_balancer.set_current_peer()设置backend信息12345678910111213141516171819202122232425262728293031323334353637383940414243444546####https://sourcegraph.com/github.com/kubernetes/ingress-nginx@dd0fe4b458cc5520f25eb8bba25bbe6f0c72ee98/-/blob/rootfs/etc/nginx/lua/balancer.lua?utm_source=share#L232:13function _M.balance() local balancer = get_balancer() if not balancer then return end local peer = balancer:balance() if not peer then ngx.log(ngx.WARN, \"no peer was returned, balancer: \" .. balancer.name) return end ngx_balancer.set_more_tries(1) local ok, err = ngx_balancer.set_current_peer(peer) ####设置server信息 if not ok then ngx.log(ngx.ERR, string.format(\"error while setting current upstream peer %s: %s\", peer, err)) endendlocal function get_balancer() if ngx.ctx.balancer then return ngx.ctx.balancer end local backend_name = ngx.var.proxy_upstream_name ###获取当前request上下文中共享的变量proxy_upstream_name local balancer = balancers[backend_name] ###获取balancers信息由sync_backend()函数定时轮询 if not balancer then return end if route_to_alternative_balancer(balancer) then local alternative_backend_name = balancer.alternative_backends[1] ngx.var.proxy_alternative_upstream_name = alternative_backend_name balancer = balancers[alternative_backend_name] end ngx.ctx.balancer = balancer return balancerend 12345###nginx.confset $proxy_upstream_name \"dev-dev-auto-deploy-5000\";set $proxy_host $proxy_upstream_name;proxy_pass http://upstream_balancer;","categories":[{"name":"k8s","slug":"k8s","permalink":"https://blog.xiemx.com/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://blog.xiemx.com/tags/k8s/"},{"name":"ingress","slug":"ingress","permalink":"https://blog.xiemx.com/tags/ingress/"}],"author":"xiemx"},{"title":"PostgreSQL查看复制状态","slug":"2019-07-08-postgresql-replica-status","date":"2019-07-08T03:07:36.000Z","updated":"2019-10-19T09:40:34.388Z","comments":true,"path":"/2019/07/08/2019-07-08-postgresql-replica-status/","link":"","permalink":"https://blog.xiemx.com/2019/07/08/2019-07-08-postgresql-replica-status/","excerpt":"","text":"postgresql查看复制状态，master上执行1234567891011121314151617181920212223242526272829#select * from pg_stat_replication; postgres=# select * from pg_stat_replication;-[ RECORD 1 ]----+------------------------------pid | 13321usesysid | 17019usename | replicationapplication_name | walreceiverclient_addr | 10.0.0.81client_hostname | client_port | 42809backend_start | 2016-08-11 10:57:35.856289+08backend_xmin | state | streaming --同步状态sent_location | 1/E0CE9750write_location | 1/E0CE9750flush_location | 1/E0CE9750replay_location | 1/E0CE9750sync_priority | 0sync_state | async --同步模式state: 同步状态 streaming : 同步 startup : 连接中 catchup: 同步中sync_state: 同步模式. async : 异步 sync : 同步 potential: 虽然现在是异步,但有可能提升到同步 查看复制的延迟有多少，字节单位，master上执行1234567#select pg_xlog_location_diff(sent_location, replay_location) from pg_stat_replication; posrgresql=# select pg_xlog_location_diff(sent_location, replay_location) from pg_stat_replication; pg_xlog_location_diff ----------------------- 0(1 row) slave上查看sql滞后时间12345678910111213SELECT CASE WHEN pg_last_xlog_receive_location() = pg_last_xlog_replay_location() THEN 0 ELSE EXTRACT (EPOCH FROM now() - pg_last_xact_replay_timestamp()) END AS log_delay;postgres=# SELECT CASE WHEN pg_last_xlog_receive_location() = pg_last_xlog_replay_location()postgres-# THEN 0postgres-# ELSE EXTRACT (EPOCH FROM now() - pg_last_xact_replay_timestamp())postgres-# END AS log_delay; log_delay----------- 0(1 row) slave上查看是否处于recovery模式123456select pg_is_in_recovery();postgres=# select pg_is_in_recovery(); pg_is_in_recovery------------------- t(1 row) slave上查看最新的reploy时间戳123456#select pg_last_xact_replay_timestamp();postgres=# select pg_last_xact_replay_timestamp(); pg_last_xact_replay_timestamp------------------------------- 2019-07-08 03:01:33.854131+00(1 row) slave上查看最新的reploy位置123456#select pg_last_xlog_replay_location();postgres=# select pg_last_xlog_replay_location(); pg_last_xlog_replay_location------------------------------ 220C/56EB4C10(1 row)","categories":[{"name":"postgresql","slug":"postgresql","permalink":"https://blog.xiemx.com/categories/postgresql/"}],"tags":[{"name":"database","slug":"database","permalink":"https://blog.xiemx.com/tags/database/"},{"name":"postgresql","slug":"postgresql","permalink":"https://blog.xiemx.com/tags/postgresql/"}],"author":"xiemx"},{"title":"http cache","slug":"2019-05-13-http-cache","date":"2019-05-13T03:05:02.000Z","updated":"2019-10-19T15:32:45.353Z","comments":false,"path":"/2019/05/13/2019-05-13-http-cache/","link":"","permalink":"https://blog.xiemx.com/2019/05/13/2019-05-13-http-cache/","excerpt":"","text":"cache流程图 “no-cache”和“no-store”“no-cache”表示必须先与服务器确认返回的响应是否发生了变化，然后才能使用该响应来满足后续对同一网址的请求。 因此，如果存在合适的验证令牌 (ETag)，no-cache 会发起往返通信来验证缓存的响应，但如果资源未发生变化，则可避免下载。 “no-store”禁止浏览器以及所有中间缓存存储任何版本的返回响应，例如，包含个人隐私数据或银行业务数据的响应。 每次用户请求该资产时，都会向服务器发送请求，并下载完整的响应。 “public”与 “private”“public”则即使它有关联的 HTTP 身份验证，甚至响应状态代码通常无法缓存，也可以缓存响应。 大多数情况下，“public”不是必需的，因为明确的缓存信息（例如“max-age”）已表示响应是可以缓存的。 “private”浏览器可以缓存响应，不允许任何中间缓存对其进行缓存。 例如，用户的浏览器可以缓存包含用户私人信息的 HTML 网页，但 CDN 却不能缓存。 “max-age”指令指定从请求的时间开始，允许提取的响应被重用的最长时间（单位：秒）。 例如，“max-age=60”表示可在接下来的 60 秒缓存和重用响应。 通过 ETag 验证缓存的响应在首次请求资源时服务器生成并返回”ETag” http请求头(通常是文件内容的哈希值或某个其他指纹)。 当120 秒后，浏览器又对该资源发起了新的请求。 首先，浏览器会检查本地缓存并找到之前的响应。如果发现缓存超过max-age, 浏览器将发起一个带有”If-None-Match”的http请求。 如果Etag相同，则返回304，使用本地缓存。 参考： https://developers.google.com/web/fundamentals/performance/optimizing-content-efficiency/http-caching","categories":[{"name":"http","slug":"http","permalink":"https://blog.xiemx.com/categories/http/"}],"tags":[{"name":"http","slug":"http","permalink":"https://blog.xiemx.com/tags/http/"},{"name":"cache","slug":"cache","permalink":"https://blog.xiemx.com/tags/cache/"}],"author":"xiemx"},{"title":"Mysqldump error","slug":"2019-05-06-mysqldump-error","date":"2019-05-06T03:05:56.000Z","updated":"2019-10-19T10:22:28.240Z","comments":true,"path":"/2019/05/06/2019-05-06-mysqldump-error/","link":"","permalink":"https://blog.xiemx.com/2019/05/06/2019-05-06-mysqldump-error/","excerpt":"","text":"现象1234[root@FCHK-instance ~]# mysqldump --host rm-xxxxxxxxxxx.mysql.rds.aliyuncs.com -u xxxx -p --databases visa &gt; hk.sqlEnter password:Warning: A partial dump from a server that has GTIDs will by default include the GTIDs of all transactions, even those that changed suppressed parts of the database. If you don't want to restore GTIDs, pass --set-gtid-purged=OFF. To make a complete dump, pass --all-databases --triggers --routines --events.mysqldump: Couldn't execute 'SELECT COLUMN_NAME, JSON_EXTRACT(HISTOGRAM, '$.\"number-of-buckets-specified\"') FROM information_schema.COLUMN_STATISTICS WHERE SCHEMA_NAME = 'visa' AND TABLE_NAME = 'admin';': Unknown table 'column_statistics' in information_schema (1109 分析123456789[root@FCHK-instance ~]# mysql --versionmysql Ver 8.0.11 for Linux on x86_64 (MySQL Community Server - GPL)可能是由于mysqldump 8中默认启用（COLUMN_STATISTICS）官方文档解释Mysql 8.0 The INFORMATION_SCHEMA COLUMN_STATISTICS Tablehttps://dev.mysql.com/doc/refman/8.0/en/column-statistics-table.html 解决123[root@FCHK-instance ~]# mysqldump --host rm-xxxxxxxxxx2.mysql.rds.aliyuncs.com -u xxxx -p --databases visa --column-statistics=0 &gt; hk.sqlEnter password:Warning: A partial dump from a server that has GTIDs will by default include the GTIDs of all transactions, even those that changed suppressed parts of the database. If you don't want to restore GTIDs, pass --set-gtid-purged=OFF. To make a complete dump, pass --all-databases --triggers --routines --events.","categories":[{"name":"mysql","slug":"mysql","permalink":"https://blog.xiemx.com/categories/mysql/"}],"tags":[{"name":"database","slug":"database","permalink":"https://blog.xiemx.com/tags/database/"},{"name":"debug","slug":"debug","permalink":"https://blog.xiemx.com/tags/debug/"},{"name":"mysql","slug":"mysql","permalink":"https://blog.xiemx.com/tags/mysql/"}],"author":"xiemx"},{"title":"redis-dump使用","slug":"2019-05-06-redis-dump","date":"2019-05-06T03:05:33.000Z","updated":"2019-10-19T09:09:04.060Z","comments":true,"path":"/2019/05/06/2019-05-06-redis-dump/","link":"","permalink":"https://blog.xiemx.com/2019/05/06/2019-05-06-redis-dump/","excerpt":"","text":"安装redis-dump/redis-load 12#需要ruby 2.2.2以上版本（可以直接使用ruby:2.2.3的docker images）gem install redis-dum dump redis 12345678910111213141516root@5dba1bd8fa77:/# redis-dump -h Try: /usr/local/bundle/bin/redis-dump show-commandsUsage: /usr/local/bundle/bin/redis-dump [global options] COMMAND [command options] -u, --uri=S Redis URI (e.g. redis://hostname[:port]) -d, --database=S Redis database (e.g. -d 15) -a, --password=S Redis password (e.g. -a 'my@pass/word') -s, --sleep=S Sleep for S seconds after dumping (for debugging) -c, --count=S Chunk size (default: 10000) -f, --filter=S Filter selected keys (passed directly to redis' KEYS command) -b, --base64 Encode key values as base64 (useful for binary values) -O, --without_optimizations Disable run time optimizations -V, --version Display version -D, --debug --nosaferoot@5dba1bd8fa77:/# redis-dump -u aux-redis.1uvkyf.0001.cnn1.cache.amazonaws.com.cn &gt; redis-uat.json load redis 123456789101112131415root@5dba1bd8fa77:/# redis-load -h Try: /usr/local/bundle/bin/redis-load show-commandsUsage: /usr/local/bundle/bin/redis-load [global options] COMMAND [command options] -u, --uri=S Redis URI (e.g. redis://hostname[:port]) -d, --database=S Redis database (e.g. -d 15) -a, --password=S Redis password (e.g. -a 'my@pass/word') -s, --sleep=S Sleep for S seconds after dumping (for debugging) -b, --base64 Decode key values from base64 (used with redis-dump -b) -n, --no_check_utf8 -V, --version Display version -D, --debug --nosaferoot@5dba1bd8fa77:/# redis-load -u aux-redis.1uvkyf.0001.cnn1.cache.amazonaws.com.cn:6379/0 &lt; redis-uat.json","categories":[{"name":"redis","slug":"redis","permalink":"https://blog.xiemx.com/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://blog.xiemx.com/tags/redis/"}],"author":"xiemx"},{"title":"SSH的AuthorizedKeysCommand、AuthorizedKeysCommandUser","slug":"2019-05-06-ssh-authorizedkeyscommand","date":"2019-05-06T03:05:17.000Z","updated":"2019-10-19T07:49:11.518Z","comments":false,"path":"/2019/05/06/2019-05-06-ssh-authorizedkeyscommand/","link":"","permalink":"https://blog.xiemx.com/2019/05/06/2019-05-06-ssh-authorizedkeyscommand/","excerpt":"","text":"AuthorizedKeysCommand 可以指定运行一个脚本，而这个脚本主要是寻找登录用户的publickey，默认传参为登录用户名，若未认证成功，将继续使用AuthorizedKeysFile文件来做认证。 AuthorizedKeysCommandUser就是指定以什么用户来运行这个脚本。 这两个配置选项的一个用处就是在用户管理上可以不再依靠本地管理，而可以通过脚本读取远程数据库系统中的用户的publickey进行认证，例如MySQL或者LDAP，这样的话，更便于用户的集中管理。","categories":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/tags/linux/"},{"name":"ssh","slug":"ssh","permalink":"https://blog.xiemx.com/tags/ssh/"}],"author":"xiemx"},{"title":"SLB 502报错Debug","slug":"2018-07-20-slb-502-debug","date":"2018-07-19T18:07:06.000Z","updated":"2019-10-19T08:11:47.770Z","comments":true,"path":"/2018/07/20/2018-07-20-slb-502-debug/","link":"","permalink":"https://blog.xiemx.com/2018/07/20/2018-07-20-slb-502-debug/","excerpt":"","text":"用户自定义站点502问题分析 现象：自定义域名用户反馈，打开网站返回502，如图 根据response header判断，请求到达captain，怀疑captain返回的502页面。查看nginx proxy_pass得知后端的地址为bobcat.sxldns.com. 12set $bobcat_backend \"bobcat.sxldns.com\";proxy_pass http://$bobcat_backend; 使用curl模拟请求，直接请求bobcat.sxldns.com,正常获得返回内容，具体针对每个服务器的IP的curl,不再单独列出。 通过上述返回基本判断，问题出在我们的代理层。具体查看代理层的nginx配置和系统资源利用率。 查看当时系统的资源状态,查看到当时的系统磁盘空间使用完。查看nginx上有关于proxy的cache相关配置,nginx会cache的response的content的内容。怀疑nginx转发请求之后但是backend返回内容后，nginx cache到本地的时候无法写入disk导致会话结束，SLB的请求无返回包认为后端宕机抛出502。 1proxy_cache_path /etc/nginx/china_cache levels=1:2 keys_zone=user_page_cache:100m max_size=20g inactive=60m use_temp_path=off; 具体DEBUG如下： 获取container中的nginx worker进程的PID strace查看下系统调用具体信息 通过上图strace追踪流程可以看到左侧是一个正常的请求的全部流程，右侧是故障状态的strace的系统调用全流程。 通过对比左右两侧的系统调用流程可以看到，当nginx cache的时候写入/etc/nginx/china_cache目录时提示no space后续的writev()和sendfile()方法就没有调用，因此导致SLB无法获得返回包，抛出badgateway的错误. PS： 为什么单独请求返回头的时候正常返回？nginx返回请求头的时候并不会走proxy的cache流程。因此没有调用open()方法读写disk，正常返回，但是实际请求数据的时候cache写磁盘直接失败，后续直接退出。 上图的系统调用过程为了debug的方便使用了非折叠模式。因此一些no space的一些报错未显示出来可以。全部流程见附录。 附录： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031.异常调用堆栈[root@iZ2ze2mzhjk3ou1vsnkthzZ rpm]# strace -p 3904 -vstrace: Process 3904 attachedepoll_wait(8, [&#123;EPOLLIN, &#123;u32=69025808, u64=140003117973520&#125;&#125;], 512, -1) = 1accept4(6, &#123;sa_family=AF_INET, sin_port=htons(59464), sin_addr=inet_addr(\"10.130.0.4\")&#125;, [16], SOCK_NONBLOCK) = 11epoll_ctl(8, EPOLL_CTL_ADD, 11, &#123;EPOLLIN|EPOLLRDHUP|EPOLLET, &#123;u32=69027249, u64=140003117974961&#125;&#125;) = 0epoll_wait(8, [&#123;EPOLLIN, &#123;u32=69027249, u64=140003117974961&#125;&#125;], 512, 60000) = 1recvfrom(11, \"GET /?key=testxxxx HTTP/1.1\\r\\nUse\"..., 1024, 0, NULL, NULL) = 88epoll_ctl(8, EPOLL_CTL_MOD, 11, &#123;EPOLLIN|EPOLLOUT|EPOLLRDHUP|EPOLLET, &#123;u32=69027249, u64=140003117974961&#125;&#125;) = 0open(\"/etc/nginx/china_cache/2/7d/2ac65d1f9080e2baff45a4332f9017d2\", O_RDONLY|O_NONBLOCK) = 14fstat(14, &#123;st_dev=makedev(253, 1), st_ino=655502, st_mode=S_IFREG|0600, st_nlink=1, st_uid=101, st_gid=101, st_blksize=4096, st_blocks=96, st_size=46750, st_atime=2018/07/17-16:34:43.967698739, st_mtime=2018/07/17-16:34:43.966698736, st_ctime=2018/07/17-16:34:43.967698739&#125;) = 0pread64(14, \"\\5\\0\\0\\0\\0\\0\\0\\0(\\252M[\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\"..., 636, 0) = 636getsockname(11, &#123;sa_family=AF_INET, sin_port=htons(80), sin_addr=inet_addr(\"172.17.0.2\")&#125;, [16]) = 0sendto(13, \"nN\\1\\0\\0\\1\\0\\0\\0\\0\\0\\0\\6bobcat\\6sxldns\\3com\\0\\0\"..., 35, 0, NULL, 0) = 35sendto(13, \"\\375\\215\\1\\0\\0\\1\\0\\0\\0\\0\\0\\0\\6bobcat\\6sxldns\\3com\\0\\0\"..., 35, 0, NULL, 0) = 35epoll_wait(8, [&#123;EPOLLOUT, &#123;u32=69027249, u64=140003117974961&#125;&#125;, &#123;EPOLLIN, &#123;u32=69026288, u64=140003117974000&#125;&#125;], 512, 5000) = 2recvfrom(13, \"nN\\201\\200\\0\\1\\0\\3\\0\\0\\0\\0\\6bobcat\\6sxldns\\3com\\0\\0\"..., 4096, 0, NULL, NULL) = 147recvfrom(13, \"\\375\\215\\201\\200\\0\\1\\0\\1\\0\\1\\0\\0\\6bobcat\\6sxldns\\3com\\0\\0\"..., 4096, 0, NULL, NULL) = 168socket(AF_INET, SOCK_STREAM, IPPROTO_IP) = 15ioctl(15, FIONBIO, [1]) = 0epoll_ctl(8, EPOLL_CTL_ADD, 15, &#123;EPOLLIN|EPOLLOUT|EPOLLRDHUP|EPOLLET, &#123;u32=69027488, u64=140003117975200&#125;&#125;) = 0connect(15, &#123;sa_family=AF_INET, sin_port=htons(80), sin_addr=inet_addr(\"54.222.148.216\")&#125;, 16) = -1 EINPROGRESS (Operation now in progress)recvfrom(13, 0x7ffd1f603790, 4096, 0, NULL, NULL) = -1 EAGAIN (Resource temporarily unavailable)epoll_wait(8, [&#123;EPOLLOUT, &#123;u32=69027488, u64=140003117975200&#125;&#125;], 512, 20000) = 1getsockopt(15, SOL_SOCKET, SO_ERROR, [0], [4]) = 0writev(15, [&#123;\"GET /?key=testxxxx HTTP/1.0\\r\\nHos\"..., 219&#125;], 1) = 219epoll_wait(8, [&#123;EPOLLIN|EPOLLOUT, &#123;u32=69027488, u64=140003117975200&#125;&#125;], 512, 60000) = 1recvfrom(15, \"HTTP/1.1 200 OK\\r\\nContent-Type: t\"..., 3723, 0, NULL, NULL) = 3723close(14) = 0readv(15, [&#123;\"a.qnssl.com/images/265818/Fntncr\"..., 4096&#125;], 1) = 4096readv(15, [&#123;\"w-card-price&#123;color: #004aa0;&#125;.s-\"..., 4096&#125;], 1) = 4096readv(15, [&#123;\" &#123;\\n font-family: \\\"Open Sans\"..., 4096&#125;], 1) = 373open(\"/etc/nginx/china_cache/2/7d/2ac65d1f9080e2baff45a4332f9017d2.0000014052\", O_RDWR|O_CREAT|O_EXCL, 0600) = 14pwritev(14, [&#123;\"\\5\\0\\0\\0\\0\\0\\0\\0\\356\\254M[\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\"..., 4096&#125;, &#123;\"a.qnssl.com/images/265818/Fntncr\"..., 4096&#125;, &#123;\"w-card-price&#123;color: #004aa0;&#125;.s-\"..., 4096&#125;], 3, 0) = -1 ENOSPC (No space left on device)gettid() = 7write(4, \"2018/07/17 08:46:33 [crit] 7#7: \"..., 328) = 328close(15) = 0unlink(\"/etc/nginx/china_cache/2/7d/2ac65d1f9080e2baff45a4332f9017d2.0000014052\") = 0write(5, \"&#123; \\\"time\\\": \\\"2018-07-17T08:46:33+0\"..., 382) = 382close(14) = 0close(11) = 02.正常调用堆栈过程[root@iZ2ze2mzhjk3ou1vsnkthzZ rpm]# strace -p 3904 -vstrace: Process 3904 attachedepoll_wait(8, [&#123;EPOLLIN, &#123;u32=69025808, u64=140003117973520&#125;&#125;], 512, -1) = 1accept4(6, &#123;sa_family=AF_INET, sin_port=htons(59360), sin_addr=inet_addr(\"10.130.0.4\")&#125;, [16], SOCK_NONBLOCK) = 11epoll_ctl(8, EPOLL_CTL_ADD, 11, &#123;EPOLLIN|EPOLLRDHUP|EPOLLET, &#123;u32=69027248, u64=140003117974960&#125;&#125;) = 0epoll_wait(8, [&#123;EPOLLIN, &#123;u32=69027248, u64=140003117974960&#125;&#125;], 512, 60000) = 1recvfrom(11, \"GET /?key=testxxxx HTTP/1.1\\r\\nUse\"..., 1024, 0, NULL, NULL) = 88epoll_ctl(8, EPOLL_CTL_MOD, 11, &#123;EPOLLIN|EPOLLOUT|EPOLLRDHUP|EPOLLET, &#123;u32=69027248, u64=140003117974960&#125;&#125;) = 0open(\"/etc/nginx/china_cache/2/7d/2ac65d1f9080e2baff45a4332f9017d2\", O_RDONLY|O_NONBLOCK) = 14fstat(14, &#123;st_dev=makedev(253, 1), st_ino=655504, st_mode=S_IFREG|0600, st_nlink=1, st_uid=101, st_gid=101, st_blksize=4096, st_blocks=96, st_size=46750, st_atime=2018/07/17-16:14:38.127407158, st_mtime=2018/07/17-16:14:38.127407158, st_ctime=2018/07/17-16:14:38.128407161&#125;) = 0pread64(14, \"\\5\\0\\0\\0\\0\\0\\0\\0s\\245M[\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\"..., 636, 0) = 636getsockname(11, &#123;sa_family=AF_INET, sin_port=htons(80), sin_addr=inet_addr(\"172.17.0.2\")&#125;, [16]) = 0sendto(12, \"\\210V\\1\\0\\0\\1\\0\\0\\0\\0\\0\\0\\6bobcat\\6sxldns\\3com\\0\\0\"..., 35, 0, NULL, 0) = 35sendto(12, \"\\27\\1\\1\\0\\0\\1\\0\\0\\0\\0\\0\\0\\6bobcat\\6sxldns\\3com\\0\\0\"..., 35, 0, NULL, 0) = 35epoll_wait(8, [&#123;EPOLLOUT, &#123;u32=69027248, u64=140003117974960&#125;&#125;, &#123;EPOLLIN, &#123;u32=69027728, u64=140003117975440&#125;&#125;], 512, 5000) = 2recvfrom(12, \"\\210V\\201\\200\\0\\1\\0\\3\\0\\0\\0\\0\\6bobcat\\6sxldns\\3com\\0\\0\"..., 4096, 0, NULL, NULL) = 147recvfrom(12, \"\\27\\1\\201\\200\\0\\1\\0\\1\\0\\1\\0\\0\\6bobcat\\6sxldns\\3com\\0\\0\"..., 4096, 0, NULL, NULL) = 168socket(AF_INET, SOCK_STREAM, IPPROTO_IP) = 15ioctl(15, FIONBIO, [1]) = 0epoll_ctl(8, EPOLL_CTL_ADD, 15, &#123;EPOLLIN|EPOLLOUT|EPOLLRDHUP|EPOLLET, &#123;u32=69027489, u64=140003117975201&#125;&#125;) = 0connect(15, &#123;sa_family=AF_INET, sin_port=htons(80), sin_addr=inet_addr(\"52.80.58.42\")&#125;, 16) = -1 EINPROGRESS (Operation now in progress)recvfrom(12, 0x7ffd1f603790, 4096, 0, NULL, NULL) = -1 EAGAIN (Resource temporarily unavailable)epoll_wait(8, [&#123;EPOLLOUT, &#123;u32=69027489, u64=140003117975201&#125;&#125;], 512, 20000) = 1getsockopt(15, SOL_SOCKET, SO_ERROR, [0], [4]) = 0writev(15, [&#123;\"GET /?key=testxxxx HTTP/1.0\\r\\nHos\"..., 219&#125;], 1) = 219epoll_wait(8, [&#123;EPOLLIN|EPOLLOUT, &#123;u32=69027489, u64=140003117975201&#125;&#125;], 512, 60000) = 1recvfrom(15, \"HTTP/1.1 200 OK\\r\\nContent-Type: t\"..., 3723, 0, NULL, NULL) = 3723close(14) = 0readv(15, [&#123;\"a.qnssl.com/images/265818/Fntncr\"..., 4096&#125;], 1) = 4096readv(15, [&#123;\"w-card-price&#123;color: #004aa0;&#125;.s-\"..., 4096&#125;], 1) = 4096readv(15, [&#123;\" &#123;\\n font-family: \\\"Open Sans\"..., 4096&#125;], 1) = 2565open(\"/etc/nginx/china_cache/2/7d/2ac65d1f9080e2baff45a4332f9017d2.0000014051\", O_RDWR|O_CREAT|O_EXCL, 0600) = 14pwritev(14, [&#123;\"\\5\\0\\0\\0\\0\\0\\0\\0(\\252M[\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\"..., 4096&#125;, &#123;\"a.qnssl.com/images/265818/Fntncr\"..., 4096&#125;, &#123;\"w-card-price&#123;color: #004aa0;&#125;.s-\"..., 4096&#125;], 3, 0) = 12288writev(11, [&#123;\"HTTP/1.1 200 OK\\r\\nServer: nginx/1\"..., 321&#125;], 1) = 321sendfile(11, 14, [636] =&gt; [12288], 11652) = 11652epoll_wait(8, [&#123;EPOLLOUT, &#123;u32=69027489, u64=140003117975201&#125;&#125;], 512, 59955) = 1epoll_wait(8, [&#123;EPOLLIN|EPOLLOUT, &#123;u32=69027489, u64=140003117975201&#125;&#125;], 512, 59953) = 1readv(15, [&#123;\"rc=\\\"//nzr2ybsda.qnssl.com/images\"..., 1531&#125;, &#123;\"lass=\\\"s-nav-item\\\" target=\\\"_self\\\"\"..., 4096&#125;, &#123;\"lign: left; font-size: 160%;\\\"&gt;\\302\\240\"..., 4096&#125;, &#123;\"OQswmpPvoA0.jpg?imageMogr2/strip\"..., 4096&#125;], 4) = 10136pwritev(14, [&#123;\" &#123;\\n font-family: \\\"Open Sans\"..., 4096&#125;, &#123;\"lass=\\\"s-nav-item\\\" target=\\\"_self\\\"\"..., 4096&#125;, &#123;\"lign: left; font-size: 160%;\\\"&gt;\\302\\240\"..., 4096&#125;], 3, 12288) = 12288sendfile(11, 14, [12288] =&gt; [24576], 12288) = 12288epoll_wait(8, [&#123;EPOLLIN|EPOLLOUT, &#123;u32=69027489, u64=140003117975201&#125;&#125;], 512, 59948) = 1readv(15, [&#123;\"olor-custom1\\\"&gt;\\347\\254\\254\\344\\270\\211\\346\\226\\271\\345\"..., 3683&#125;, &#123;\"container title-group-container\\\"\"..., 4096&#125;, &#123;\";cs=srgb&amp;s=f71a6adc1e5953a52\"..., 4096&#125;, &#123;\")\\\" data-bg=\\\"//nzr2ybsda.qnssl.co\"..., 4096&#125;], 4) = 15971readv(15, [&#123;\"\\230\\263\\345\\224\\257\\346\\231\\237\\\" title=\\\"\\346\\262\\210\\351\\230\\263\\345\\224\\257\\346\\231\\237\\345\\225\\206\"..., 4096&#125;], 1) = 2853pwritev(14, [&#123;\"OQswmpPvoA0.jpg?imageMogr2/strip\"..., 4096&#125;, &#123;\"container title-group-container\\\"\"..., 4096&#125;, &#123;\";cs=srgb&amp;s=f71a6adc1e5953a52\"..., 4096&#125;, &#123;\")\\\" data-bg=\\\"//nzr2ybsda.qnssl.co\"..., 4096&#125;], 4, 24576) = 16384sendfile(11, 14, [24576] =&gt; [40960], 16384) = 16384epoll_wait(8, [&#123;EPOLLIN|EPOLLOUT, &#123;u32=69027489, u64=140003117975201&#125;&#125;], 512, 59947) = 1readv(15, [&#123;\"&lt;d\"..., 1243&#125;, &#123;\"\\232\\204\\344\\270\\232\\347\\273\\251\\344\\271\\237\\345\\234\\250\\344\\270\\215\\346\\226\\255\\346\\224\\200\\345\\215\\207\\343\\200\\202&lt;/d\"..., 4096&#125;, &#123;\"\", 4096&#125;, &#123;\"\", 4096&#125;, &#123;\"\", 4096&#125;], 5) = 2937pwritev(14, [&#123;\"\\230\\263\\345\\224\\257\\346\\231\\237\\\" title=\\\"\\346\\262\\210\\351\\230\\263\\345\\224\\257\\346\\231\\237\\345\\225\\206\"..., 4096&#125;, &#123;\"\\232\\204\\344\\270\\232\\347\\273\\251\\344\\271\\237\\345\\234\\250\\344\\270\\215\\346\\226\\255\\346\\224\\200\\345\\215\\207\\343\\200\\202&lt;/d\"..., 1694&#125;], 2, 40960) = 5790 sendfile(11, 14, [40960] =&gt; [46750], 5790) = 5790chmod(\"/etc/nginx/china_cache/2/7d/2ac65d1f9080e2baff45a4332f9017d2.0000014051\", 0600) = 0rename(\"/etc/nginx/china_cache/2/7d/2ac65d1f9080e2baff45a4332f9017d2.0000014051\", \"/etc/nginx/china_cache/2/7d/2ac65d1f9080e2baff45a4332f9017d2\") = 0fstat(14, &#123;st_dev=makedev(253, 1), st_ino=655502, st_mode=S_IFREG|0600, st_nlink=1, st_uid=101, st_gid=101, st_blksize=4096, st_blocks=96, st_size=46750, st_atime=2018/07/17-16:34:43.967698739, st_mtime=2018/07/17-16:34:43.966698736, st_ctime=2018/07/17-16:34:43.967698739&#125;) = 0close(15) = 0write(5, \"&#123; \\\"time\\\": \\\"2018-07-17T08:34:43+0\"..., 386) = 386close(14) = 0setsockopt(11, SOL_TCP, TCP_NODELAY, [1], 4) = 0epoll_wait(8, [&#123;EPOLLIN|EPOLLOUT|EPOLLRDHUP, &#123;u32=69027248, u64=140003117974960&#125;&#125;], 512, 65000) = 1recvfrom(11, \"\", 1024, 0, NULL, NULL) = 0close(11) = 0","categories":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/categories/linux/"},{"name":"nginx","slug":"linux/nginx","permalink":"https://blog.xiemx.com/categories/linux/nginx/"},{"name":"debug","slug":"linux/nginx/debug","permalink":"https://blog.xiemx.com/categories/linux/nginx/debug/"}],"tags":[{"name":"debug","slug":"debug","permalink":"https://blog.xiemx.com/tags/debug/"},{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/tags/linux/"},{"name":"nginx","slug":"nginx","permalink":"https://blog.xiemx.com/tags/nginx/"}],"author":"xiemx"},{"title":"TCP状态机","slug":"2018-01-23-tcp-state","date":"2018-01-23T03:01:20.000Z","updated":"2019-10-19T07:36:31.310Z","comments":false,"path":"/2018/01/23/2018-01-23-tcp-state/","link":"","permalink":"https://blog.xiemx.com/2018/01/23/2018-01-23-tcp-state/","excerpt":"","text":"TCP状态分析 listen／close syn-sent/syn-revd established fin_wait_1/close_wait fin_wait_2/last_ack time_wait/close 12345678910LISTEN 等待来自远程TCP应用程序的请求SYN_SENT 发送连接请求后等待来自远程端点的确认。TCP第一次握手后客户端所处的状态SYN-RECEIVED 该端点已经接收到连接请求并发送确认。该端点正在等待最终确认。TCP第二次握手后服务端所处的状态ESTABLISHED 代表连接已经建立起来了。这是连接数据传输阶段的正常状态FIN_WAIT_1 等待来自远程TCP的终止连接请求或终止请求的确认FIN_WAIT_2 在此端点发送终止连接请求后，等待来自远程TCP的连接终止请求CLOSE_WAIT 该端点已经收到来自远程端点的关闭请求，此TCP正在等待本地应用程序的连接终止请求CLOSING 等待来自远程TCP的连接终止请求确认LAST_ACK 等待先前发送到远程TCP的连接终止请求的确认TIME_WAIT 等待足够的时间来确保远程TCP接收到其连接终止请求的确认 以上大致为一个Tcp从三次握手建立连接到四次挥手断开连接的整个过程C/S对应的TCP状态。 详细流程12345678910111. 客户端(close)发送syn连接请求给服务端(listen),客户端等待服务端ack(syn_sent)2. 服务端收到syn请求,发送ack/syn(syn_rec)3. 客户端收到ack(establelished)4. 传输数据5. 客户端数据交互完成请求关闭连接，发送fin请求(fin_wait_1)6. 服务端收到fin请求,发送ack(close_wait)7. 服务端数据交互完成,发送fin请求关闭连接(last_ack)8. 客户端收到服务端的ack请求(fin_wait_2)9. 客户端收到服务端的fin请求,发送ack确认断开(time_wait)10. 服务端收到客户端的ack,关闭连接(close)11. 客户端维护2个msl时间后回收socket 引用网上的一张图：","categories":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/tags/linux/"},{"name":"tcp","slug":"tcp","permalink":"https://blog.xiemx.com/tags/tcp/"}],"author":"xiemx"},{"title":"TCP TIME_WAIT","slug":"2018-01-23-tcp-time_wait","date":"2018-01-23T02:01:21.000Z","updated":"2019-10-19T07:40:03.304Z","comments":true,"path":"/2018/01/23/2018-01-23-tcp-time_wait/","link":"","permalink":"https://blog.xiemx.com/2018/01/23/2018-01-23-tcp-time_wait/","excerpt":"","text":"维持TIME_WAIT有两个原因： 可靠地实现TCP的全双工连接终止 在四次挥手中，假设最后的ACK丢失了，被动关闭方会重发FIN。主动关闭端必须维护状态，来允许被动关闭方重发最后的ACK；如果它没有维护这个状态，将会对重发FIN返回RST，被动关闭方会认为这是个错误。如果TCP正在执行彻底终止数据流的两个方向所需的所有工作（即全双工关闭），则必须正确处理这四个段中任何一个的丢失。所以执行主动关闭的一方必须在结束时保持TIME_WAIT状态：因为它可能必须重传最后的ACK。 允许旧的重复数据段在网络中过期 假设在主机1.1.1.1的1111端口和2.2.2.2的2222端口之间有一个TCP连接。此连接关闭后，相同的地址和端口建立了一个新连接。由于IP地址和端口相同，TCP必须防止旧连接的数据包再次出现，被新的连接误收。为此，TCP将不会启动当前处于TIME_WAIT状态的连接。由于TIME_WAIT状态的持续时间是两倍的MSL，因此TCP允许一个方向的数据在MSL秒内丢失，也允许回复在一个MSL秒内丢失。通过此规则来保证当一个TCP连接成功建立时，来自先前连接的所有旧的副本在网络中已过期。","categories":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/tags/linux/"},{"name":"tcp","slug":"tcp","permalink":"https://blog.xiemx.com/tags/tcp/"}],"author":"xiemx"},{"title":"zabbix distrubuted monitor","slug":"2017-09-21-zabbix-distrubuted-monitor","date":"2017-09-20T18:09:37.000Z","updated":"2019-09-27T06:52:36.183Z","comments":false,"path":"/2017/09/21/2017-09-21-zabbix-distrubuted-monitor/","link":"","permalink":"https://blog.xiemx.com/2017/09/21/2017-09-21-zabbix-distrubuted-monitor/","excerpt":"","text":"zabbix 分布式监控2种模式 node模式 proxy模式 PS: node模式官方在2.4版本之后已经弃用，重点讨论proxy模式 proxy 模式Zabbix proxy可以代替Zabbix服务器收集性能和可用性数据，一个代理可以承担一些收集数据的负载。使用代理是实现集中式和分布式监控的最简单方法。proxy需要使用单独的数据库来缓存agent数据，在发给server防止出现因网络问题造成的数据丢失。zabbix proxy只是一个数据收集组件，不会触发任何trigger／alert. 使用场景 Monitor remote locations Monitor locations having unreliable communications Offload the Zabbix server when monitoring thousands of devices Simplify the maintenance of distributed monitoring 安装配置1234567891011121314#同安装zabbix server 类似，不赘述,需要其他功能也可以在编译时自行开启。./configure --prefix=/opt/zabbix_proxy/ --enable-proxy --with-mysql --with-libcurlmake installcreate databases zabbixgrant all to zabbix.* to zabbix@'%' identified by \"zabbix\";#导入schema.sql#配置文件中hostname需要和zabbix上添加的保持一致#其它参考server设置参数#ps：设置适当的配置同步时间，默认一小时。建议设置短一点，这样如果有新机器加入配置修改都可以快速同步并监控。ConfigFrequency=600 zabbix server 配置 添加主机时选择指定的proxy","categories":[{"name":"zabbix","slug":"zabbix","permalink":"https://blog.xiemx.com/categories/zabbix/"}],"tags":[{"name":"zabbix","slug":"zabbix","permalink":"https://blog.xiemx.com/tags/zabbix/"}],"author":"xiemx"},{"title":"graphite_relay sharding","slug":"2017-09-08-graphite_relay-sharding","date":"2017-09-07T22:09:47.000Z","updated":"2019-10-19T15:38:29.276Z","comments":false,"path":"/2017/09/08/2017-09-08-graphite_relay-sharding/","link":"","permalink":"https://blog.xiemx.com/2017/09/08/2017-09-08-graphite_relay-sharding/","excerpt":"","text":"当statsd发送超量的metrics到graphite中，graphite单节点无法负载的情况，可以使用consistent-hashing的模式来将数据分片到backend中。同样的consistent-hashing模式下可以自动剔除／加入节点。 官方文档： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657carbon-relay.py serves two distinct purposes: replication and sharding.When running with RELAY_METHOD = rules, a carbon-relay.py instance can run in place of a carbon-cache.py server and relay all incoming metrics to multiple backend carbon-cache.py‘s running on different ports or hosts.In RELAY_METHOD = consistent-hashing mode, a DESTINATIONS setting defines a sharding strategy across multiple carbon-cache.py backends. The same consistent hashing list can be provided to the graphite webapp via CARBONLINK_HOSTS to spread reads across the multiple backends.本例模拟一个双后端的carbon-cache instance.单机器运行,使用carbon-cache.py 的instance功能config文件：#####carbon.conf[cache:a]LINE_RECEIVER_PORT = 2203PICKLE_RECEIVER_PORT = 2204CACHE_QUERY_PORT = 7102LOCAL_DATA_DIR = /opt/graphite/storage/whisper_a/[cache:b]LINE_RECEIVER_PORT = 2205PICKLE_RECEIVER_PORT = 2206CACHE_QUERY_PORT = 7202LOCAL_DATA_DIR = /opt/graphite/storage/whisper_b/[relay]LINE_RECEIVER_INTERFACE = 0.0.0.0LINE_RECEIVER_PORT = 2003PICKLE_RECEIVER_INTERFACE = 0.0.0.0PICKLE_RECEIVER_PORT = 2004DESTINATIONS = 127.0.0.1:2204:a, 127.0.0.1:2206:b 启动服务：启动instance:axmx@xiemx-test:/opt/graphisudo ./carbon-cache.py --instance=a --config=/opt/graphite/conf/carbon.conf startStarting carbon-cache (instance a)启动instance:bxmx@xiemx-test:/opt/graphite/bin$ sudo ./carbon-cache.py --instance=b --config=/opt/graphite/conf/carbon.conf startStarting carbon-cache (instance b)启动relay:xmx@xiemx-test:/opt/graphite/conf$ sudo /opt/graphite/bin/carbon-relay.py --debug --config=/opt/graphite/conf/carbon.conf startStarting carbon-relay (instance a)08/09/2017 16:02:16 :: [console] Using sorted write strategy for cache08/09/2017 16:02:16 :: [clients] connecting to carbon daemon at 127.0.0.1:2204:a08/09/2017 16:02:16 :: [clients] connecting to carbon daemon at 127.0.0.1:2206:b08/09/2017 16:02:16 :: [console] twistd 16.4.1 (/usr/bin/python 2.7.6) starting up.08/09/2017 16:02:16 :: [console] reactor class: twisted.internet.epollreactor.EPollReactor.08/09/2017 16:02:16 :: [console] Starting factory CarbonClientFactory(127.0.0.1:2206:b)08/09/2017 16:02:16 :: [clients] CarbonClientFactory(127.0.0.1:2206:b)::startedConnecting (127.0.0.1:2206)08/09/2017 16:02:16 :: [console] Starting factory CarbonClientFactory(127.0.0.1:2204:a)08/09/2017 16:02:16 :: [clients] CarbonClientFactory(127.0.0.1:2204:a)::startedConnecting (127.0.0.1:2204)08/09/2017 16:02:16 :: [console] CarbonReceiverFactory starting on 200308/09/2017 16:02:16 :: [console] Starting factory 08/09/2017 16:02:16 :: [console] CarbonReceiverFactory starting on 200408/09/2017 16:02:16 :: [console] Starting factory 08/09/2017 16:02:16 :: [clients] CarbonClientProtocol(127.0.0.1:2206:b)::connectionMade08/09/2017 16:02:16 :: [clients] CarbonClientFactory(127.0.0.1:2206:b)::connectionMade (CarbonClientProtocol(127.0.0.1:2206:b))08/09/2017 16:02:16 :: [clients] Destination is up: 127.0.0.1:2206:b08/09/2017 16:02:16 :: [clients] CarbonClientProtocol(127.0.0.1:2204:a)::connectionMade08/09/2017 16:02:16 :: [clients] CarbonClientFactory(127.0.0.1:2204:a)::connectionMade (CarbonClientProtocol(127.0.0.1:2204:a))08/09/2017 16:02:16 :: [clients] Destination is up: 127.0.0.1:2204:a 测试： 模拟5个客户端同时发送100个key 模拟node掉线重连 graphite-web数据聚合展示 12345修改local_settings.pyCARBONLINK_HOSTS = [\"127.0.0.1:7102:a\", \"127.0.0.1:7202:b\"]启动djangosudo PYTHONPATH=/opt/graphite/webapp django-admin.py runserver 0.0.0.0:5000 --settings","categories":[{"name":"statsd","slug":"statsd","permalink":"https://blog.xiemx.com/categories/statsd/"},{"name":"graphite","slug":"statsd/graphite","permalink":"https://blog.xiemx.com/categories/statsd/graphite/"}],"tags":[],"author":"xiemx"},{"title":"statsd cluster proxy","slug":"2017-09-08-statsd-cluster-proxy","date":"2017-09-07T21:09:37.000Z","updated":"2019-10-19T15:36:39.659Z","comments":false,"path":"/2017/09/08/2017-09-08-statsd-cluster-proxy/","link":"","permalink":"https://blog.xiemx.com/2017/09/08/2017-09-08-statsd-cluster-proxy/","excerpt":"","text":"通过UDP proxy程序将前端的数据通过一定的hash算法将相同的metric发送的固定的后aggregation数据。proxy代理支持健康检测自动剔除／加入后端statsd。 1.配置本例展示一个3节点的statsd后端且将聚合数据发送到standout方便观测。 123456789101112node1######config文件：&#123; port: 8127, mgmt_port: 8227, backends: [ \"./backends/console\" ]&#125;启动:xiemx➜ statsd : master ✘ :✹✭ ᐅ node stats.js config8127.js8 Sep 11:06:36 - [81604] reading config file: config8127.js8 Sep 11:06:36 - server is up INFO 123456789101112node2######config文件：&#123; port: 8128, mgmt_port: 8228, backends: [ \"./backends/console\" ]&#125;启动：xiemx➜ statsd : master ✘ :✹✭ ᐅ node stats.js config8128.js8 Sep 11:07:09 - [81665] reading config file: config8128.js8 Sep 11:07:09 - server is up INFO 123456789101112node3######config文件：&#123; port: 8129, mgmt_port: 8229, backends: [ \"./backends/console\" ]&#125;启动：xiemx➜ statsd : master ✘ :✹✭ ᐅ node stats.js config8129.js8 Sep 11:07:45 - [81723] reading config file: config8129.js8 Sep 11:07:45 - server is up INFO 12345678910111213141516171819202122proxy######config文件：&#123;nodes: [&#123;host: '127.0.0.1', port: 8127, adminport: 8227&#125;,&#123;host: '127.0.0.1', port: 8128, adminport: 8228&#125;,&#123;host: '127.0.0.1', port: 8129, adminport: 8229&#125;],server: './servers/udp',host: '0.0.0.0',port: 8125,mgmt_port: 8126,forkCount: 0,checkInterval: 1000,cacheSize: 10000,deleteIdleStats: true&#125;启动：xiemx➜ statsd : master ✘ :✹✭ ᐅ node proxy.js proxyconfig.js8 Sep 11:09:02 - [81938] reading config file: proxyconfig.js8 Sep 11:09:02 - INFO: [81938] server is up 测试：5个线程同时推送500个metric到代理查看分片情况 12测试命令：for i in $(seq 1 500);do echo \"Ezbuy-$i:1|c\" | nc -u -w0 127.0.0.1 8125;done 节点自动剔除和加入：","categories":[{"name":"statsd","slug":"statsd","permalink":"https://blog.xiemx.com/categories/statsd/"},{"name":"graphite","slug":"statsd/graphite","permalink":"https://blog.xiemx.com/categories/statsd/graphite/"}],"tags":[{"name":"database","slug":"database","permalink":"https://blog.xiemx.com/tags/database/"},{"name":"monitor","slug":"monitor","permalink":"https://blog.xiemx.com/tags/monitor/"},{"name":"statsd","slug":"statsd","permalink":"https://blog.xiemx.com/tags/statsd/"},{"name":"graphite","slug":"graphite","permalink":"https://blog.xiemx.com/tags/graphite/"}],"author":"xiemx"},{"title":"redis 淘汰策略","slug":"2017-07-14-redis-maxmemory-policy","date":"2017-07-14T02:07:44.000Z","updated":"2019-10-19T09:11:22.213Z","comments":true,"path":"/2017/07/14/2017-07-14-redis-maxmemory-policy/","link":"","permalink":"https://blog.xiemx.com/2017/07/14/2017-07-14-redis-maxmemory-policy/","excerpt":"","text":"redis 内存数据使用到maxmemory的时候，就会根据maxmemory_policy设定的淘汰策略进行内存整理数据回收。选择不同的策略，要根据redis的用途来区分。 redis 提供 6种数据淘汰策略： volatile-lru：从已设置过期时间的数据集中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集中任意选择数据淘汰 allkeys-lru：从数据集中挑选最近最少使用的数据淘汰 allkeys-random：从数据集中任意选择数据淘汰 noenviction：禁止删除数据","categories":[{"name":"redis","slug":"redis","permalink":"https://blog.xiemx.com/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://blog.xiemx.com/tags/redis/"}],"author":"xiemx"},{"title":"iptables count计数","slug":"2017-04-11-iptables-count","date":"2017-04-10T21:04:23.000Z","updated":"2019-10-19T15:40:20.946Z","comments":false,"path":"/2017/04/11/2017-04-11-iptables-count/","link":"","permalink":"https://blog.xiemx.com/2017/04/11/2017-04-11-iptables-count/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445两个参数： --verbose -v verbose mode --zero -Z [chain [rulenum]] Zero counters in chain or all chainsp-hsg-cache-6% sudo iptables -nL -v -t natChain PREROUTING (policy ACCEPT 50741 packets, 2370K bytes) pkts bytes target prot opt in out source destination 436 22672 DNAT tcp -- * * 0.0.0.0/0 192.168.10.226 tcp dpt:6379 to:192.168.10.82:6379 0 0 DNAT tcp -- * * 0.0.0.0/0 192.168.10.226 tcp dpt:6380 to:192.168.10.81:6380 0 0 DNAT tcp -- * * 0.0.0.0/0 192.168.10.226 tcp dpt:6381 to:192.168.10.81:6381 0 0 DNAT tcp -- * * 0.0.0.0/0 192.168.10.226 tcp dpt:6382 to:192.168.10.81:6382 0 0 DNAT tcp -- * * 0.0.0.0/0 192.168.10.226 tcp dpt:6383 to:192.168.10.81:6383Chain INPUT (policy ACCEPT 18988 packets, 1082K bytes) pkts bytes target prot opt in out source destinationChain OUTPUT (policy ACCEPT 10532 packets, 549K bytes) pkts bytes target prot opt in out source destinationChain POSTROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination10968 572K MASQUERADE all -- * * 0.0.0.0/0 0.0.0.0/0p-hsg-cache-6% sudo iptables -Z -t natp-hsg-cache-6% sudo iptables -nL -v -t natChain PREROUTING (policy ACCEPT 12 packets, 624 bytes) pkts bytes target prot opt in out source destination 1 52 DNAT tcp -- * * 0.0.0.0/0 192.168.10.226 tcp dpt:6379 to:192.168.10.82:6379 0 0 DNAT tcp -- * * 0.0.0.0/0 192.168.10.226 tcp dpt:6380 to:192.168.10.81:6380 0 0 DNAT tcp -- * * 0.0.0.0/0 192.168.10.226 tcp dpt:6381 to:192.168.10.81:6381 0 0 DNAT tcp -- * * 0.0.0.0/0 192.168.10.226 tcp dpt:6382 to:192.168.10.81:6382 0 0 DNAT tcp -- * * 0.0.0.0/0 192.168.10.226 tcp dpt:6383 to:192.168.10.81:6383Chain INPUT (policy ACCEPT 12 packets, 624 bytes) pkts bytes target prot opt in out source destinationChain OUTPUT (policy ACCEPT 6 packets, 312 bytes) pkts bytes target prot opt in out source destinationChain POSTROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 7 364 MASQUERADE all -- * * 0.0.0.0/0 0.0.0.0/0p-hsg-cache-6%","categories":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/tags/linux/"},{"name":"iptables","slug":"iptables","permalink":"https://blog.xiemx.com/tags/iptables/"}],"author":"xiemx"},{"title":"redis repl_diskless_replication","slug":"2017-03-06-redis-repl_diskless_replication","date":"2017-03-05T20:03:55.000Z","updated":"2019-10-19T11:02:30.964Z","comments":false,"path":"/2017/03/06/2017-03-06-redis-repl_diskless_replication/","link":"","permalink":"https://blog.xiemx.com/2017/03/06/2017-03-06-redis-repl_diskless_replication/","excerpt":"","text":"redis diskless replication 12345678910111213141516171819202122232425262728293031# Replicationrole:slavemaster_host:192.168.10.226master_port:6379master_link_status:downmaster_last_io_seconds_ago:-1master_sync_in_progress:1slave_repl_offset:1master_sync_left_bytes:-4022404224master_sync_last_io_seconds_ago:37master_link_down_since_seconds:1488785860slave_priority:100slave_read_only:1connected_slaves:0master_repl_offset:0repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:0repl_backlog_histlen:0# Replicationrole:masterconnected_slaves:2slave0:ip=192.168.10.192,port=6379,state=online,offset=1134556373255,lag=1slave1:ip=192.168.10.82,port=6379,state=wait_bgsave,offset=0,lag=0master_repl_offset:1134556812751repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1134555764176repl_backlog_histlen:1048576 1234567891011121314151617181920212223246581:C 06 Mar 15:28:27.766 * DB saved on disk26581:C 06 Mar 15:28:27.990 * RDB: 212 MB of memory used by copy-on-write2251:M 06 Mar 15:28:28.379 * Background saving terminated with success2251:M 06 Mar 15:29:08.687 * Synchronization with slave 192.168.10.82:6379 succeeded2251:M 06 Mar 15:29:47.093 # Client id=314558845 addr=192.168.10.82:58002 fd=16 name= age=194 idle=1 flags=S db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=3617 omem=95797745 events=rw cmd=psync scheduled to be closed ASAP for overcoming of output buffer limits.2251:M 06 Mar 15:29:47.093 # Connection with slave 192.168.10.82:6379 lost.2251:M 06 Mar 15:30:15.999 * Slave 192.168.10.82:6379 asks for synchronization2251:M 06 Mar 15:30:15.999 * Unable to partial resync with slave 192.168.10.82:6379 for lack of backlog (Slave request was: 1134354232038).2251:M 06 Mar 15:30:15.999 * Starting BGSAVE for SYNC with target: disk2251:M 06 Mar 15:30:16.232 * Background saving started by pid 3049430494:C 06 Mar 15:31:52.413 * DB saved on disk30494:C 06 Mar 15:31:52.597 * RDB: 223 MB of memory used by copy-on-write2251:M 06 Mar 15:31:52.950 * Background saving terminated with success2251:M 06 Mar 15:32:33.375 * Synchronization with slave 192.168.10.82:6379 succeeded2251:M 06 Mar 15:33:08.106 # Client id=314561307 addr=192.168.10.82:38644 fd=16 name= age=173 idle=1 flags=S db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=4325 omem=102199438 events=rw cmd=psync scheduled to be closed ASAP for overcoming of output buffer limits.2251:M 06 Mar 15:33:08.106 # Connection with slave 192.168.10.82:6379 lost.2251:M 06 Mar 15:33:40.471 * Slave 192.168.10.82:6379 asks for synchronization2251:M 06 Mar 15:33:40.471 * Unable to partial resync with slave 192.168.10.82:6379 for lack of backlog (Slave request was: 1134453799057).2251:M 06 Mar 15:33:40.471 * Starting BGSAVE for SYNC with target: disk2251:M 06 Mar 15:33:40.715 * Background saving started by pid 14402251:M 06 Mar 15:35:14.354 # Connection with slave 192.168.10.82:6379 lost.1440:C 06 Mar 15:35:20.564 * DB saved on disk1440:C 06 Mar 15:35:20.743 * RDB: 296 MB of memory used by copy-on-write2251:M 06 Mar 15:35:21.013 * Background saving terminated with success 原理分析如果设置了一个slave，不管是在第一次链接还是重新链接master的时候，slave会发送一个同步命令然后master开始后台保存，收集所有对修改数据的命令。当后台保存完成，master会将这个数据文件传送到slave，然后保存在磁盘，加载到内存中，master接着发送收集到的所有的修改数据的命令。 Redis为避免输出缓冲区过度耗用内存，使用client-output-buffer-limit参数限制客户端输出缓冲区内存使用量。Redis数据复制过程中，Slave有个flags=S的客户端连接到Master; 它和其他客户端一样有输出缓冲区和缓冲区大小限制。client-output-buffer-limit slave 256mb 64mb 60当缓冲区使用超过256mb,Master会尽快杀掉它；当缓冲区使用大于64mb,且小于256mb的soft limit值时，并持续时间达60秒，也会被Master尽快杀掉。 通过以上的原理分析我们可以得到一个解决方法：扩大buffer和timeout，但对于一个数据量比较大的db且服务器本身内存不足的情况下。我们可以采用diskless replication，直接通过网络将数据更新过去，不再让服务器去同步到disk再从disk输出到内存导致output报错 解决开启redis diskless replication 123127.0.0.1:6379&gt; CONFIG set repl-diskless-sync yesOK(1.97s) 1234567891011121314152251:M 06 Mar 15:35:34.896 * Slave 192.168.10.82:6379 asks for synchronization2251:M 06 Mar 15:35:34.896 * Full resync requested by slave 192.168.10.82:63792251:M 06 Mar 15:35:34.896 * Delay next BGSAVE for SYNC2251:M 06 Mar 15:35:40.203 * Starting BGSAVE for SYNC with target: slaves sockets2251:M 06 Mar 15:35:40.469 * Background RDB transfer started by pid 33513351:C 06 Mar 15:37:03.206 * RDB: 233 MB of memory used by copy-on-write2251:M 06 Mar 15:37:03.473 * Background RDB transfer terminated with success2251:M 06 Mar 15:37:03.473 # Slave 192.168.10.82:6379 correctly received the streamed RDB file.2251:M 06 Mar 15:37:03.473 * Streamed RDB transfer with slave 192.168.10.82:6379 succeeded (socket). Waiting for REPLCONF ACK from slave to enable streaming2251:M 06 Mar 15:37:24.848 * 50000 changes in 60 seconds. Saving...2251:M 06 Mar 15:37:25.097 * Background saving started by pid 59162251:M 06 Mar 15:38:10.510 * Synchronization with slave 192.168.10.82:6379 succeeded5916:C 06 Mar 15:39:06.535 * DB saved on disk5916:C 06 Mar 15:39:06.742 * RDB: 250 MB of memory used by copy-on-write2251:M 06 Mar 15:39:07.093 * Background saving terminated with success 123456789101112131415161718192021222324252627# Replicationrole:slavemaster_host:192.168.10.226master_port:6379master_link_status:upmaster_last_io_seconds_ago:1master_sync_in_progress:0slave_repl_offset:1134697190334slave_priority:100slave_read_only:1connected_slaves:0master_repl_offset:0repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:0repl_backlog_histlen:0# Replicationrole:masterconnected_slaves:2slave0:ip=192.168.10.192,port=6379,state=online,offset=1135553694283,lag=1slave1:ip=192.168.10.82,port=6379,state=online,offset=1135553653727,lag=1master_repl_offset:1135554186742repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1135553138167repl_backlog_histlen:1048576","categories":[{"name":"redis","slug":"redis","permalink":"https://blog.xiemx.com/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://blog.xiemx.com/tags/redis/"}],"author":"xiemx"},{"title":"redis-trib.rb工具使用","slug":"2017-02-27-redis-trib-rb","date":"2017-02-26T21:02:35.000Z","updated":"2019-10-19T08:59:05.863Z","comments":false,"path":"/2017/02/27/2017-02-27-redis-trib-rb/","link":"","permalink":"https://blog.xiemx.com/2017/02/27/2017-02-27-redis-trib-rb/","excerpt":"","text":"redis-trib.rb是redis官方推出的管理redis集群的工具，集成在redis的源码src目录下，是基于redis提供的集群命令封装成简单、便捷、实用的操作工具。redis-trib.rb是redis作者用ruby完成的。 12345678910111213141516171819202122232425262728293031323334353637root@p-hsg-redis-1:~# redis-trib.rbUsage: redis-trib create host1:port1 ... hostN:portN --replicas check host:port info host:port fix host:port --timeout reshard host:port --from --to --slots --yes --timeout --pipeline rebalance host:port --weight --auto-weights --use-empty-masters --timeout --simulate --pipeline --threshold add-node new_host:new_port existing_host:existing_port --slave --master-id del-node host:port node_id set-timeout host:port milliseconds call host:port command arg arg .. arg import host:port --from --copy --replace help (show this help)For check, fix, reshard, del-node, set-timeout you can specify the host and port of any working node in the cluster. 可以看到redis-trib.rb具有以下功能： create：创建集群 check：检查集群 info：查看集群信息 fix：修复集群 reshard：在线迁移slot rebalance：平衡集群节点slot数量 add-node：将新节点加入集群 del-node：从集群中删除节点 set-timeout：设置集群节点间心跳连接的超时时间 call：在集群全部节点上执行命令 import：将外部redis数据导入集群 create创建集群create命令可选replicas参数，replicas表示需要有几个slave。最简单命令使用如下： 1$ruby redis-trib.rb create 10.180.157.199:6379 10.180.157.200:6379 10.180.157.201:6379 有一个slave的创建命令如下： 1$ruby redis-trib.rb create --replicas 1 10.180.157.199:6379 10.180.157.200:6379 10.180.157.201:6379 10.180.157.202:6379 10.180.157.205:6379 10.180.157.208:6379 创建流程如下： 123456789101112131415161718191、首先为每个节点创建ClusterNode对象，包括连接每个节点。检查每个节点是否为独立且db为空的节点。执行load_info方法导入节点信息。2、检查传入的master节点数量是否大于等于3个。只有大于3个节点才能组成集群。3、计算每个master需要分配的slot数量，以及给master分配slave。分配的算法大致如下：先把节点按照host分类，这样保证master节点能分配到更多的主机中。不停遍历遍历host列表，从每个host列表中弹出一个节点，放入interleaved数组。直到所有的节点都弹出为止。master节点列表就是interleaved前面的master数量的节点列表。保存在masters数组。计算每个master节点负责的slot数量，保存在slots_per_node对象，用slot总数除以master数量取整即可。遍历masters数组，每个master分配slots_per_node个slot，最后一个master，分配到16384个slot为止。接下来为master分配slave，分配算法会尽量保证master和slave节点不在同一台主机上。对于分配完指定slave数量的节点，还有多余的节点，也会为这些节点寻找master。分配算法会遍历两次masters数组。第一次遍历masters数组，在余下的节点列表找到replicas数量个slave。每个slave为第一个和master节点host不一样的节点，如果没有不一样的节点，则直接取出余下列表的第一个节点。第二次遍历是在对于节点数除以replicas不为整数，则会多余一部分节点。遍历的方式跟第一次一样，只是第一次会一次性给master分配replicas数量个slave，而第二次遍历只分配一个，直到余下的节点被全部分配出去。4、打印出分配信息，并提示用户输入“yes”确认是否按照打印出来的分配方式创建集群。5、输入“yes”后，会执行flush_nodes_config操作，该操作执行前面的分配结果，给master分配slot，让slave复制master，对于还没有握手（cluster meet）的节点，slave复制操作无法完成，不过没关系，flush_nodes_config操作出现异常会很快返回，后续握手后会再次执行flush_nodes_config。6、给每个节点分配epoch，遍历节点，每个节点分配的epoch比之前节点大1。7、节点间开始相互握手，握手的方式为节点列表的其他节点跟第一个节点握手。8、然后每隔1秒检查一次各个节点是否已经消息同步完成，使用ClusterNode的get_config_signature方法，检查的算法为获取每个节点cluster nodes信息，排序每个节点，组装成node_id1:slots|node_id2:slot2|...的字符串。如果每个节点获得字符串都相同，即认为握手成功。9、此后会再执行一次flush_nodes_config，这次主要是为了完成slave复制操作。10、最后再执行check_cluster，全面检查一次集群状态。包括和前面握手时检查一样的方式再检查一遍。确认没有迁移的节点。确认所有的slot都被分配出去了。11、至此完成了整个创建流程，返回[OK] All 16384 slots covered.。 check检查集群检查集群状态的命令，没有其他参数，只需要选择一个集群中的一个节点即可。执行命令以及结果如下： 123456789101112131415161718192021222324$ruby redis-trib.rb check 10.180.157.199:6379&gt;&gt;&gt; Performing Cluster Check (using node 10.180.157.199:6379)M: b2506515b38e6bbd3034d540599f4cd2a5279ad1 10.180.157.199:6379 slots:0-5460 (5461 slots) master 1 additional replica(s)S: d376aaf80de0e01dde1f8cd4647d5ac3317a8641 10.180.157.205:6379 slots: (0 slots) slave replicates e36c46dbe90960f30861af00786d4c2064e63df2M: 15126fb33796c2c26ea89e553418946f7443d5a5 10.180.157.201:6379 slots:10923-16383 (5461 slots) master 1 additional replica(s)S: 59fa6ee455f58a5076f6d6f83ddd74161fd7fb55 10.180.157.208:6379 slots: (0 slots) slave replicates 15126fb33796c2c26ea89e553418946f7443d5a5S: 460b3a11e296aafb2615043291b7dd98274bb351 10.180.157.202:6379 slots: (0 slots) slave replicates b2506515b38e6bbd3034d540599f4cd2a5279ad1M: e36c46dbe90960f30861af00786d4c2064e63df2 10.180.157.200:6379 slots:5461-10922 (5462 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 检查前会先执行load_cluster_info_from_node方法，把所有节点数据load进来。load的方式为通过自己的cluster nodes发现其他节点，然后连接每个节点，并加入nodes数组。接着生成节点间的复制关系。 load完数据后，开始检查数据，检查的方式也是调用创建时候使用的check_cluster。 info查看集群信息info命令用来查看集群的信息。info命令也是先执行load_cluster_info_from_node获取完整的集群信息。然后显示ClusterNode的info_string结果，示例如下： 123456$ruby redis-trib.rb info 10.180.157.199:637910.180.157.199:6379 (b2506515...) -&gt; 0 keys | 5461 slots | 1 slaves.10.180.157.201:6379 (15126fb3...) -&gt; 0 keys | 5461 slots | 1 slaves.10.180.157.200:6379 (e36c46db...) -&gt; 0 keys | 5462 slots | 1 slaves.[OK] 0 keys in 3 masters.0.00 keys per slot on average. fix修复集群fix命令的流程跟check的流程很像，显示加载集群信息，然后在check_cluster方法内传入fix为true的变量，会在集群检查出现异常的时候执行修复流程。目前fix命令能修复两种异常，一种是集群有处于迁移中的slot的节点，一种是slot未完全分配的异常。 fix_open_slot方法是修复集群有处于迁移中的slot的节点异常。 12345678910111、先检查该slot是谁负责的，迁移的源节点如果没完成迁移，owner还是该节点。没有owner的slot无法完成修复功能。2、遍历每个节点，获取哪些节点标记该slot为migrating状态，哪些节点标记该slot为importing状态。对于owner不是该节点，但是通过cluster countkeysinslot获取到该节点有数据的情况，也认为该节点为importing状态。3、如果migrating和importing状态的节点均只有1个，这可能是迁移过程中redis-trib.rb被中断所致，直接执行move_slot继续完成迁移任务即可。传递dots和fix为true。4、如果migrating为空，importing状态的节点大于0，那么这种情况执行回滚流程，将importing状态的节点数据通过move_slot方法导给slot的owner节点，传递dots、fix和cold为true。接着对importing的节点执行cluster stable命令恢复稳定。5、如果importing状态的节点为空，有一个migrating状态的节点，而且该节点在当前slot没有数据，那么可以直接把这个slot设为stable。6、如果migrating和importing状态不是上述情况，目前redis-trib.rb工具无法修复，上述的三种情况也已经覆盖了通过redis-trib.rb工具迁移出现异常的各个方面，人为的异常情形太多，很难考虑完全。fix_slots_coverage方法能修复slot未完全分配的异常。未分配的slot有三种状态。1、所有节点的该slot都没有数据。该状态redis-trib.rb工具直接采用随机分配的方式，并没有考虑节点的均衡。本人尝试对没有分配slot的集群通过fix修复集群，结果slot还是能比较平均的分配，但是没有了连续性，打印的slot信息非常离散。2、有一个节点的该slot有数据。该状态下，直接把slot分配给该slot有数据的节点。3、有多个节点的该slot有数据。此种情况目前还处于TODO状态，不过redis作者列出了修复的步骤，对这些节点，除第一个节点，执行cluster migrating命令，然后把这些节点的数据迁移到第一个节点上。清除migrating状态，然后把slot分配给第一个节点。 reshard在线迁移slotreshard命令可以在线把集群的一些slot从集群原来slot负责节点迁移到新的节点，利用reshard可以完成集群的在线横向扩容和缩容。 reshard的参数很多，下面来一一解释一番： 1234567891011121314reshard host:port --from --to --slots --yes --timeout --pipeline host:port：这个是必传参数，用来从一个节点获取整个集群信息，相当于获取集群信息的入口。--from ：需要从哪些源节点上迁移slot，可从多个源节点完成迁移，以逗号隔开，传递的是节点的node id，还可以直接传递--from all，这样源节点就是集群的所有节点，不传递该参数的话，则会在迁移过程中提示用户输入。--to ：slot需要迁移的目的节点的node id，目的节点只能填写一个，不传递该参数的话，则会在迁移过程中提示用户输入。--slots ：需要迁移的slot数量，不传递该参数的话，则会在迁移过程中提示用户输入。--yes：设置该参数，可以在打印执行reshard计划的时候，提示用户输入yes确认后再执行reshard。--timeout ：设置migrate命令的超时时间。--pipeline ：定义cluster getkeysinslot命令一次取出的key数量，不传的话使用默认值为10。 迁移的流程如下： 12345678910111213141516171、通过load_cluster_info_from_node方法装载集群信息。2、执行check_cluster方法检查集群是否健康。只有健康的集群才能进行迁移。3、获取需要迁移的slot数量，用户没传递--slots参数，则提示用户手动输入。4、获取迁移的目的节点，用户没传递--to参数，则提示用户手动输入。此处会检查目的节点必须为master节点。5、获取迁移的源节点，用户没传递--from参数，则提示用户手动输入。此处会检查源节点必须为master节点。--from all的话，源节点就是除了目的节点外的全部master节点。这里为了保证集群slot分配的平均，建议传递--from all。6、执行compute_reshard_table方法，计算需要迁移的slot数量如何分配到源节点列表，采用的算法是按照节点负责slot数量由多到少排序，计算每个节点需要迁移的slot的方法为：迁移slot数量 * (该源节点负责的slot数量 / 源节点列表负责的slot总数)。这样算出的数量可能不为整数，这里代码用了下面的方式处理：n = (numslots/source_tot_slots*s.slots.length)if i == 0 n = n.ceilelse n = n.floor这样的处理方式会带来最终分配的slot与请求迁移的slot数量不一致，这个BUG已经在github上提给作者，https://github.com/antirez/redis/issues/2990。7、打印出reshard计划，如果用户没传--yes，就提示用户确认计划。8、根据reshard计划，一个个slot的迁移到新节点上，迁移使用move_slot方法，该方法被很多命令使用，具体可以参见下面的迁移流程。move_slot方法传递dots为true和pipeline数量。9、至此，就完成了全部的迁移任务。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465ruby redis-trib.rb reshard --from all --to 80b661ecca260c89e3d8ea9b98f77edaeef43dcd --slots 11 10.180.157.199:6379&gt;&gt;&gt; Performing Cluster Check (using node 10.180.157.199:6379)S: b2506515b38e6bbd3034d540599f4cd2a5279ad1 10.180.157.199:6379 slots: (0 slots) slave replicates 460b3a11e296aafb2615043291b7dd98274bb351S: d376aaf80de0e01dde1f8cd4647d5ac3317a8641 10.180.157.205:6379 slots: (0 slots) slave replicates e36c46dbe90960f30861af00786d4c2064e63df2M: 15126fb33796c2c26ea89e553418946f7443d5a5 10.180.157.201:6379 slots:10923-16383 (5461 slots) master 1 additional replica(s)S: 59fa6ee455f58a5076f6d6f83ddd74161fd7fb55 10.180.157.208:6379 slots: (0 slots) slave replicates 15126fb33796c2c26ea89e553418946f7443d5a5M: 460b3a11e296aafb2615043291b7dd98274bb351 10.180.157.202:6379 slots:0-5460 (5461 slots) master 1 additional replica(s)M: 80b661ecca260c89e3d8ea9b98f77edaeef43dcd 10.180.157.200:6380 slots: (0 slots) master 0 additional replica(s)M: e36c46dbe90960f30861af00786d4c2064e63df2 10.180.157.200:6379 slots:5461-10922 (5462 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.Ready to move 11 slots. Source nodes: M: 15126fb33796c2c26ea89e553418946f7443d5a5 10.180.157.201:6379 slots:10923-16383 (5461 slots) master 1 additional replica(s) M: 460b3a11e296aafb2615043291b7dd98274bb351 10.180.157.202:6379 slots:0-5460 (5461 slots) master 1 additional replica(s) M: e36c46dbe90960f30861af00786d4c2064e63df2 10.180.157.200:6379 slots:5461-10922 (5462 slots) master 1 additional replica(s) Destination node: M: 80b661ecca260c89e3d8ea9b98f77edaeef43dcd 10.180.157.200:6380 slots: (0 slots) master 0 additional replica(s) Resharding plan: Moving slot 5461 from e36c46dbe90960f30861af00786d4c2064e63df2 Moving slot 5462 from e36c46dbe90960f30861af00786d4c2064e63df2 Moving slot 5463 from e36c46dbe90960f30861af00786d4c2064e63df2 Moving slot 5464 from e36c46dbe90960f30861af00786d4c2064e63df2 Moving slot 0 from 460b3a11e296aafb2615043291b7dd98274bb351 Moving slot 1 from 460b3a11e296aafb2615043291b7dd98274bb351 Moving slot 2 from 460b3a11e296aafb2615043291b7dd98274bb351 Moving slot 10923 from 15126fb33796c2c26ea89e553418946f7443d5a5 Moving slot 10924 from 15126fb33796c2c26ea89e553418946f7443d5a5 Moving slot 10925 from 15126fb33796c2c26ea89e553418946f7443d5a5Do you want to proceed with the proposed reshard plan (yes/no)? yesMoving slot 5461 from 10.180.157.200:6379 to 10.180.157.200:6380:Moving slot 5462 from 10.180.157.200:6379 to 10.180.157.200:6380:Moving slot 5463 from 10.180.157.200:6379 to 10.180.157.200:6380:Moving slot 5464 from 10.180.157.200:6379 to 10.180.157.200:6380:Moving slot 0 from 10.180.157.202:6379 to 10.180.157.200:6380:Moving slot 1 from 10.180.157.202:6379 to 10.180.157.200:6380:Moving slot 2 from 10.180.157.202:6379 to 10.180.157.200:6380:Moving slot 10923 from 10.180.157.201:6379 to 10.180.157.200:6380:Moving slot 10924 from 10.180.157.201:6379 to 10.180.157.200:6380:Moving slot 10925 from 10.180.157.201:6379 to 10.180.157.200:6380: move_slot方法可以在线将一个slot的全部数据从源节点迁移到目的节点，fix、reshard、rebalance都需要调用该方法迁移slot。 move_slot接受下面几个参数， 123451、pipeline：设置一次从slot上获取多少个key。2、quiet：迁移会打印相关信息，设置quiet参数，可以不用打印这些信息。3、cold：设置cold，会忽略执行importing和migrating。4、dots：设置dots，则会在迁移过程打印迁移key数量的进度。5、update：设置update，则会更新内存信息，方便以后的操作。 move_slot流程如下： 123456781、如果没有设置cold，则对源节点执行cluster importing命令，对目的节点执行migrating命令。fix的时候有可能importing和migrating已经执行过来，所以此种场景会设置cold。2、通过cluster getkeysinslot命令，一次性获取远节点迁移slot的pipeline个key的数量.3、对这些key执行migrate命令，将数据从源节点迁移到目的节点。4、如果migrate出现异常，在fix模式下，BUSYKEY的异常，会使用migrate的replace模式再执行一次，BUSYKEY表示目的节点已经有该key了，replace模式可以强制替换目的节点的key。不是fix模式就直接返回错误了。5、循环执行cluster getkeysinslot命令，直到返回的key数量为0，就退出循环。6、如果没有设置cold，对每个节点执行cluster setslot命令，把slot赋给目的节点。7、如果设置update，则修改源节点和目的节点的slot信息。8、至此完成了迁移slot的流程。 rebalance平衡集群节点slot数量rebalance命令可以根据用户传入的参数平衡集群节点的slot数量，rebalance功能非常强大，可以传入的参数很多，以下是rebalance的参数列表和命令示例。 1234567891011121314151617181920rebalance host:port --weight --auto-weights --threshold --use-empty-masters --timeout --simulate --pipeline $ruby redis-trib.rb rebalance --threshold 1 --weight b31e3a2e=5 --weight 60b8e3a1=5 --use-empty-masters --simulate 10.180.157.199:6379下面也先一一解释下每个参数的用法：host:port：这个是必传参数，用来从一个节点获取整个集群信息，相当于获取集群信息的入口。--weight ：节点的权重，格式为node_id=weight，如果需要为多个节点分配权重的话，需要添加多个--weight 参数，即--weight b31e3a2e=5 --weight 60b8e3a1=5，node_id可为节点名称的前缀，只要保证前缀位数能唯一区分该节点即可。没有传递–weight的节点的权重默认为1。--auto-weights：这个参数在rebalance流程中并未用到。--threshold ：只有节点需要迁移的slot阈值超过threshold，才会执行rebalance操作。具体计算方法可以参考下面的rebalance命令流程的第四步。--use-empty-masters：rebalance是否考虑没有节点的master，默认没有分配slot节点的master是不参与rebalance的，设置--use-empty-masters可以让没有分配slot的节点参与rebalance。--timeout ：设置migrate命令的超时时间。--simulate：设置该参数，可以模拟rebalance操作，提示用户会迁移哪些slots，而不会真正执行迁移操作。--pipeline ：与reshar的pipeline参数一样，定义cluster getkeysinslot命令一次取出的key数量，不传的话使用默认值为10。 rebalance命令流程如下： 1234567891011121314151、load_cluster_info_from_node方法先加载集群信息。2、计算每个master的权重，根据参数--weight ，为每个设置的节点分配权重，没有设置的节点，则权重默认为1。3、根据每个master的权重，以及总的权重，计算自己期望被分配多少个slot。计算的方式为：总slot数量 * （自己的权重 / 总权重）。4、计算每个master期望分配的slot是否超过设置的阈值，即--threshold 设置的阈值或者默认的阈值。计算的方式为：先计算期望移动节点的阈值，算法为：(100-(100.0*expected/n.slots.length)).abs，如果计算出的阈值没有超出设置阈值，则不需要为该节点移动slot。只要有一个master的移动节点超过阈值，就会触发rebalance操作。5、如果触发了rebalance操作。那么就开始执行rebalance操作，先将每个节点当前分配的slots数量减去期望分配的slot数量获得balance值。将每个节点的balance从小到大进行排序获得sn数组。6、用dst_idx和src_idx游标分别从sn数组的头部和尾部开始遍历。目的是为了把尾部节点的slot分配给头部节点。sn数组保存的balance列表排序后，负数在前面，正数在后面。负数表示需要有slot迁入，所以使用dst_idx游标，正数表示需要有slot迁出，所以使用src_idx游标。理论上sn数组各节点的balance值加起来应该为0，不过由于在计算期望分配的slot的时候只是使用直接取整的方式，所以可能出现balance值之和不为0的情况，balance值之和不为0即为节点不平衡的slot数量，由于slot总数有16384个，不平衡数量相对于总数，基数很小，所以对rebalance流程影响不大。7、获取sn[dst_idx]和sn[src_idx]的balance值较小的那个值，该值即为需要从sn[src_idx]节点迁移到sn[dst_idx]节点的slot数量。8、接着通过compute_reshard_table方法计算源节点的slot如何分配到源节点列表。这个方法在reshard流程中也有调用，具体步骤可以参考reshard流程的第六步。9、如果是simulate模式，则只是打印出迁移列表。10、如果没有设置simulate，则执行move_slot操作，迁移slot，传入的参数为:quiet=&gt;true,:dots=&gt;false,:update=&gt;true。11、迁移完成后更新sn[dst_idx]和sn[src_idx]的balance值。如果balance值为0后，游标向前进1。12、直到dst_idx到达src_idx游标，完成整个rebalance操作。 add-node将新节点加入集群add-node命令可以将新节点加入集群，节点可以为master，也可以为某个master节点的slave。 1234567add-node new_host:new_port existing_host:existing_port --slave --master-id add-node有两个可选参数：--slave：设置该参数，则新节点以slave的角色加入集群--master-id：这个参数需要设置了--slave才能生效，--master-id用来指定新节点的master节点。如果不设置该参数，则会随机为节点选择master节点。 可以看下add-node命令的执行示例： 1234567891011121314151617181920$ruby redis-trib.rb add-node --slave --master-id dcb792b3e85726f012e83061bf237072dfc45f99 10.180.157.202:6379 10.180.157.199:6379&gt;&gt;&gt; Adding node 10.180.157.202:6379 to cluster 10.180.157.199:6379&gt;&gt;&gt; Performing Cluster Check (using node 10.180.157.199:6379)M: dcb792b3e85726f012e83061bf237072dfc45f99 10.180.157.199:6379 slots:0-5460 (5461 slots) master 0 additional replica(s)M: 464d740bf48953ebcf826f4113c86f9db3a9baf3 10.180.157.201:6379 slots:10923-16383 (5461 slots) master 0 additional replica(s)M: befa7e17b4e5f239e519bc74bfef3264a40f96ae 10.180.157.200:6379 slots:5461-10922 (5462 slots) master 0 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.&gt;&gt;&gt; Send CLUSTER MEET to node 10.180.157.202:6379 to make it join the cluster.Waiting for the cluster to join.&gt;&gt;&gt; Configure node as replica of 10.180.157.199:6379.[OK] New node added correctly. add-node流程如下： 123451、通过load_cluster_info_from_node方法转载集群信息，check_cluster方法检查集群是否健康。2、如果设置了--slave，则需要为该节点寻找master节点。设置了--master-id，则以该节点作为新节点的master，如果没有设置--master-id，则调用get_master_with_least_replicas方法，寻找slave数量最少的master节点。如果slave数量一致，则选取load_cluster_info_from_node顺序发现的第一个节点。load_cluster_info_from_node顺序的第一个节点是add-node设置的existing_host:existing_port节点，后面的顺序根据在该节点执行cluster nodes返回的结果返回的节点顺序。3、连接新的节点并与集群第一个节点握手。4、如果没设置–slave就直接返回ok，设置了–slave，则需要等待确认新节点加入集群，然后执行cluster replicate命令复制master节点。5、至此，完成了全部的增加节点的流程。 del-node从集群中删除节点del-node可以把某个节点从集群中删除。del-node只能删除没有分配slot的节点。删除命令传递两个参数： 12host:port：从该节点获取集群信息。node_id：需要删除的节点id。 del-node执行结果示例如下： 1234$ruby redis-trib.rb del-node 10.180.157.199:6379 d5f6d1d17426bd564a6e309f32d0f5b96962fe53&gt;&gt;&gt; Removing node d5f6d1d17426bd564a6e309f32d0f5b96962fe53 from cluster 10.180.157.199:6379&gt;&gt;&gt; Sending CLUSTER FORGET messages to the cluster...&gt;&gt;&gt; SHUTDOWN the node. del-node流程如下： 123451、通过load_cluster_info_from_node方法转载集群信息。2、根据传入的node id获取节点，如果节点没找到，则直接提示错误并退出。3、如果节点分配的slot不为空，则直接提示错误并退出。4、遍历集群内的其他节点，执行cluster forget命令，从每个节点中去除该节点。如果删除的节点是master，而且它有slave的话，这些slave会去复制其他master，调用的方法是get_master_with_least_replicas，与add-node没设置--master-id寻找master的方法一样。5、然后关闭该节点。 set-timeout设置集群节点间心跳连接的超时时间set-timeout用来设置集群节点间心跳连接的超时时间，单位是毫秒，不得小于100毫秒，因为100毫秒对于心跳时间来说太短了。该命令修改是节点配置参数cluster-node-timeout，默认是15000毫秒。通过该命令，可以给每个节点设置超时时间，设置的方式使用config set命令动态设置，然后执行config rewrite命令将配置持久化保存到硬盘。以下是示例： 12345678ruby redis-trib.rb set-timeout 10.180.157.199:6379 30000&gt;&gt;&gt; Reconfiguring node timeout in every cluster node...*** New timeout set for 10.180.157.199:6379*** New timeout set for 10.180.157.205:6379*** New timeout set for 10.180.157.201:6379*** New timeout set for 10.180.157.200:6379*** New timeout set for 10.180.157.208:6379&gt;&gt;&gt; New node timeout set. 5 OK, 0 ERR. call在集群全部节点上执行命令call命令可以用来在集群的全部节点执行相同的命令。call命令也是需要通过集群的一个节点地址，连上整个集群，然后在集群的每个节点执行该命令。 1234567$ruby redis-trib.rb call 10.180.157.199:6379 get key&gt;&gt;&gt; Calling GET key10.180.157.199:6379: MOVED 12539 10.180.157.201:637910.180.157.205:6379: MOVED 12539 10.180.157.201:637910.180.157.201:6379:10.180.157.200:6379: MOVED 12539 10.180.157.201:637910.180.157.208:6379: MOVED 12539 10.180.157.201:6379 import将外部redis数据导入集群import命令可以把外部的redis节点数据导入集群。导入的流程如下： 123456781、通过load_cluster_info_from_node方法转载集群信息，check_cluster方法检查集群是否健康。2、连接外部redis节点，如果外部节点开启了cluster_enabled，则提示错误。3、通过scan命令遍历外部节点，一次获取1000条数据。4、遍历这些key，计算出key对应的slot。5、执行migrate命令,源节点是外部节点,目的节点是集群slot对应的节点，如果设置了--copy参数，则传递copy参数，如果设置了--replace，则传递replace参数。6、不停执行scan命令，直到遍历完全部的key。7、至此完成整个迁移流程这中间如果出现异常，程序就会停止。没使用--copy模式，则可以重新执行import命令，使用--copy的话，最好清空新的集群再导入一次。 import命令更适合离线的把外部redis数据导入，在线导入的话最好使用更专业的导入工具，以slave的方式连接redis节点去同步节点数据应该是更好的方式。","categories":[{"name":"redis","slug":"redis","permalink":"https://blog.xiemx.com/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://blog.xiemx.com/tags/redis/"}],"author":"xiemx"},{"title":"redis-trib.rb构建redis集群","slug":"2017-02-27-redis-trib-rb-create-cluster","date":"2017-02-26T20:02:11.000Z","updated":"2019-10-19T08:47:57.645Z","comments":false,"path":"/2017/02/27/2017-02-27-redis-trib-rb-create-cluster/","link":"","permalink":"https://blog.xiemx.com/2017/02/27/2017-02-27-redis-trib-rb-create-cluster/","excerpt":"","text":"redis-cluster 使用官方的redis-trib.rb构建redis集群,要让redis cluster集群正常工作需要有三个主节点，因此我们在三台机器上部署6个节点，每台机器一个master一个slave。 环境：p-hsg-redis-1 192.168.10.81 7000/7001p-hsg-redis-2 192.168.10.82 7000/7001p-hsg-redis-3 192.168.10.83 7000/7001 部署流程： 1.安装redis－server 我这里已经有saltstack的自动化构建脚本，因此使用salt安装软件。具体redis安装方法不再赘述。salt-call state.sls redis.cluster 2.修改配置文件 123456789root@p-hsg-redis-1:/etc/redis# cat /etc/redis/7000.confprotected-mode nodaemonize yesport 7000cluster-enabled yescluster-config-file nodes-7000.confcluster-node-timeout 15000appendonly yesdir /var/lib/redis/7000 3.启动redis节点 1234root@p-hsg-redis-3:/etc/redis# ps -ef | grep redisroot 30431 1 0 15:12 ? 00:00:00 /usr/local/bin/redis-server *:7001 [cluster]root 30446 1 0 15:12 ? 00:00:00 /usr/local/bin/redis-server *:7000 [cluster]root 30450 11073 0 15:12 pts/1 00:00:00 grep --color=auto redis 4.创建集群 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950redis-trib.rb create --replicas 1 192.168.10.81:7001 192.168.10.81:7000 192.168.10.82:7000 192.168.10.82:7001 192.168.10.83:7000 192.168.10.83:7001&gt;&gt;&gt; Creating cluster&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Using 3 masters:192.168.10.81:7001192.168.10.82:7000192.168.10.83:7000Adding replica 192.168.10.82:7001 to 192.168.10.81:7001Adding replica 192.168.10.81:7000 to 192.168.10.82:7000Adding replica 192.168.10.83:7001 to 192.168.10.83:7000M: 5fd250591aa474d6370e1df547959c5895716192 192.168.10.81:7001 slots:0-5460 (5461 slots) masterS: ef8bf8a326894fee37a398d3f88de3120fc71ea5 192.168.10.81:7000 replicates dc62f19b8894db1816f7b92e3fe05b8244832d3eM: dc62f19b8894db1816f7b92e3fe05b8244832d3e 192.168.10.82:7000 slots:5461-10922 (5462 slots) masterS: eb2c7cc3cb56bb5ee124d80e353a91331d092042 192.168.10.82:7001 replicates 5fd250591aa474d6370e1df547959c5895716192M: 1773d96052f573361a13b5e9015f947b340d45cd 192.168.10.83:7000 slots:10923-16383 (5461 slots) masterS: 670f158355b93b4a7ac30d69dc3e1e8c4c484a9f 192.168.10.83:7001 replicates 1773d96052f573361a13b5e9015f947b340d45cdCan I set the above configuration? (type 'yes' to accept): yes&gt;&gt;&gt; Nodes configuration updated&gt;&gt;&gt; Assign a different config epoch to each node&gt;&gt;&gt; Sending CLUSTER MEET messages to join the clusterWaiting for the cluster to join...&gt;&gt;&gt; Performing Cluster Check (using node 192.168.10.81:7001)M: 5fd250591aa474d6370e1df547959c5895716192 192.168.10.81:7001 slots:0-5460 (5461 slots) master 1 additional replica(s)M: 1773d96052f573361a13b5e9015f947b340d45cd 192.168.10.83:7000 slots:10923-16383 (5461 slots) master 1 additional replica(s)S: ef8bf8a326894fee37a398d3f88de3120fc71ea5 192.168.10.81:7000 slots: (0 slots) slave replicates dc62f19b8894db1816f7b92e3fe05b8244832d3eS: 670f158355b93b4a7ac30d69dc3e1e8c4c484a9f 192.168.10.83:7001 slots: (0 slots) slave replicates 1773d96052f573361a13b5e9015f947b340d45cdM: dc62f19b8894db1816f7b92e3fe05b8244832d3e 192.168.10.82:7000 slots:5461-10922 (5462 slots) master 1 additional replica(s)S: eb2c7cc3cb56bb5ee124d80e353a91331d092042 192.168.10.82:7001 slots: (0 slots) slave replicates 5fd250591aa474d6370e1df547959c5895716192[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.","categories":[{"name":"redis","slug":"redis","permalink":"https://blog.xiemx.com/categories/redis/"}],"tags":[{"name":"cluster","slug":"cluster","permalink":"https://blog.xiemx.com/tags/cluster/"},{"name":"redis","slug":"redis","permalink":"https://blog.xiemx.com/tags/redis/"}],"author":"xiemx"},{"title":"magent构建memcache集群","slug":"2017-02-09-magent-create-memcache-cluster","date":"2017-02-08T23:02:18.000Z","updated":"2019-10-19T11:32:47.899Z","comments":false,"path":"/2017/02/09/2017-02-09-magent-create-memcache-cluster/","link":"","permalink":"https://blog.xiemx.com/2017/02/09/2017-02-09-magent-create-memcache-cluster/","excerpt":"","text":"magent功能相当于一个proxy调度器，通过一定的算法将数据存到不同后端。magent只是一个调度器，不处理数据同步问题。（另外一个由twitter开源的twemproxy，功能作用类似。可参考：twemproxy安装配置） 部署： memcached部署自行百度。 magent参考：repcache＋magent构建高可用memcache 启动3个Memcached进程,端口11221、11222、11223. 123# memcached -u xiemx -d -p 11221# memcached -u xiemx -d -p 11222# memcached -u xiemx -d -p 11223 启动magent进程，端口11211，11221、11222为master，11223为backup； 1magent -p 12000 -s 127.0.0.1:11221 -s127.0.0.1:11222 -b 127.0.0.1:11223 telnet 11211的magent，set key1和set key2，根据算法，key1/key2将被分配到11221/11222和11213端口的Memcached； 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758[root@memcache ~]# telnet 127.0.0.1 11211Trying 127.0.0.1...Connected to 127.0.0.1.Escape character is '^]'.statsmemcached agent v0.4matrix 1 -&gt; 127.0.0.1:11221, pool size 0matrix 2 -&gt; 127.0.0.1:11222, pool size 0ENDset key1 0 0 11STOREDset key2 0 0 12STOREDquitConnection closed by foreign host.[root@memcache ~]# telnet 127.0.0.1 11221Trying 127.0.0.1...Connected to 127.0.0.1.Escape character is '^]'.get key1ENDget key2VALUE key2 0 12ENDquitConnection closed by foreign host.[root@memcache ~]# telnet 127.0.0.1 11222Trying 127.0.0.1...Connected to 127.0.0.1.Escape character is '^]'.get key1VALUE key1 0 11ENDget key2ENDquitConnection closed by foreign host.[root@memcache ~]# telnet 127.0.0.1 11223Trying 127.0.0.1...Connected to 127.0.0.1.Escape character is '^]'.get key1VALUE key1 0 11ENDget key2VALUE key2 0 12ENDquitConnection closed by foreign host. 模拟11211、11212端口的Memcached挂掉 123456789101112131415161718192021[root@memcache ~]# ps -ef | grep memcachedroot 6589 1 0 01:25 ? 00:00:00 memcached -u xiemx -d -p 11221root 6591 1 0 01:25 ? 00:00:00 memcached -u xiemx -d -p 11222root 6593 1 0 01:25 ? 00:00:00 memcached -u xiemx -d -p 11223root 6609 6509 001:44 pts/0 00:00:00 grep memcached[root@memcache ~]# kill -9 6589[root@memcache ~]# kill -9 6591[root@memcache ~]# telnet 127.0.0.1 11211Trying 127.0.0.1...Connected to 127.0.0.1.Escape character is '^]'.get key1 VALUE key1 0 11ENDget key2VALUE key2 0 12ENDquitConnection closed by foreign host. 模拟11211、11212端口的Memcached重启复活(恢复后的memcache是没有数据的，需要逻辑处理启动后导入数据) 123456789101112[root@memcache ~]# memcached -m 1 -u root -d -l 127.0.0.1 -p 11221[root@memcache ~]# memcached -m 1 -u root -d -l 127.0.0.1 -p 11222[root@memcache ~]# telnet 127.0.0.1 11211Trying 127.0.0.1...Connected to 127.0.0.1.Escape character is '^]'.get key1ENDget key2ENDquitConnection closed by foreign host.","categories":[{"name":"memcached","slug":"memcached","permalink":"https://blog.xiemx.com/categories/memcached/"}],"tags":[{"name":"database","slug":"database","permalink":"https://blog.xiemx.com/tags/database/"},{"name":"cluster","slug":"cluster","permalink":"https://blog.xiemx.com/tags/cluster/"},{"name":"memcached","slug":"memcached","permalink":"https://blog.xiemx.com/tags/memcached/"}],"author":"xiemx"},{"title":"repcache＋magent构建高可用memcache","slug":"2017-02-09-repcache-magent","date":"2017-02-08T22:02:53.000Z","updated":"2019-10-19T08:34:23.760Z","comments":false,"path":"/2017/02/09/2017-02-09-repcache-magent/","link":"","permalink":"https://blog.xiemx.com/2017/02/09/2017-02-09-repcache-magent/","excerpt":"","text":"repcache+magnet magent：一款开源的Memcached代理服务器软件，功能和mysql-proxy类似。 repcache：日本开发的一款开源工具，使memcache能够做主从复制。可以通过patch包升级memcache，也可以下载包含replication的memcache版本 先将master/slave通过replication构建自动复制的主主，通过magent将K／V，写入到master和backup中，当master宕机时，magent将所有读写请求交给slave，slave等待master启动，默认情况下master宕机重启后内存中的数据回丢失，但由于repcache的自动主主，master启动时会自动从salve端复制所有数据。 安装部署123456789101112131415161718libevent: apt-get install -y libevent-devmagent： wget http://memagent.googlecode.com/files/magent-0.5.tar.gz tar zxvf magent-0.5.tar.gz /sbin/ldconfig sed -i \"s#LIBS = -levent#LIBS = -levent -lm#g\" Makefile make cp magent /usr/bin/magent repcache: wget http://downloads.sourceforge.net/repcached/memcached-1.2.8-repcached-2.2.tar.gz tar xf memcached-1.2.8-repcached-2.2.tar.gz cd memcached-1.2.8-repcached-2.2/ ./configure --prefix=/opt/repcached/ --enable-replication --program-transform-name=s/memcached/repcached/ make&amp;&amp;make install ln -s /opt/repcached/bin/repcached /usr/bin/ 编译错误汇总1234567891011121314151617181920212223242526272829### magent[root@test magent]# make gcc -Wall -g -O2 -I/usr/local/include -m64 -c -o magent.o magent.cmagent.c: In function ‘writev_list’:magent.c:729: error: ‘SSIZE_MAX’ undeclared (first use in this function)magent.c:729: error: (Each undeclared identifier is reported only oncemagent.c:729: error: for each function it appears in.)make: *** [magent.o] Error 1解决方法:[root@test magent]# vim ketama.h#ifndef SSIZE_MAX#define SSIZE_MAX 32767#endif#ifndef _KETAMA_H#define _KETAMA_H[root@test magent]# make gcc -Wall -O2 -g -c -o magent.o magent.cgcc -Wall -O2 -g -c -o ketama.o ketama.cgcc -Wall -O2 -g -o magent magent.o ketama.o -leventketama.o: In function `create_ketama':/opt/root/magent-0.5/ketama.c:399: undefined reference to `floorf'collect2: ld returned 1 exit statusmake: *** [magent] Error 1修改MakefileLIBS = -levent 为LIBS = -levent -lm#sed -i \"s#LIBS = -levent#LIBS = -levent -lm#g\" Makefile 123456789101112131415161718192021222324252627282930313233343536373839### repcached[root@test magent]# make make all-recursive make[1]: Entering directory `/usr/local/memcached' Making all in doc make[2]: Entering directory `/usr/local/memcached/doc' make[2]: Nothing to be done for `all'. make[2]: Leaving directory `/usr/local/memcached/doc' make[2]: Entering directory `/usr/local/memcached' gcc -DHAVE_CONFIG_H -I. -DNDEBUG -m64 -g -O2 -MT memcached-memcached.o -MD MP -MF .deps/memcached-memcached.Tpo -c -o memcached-memcached.o `test -f memcached.c' || echo './'`memcached.c memcached.c: In function ‘add_iov’: memcached.c:697: error: ‘IOV_MAX’ undeclared (first use in this function) memcached.c:697: error: (Each undeclared identifier is reported only once memcached.c:697: error: for each function it appears in.) make[2]: *** [memcached-memcached.o] Error 1 make[2]: Leaving directory `/usr/local/memcached' make[1]: *** [all-recursive] Error 1 make[1]: Leaving directory `/usr/local/memcached' make: *** [all] Error 2 解决方案： vi memcached.c 将下面的几行 /* FreeBSD 4.x doesn't have IOV_MAX exposed. */ #ifndef IOV_MAX #if defined(__FreeBSD__) || defined(__APPLE__) # define IOV_MAX 1024 #endif #endif 修改为/* FreeBSD 4.x doesn't have IOV_MAX exposed. */ #ifndef IOV_MAX # define IOV_MAX 1024 #endif 启动服务12345678910111213### 启动repcached启动master：repcached -d -v -x 127.0.0.1 -u vagrant 启动slave：repcached -d -v -x 127.0.0.1 -u vagrant -p 11222参数说明： -x 设置从哪个IP上进行同步。 -X 指定数据同步的端口。 Memcached默认服务端口是11211，默认同步监听端口是11212。### 启动magentmagent -u root -p 11233 -s 127.0.0.1:11211 -s 127.0.0.1:11222","categories":[{"name":"memcached","slug":"memcached","permalink":"https://blog.xiemx.com/categories/memcached/"}],"tags":[{"name":"memcached","slug":"memcached","permalink":"https://blog.xiemx.com/tags/memcached/"},{"name":"repcache","slug":"repcache","permalink":"https://blog.xiemx.com/tags/repcache/"},{"name":"magent","slug":"magent","permalink":"https://blog.xiemx.com/tags/magent/"}],"author":"xiemx"},{"title":"vagrant创建多个虚拟机","slug":"2017-02-09-vagrant-mulit-vm","date":"2017-02-08T22:02:30.000Z","updated":"2019-09-27T06:52:36.184Z","comments":false,"path":"/2017/02/09/2017-02-09-vagrant-mulit-vm/","link":"","permalink":"https://blog.xiemx.com/2017/02/09/2017-02-09-vagrant-mulit-vm/","excerpt":"","text":"建立多台VM，並且让他们之间能够相通信，一台是应用服务器、一台是DB服务器,参考如下 12345678910111213141516171819Vagrant.configure(\"2\") do |config| config.vm.define :web do |web| web.vm.provider \"virtualbox\" do |v| v.customize [\"modifyvm\", :id, \"--name\", \"web\", \"--memory\", \"512\"] end web.vm.box = \"base\" web.vm.hostname = \"xiemx-website\" web.vm.network :private_network, ip: \"10.11.1.1\" end config.vm.define :db do |db| db.vm.provider \"virtualbox\" do |v| v.customize [\"modifyvm\", :id, \"--name\", \"db\", \"--memory\", \"512\"] end db.vm.box = \"base\" db.vm.hostname = \"xiemx-db\" db.vm.network :private_network, ip: \"10.11.1.2\" endend","categories":[{"name":"vagrant","slug":"vagrant","permalink":"https://blog.xiemx.com/categories/vagrant/"}],"tags":[],"author":"xiemx"},{"title":"vagrant使用","slug":"2017-02-09-vagrant","date":"2017-02-08T21:02:54.000Z","updated":"2019-09-27T06:52:36.184Z","comments":false,"path":"/2017/02/09/2017-02-09-vagrant/","link":"","permalink":"https://blog.xiemx.com/2017/02/09/2017-02-09-vagrant/","excerpt":"","text":"删除文档，参考官方：https://www.vagrantup.com/docs/cli/","categories":[{"name":"vagrant","slug":"vagrant","permalink":"https://blog.xiemx.com/categories/vagrant/"}],"tags":[{"name":"vagrant","slug":"vagrant","permalink":"https://blog.xiemx.com/tags/vagrant/"}],"author":"xiemx"},{"title":"twemproxy配置文件详解","slug":"2017-02-09-twemproxy-configure","date":"2017-02-08T21:02:29.000Z","updated":"2019-10-18T08:48:24.161Z","comments":false,"path":"/2017/02/09/2017-02-09-twemproxy-configure/","link":"","permalink":"https://blog.xiemx.com/2017/02/09/2017-02-09-twemproxy-configure/","excerpt":"","text":"配置文件详解： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263listentwemproxy监听的端口。可以以ip:port或name:port的形式来书写。 hash可以选择的key值的hash算法：&gt; one_at_a_time&gt; md5&gt; crc16&gt; crc32(crc32 implementation compatible with libmemcached)&gt; crc32a(correct crc32 implementation as per the spec)&gt; fnv1_64&gt; fnv1a_64&gt; fnv1_32&gt; fnv1a_32&gt; hsieh&gt; murmur&gt; jenkins如果没选择，默认是fnv1a_64。hash_taghash_tag允许根据key的一个部分来计算key的hash值。hash_tag由两个字符组成，一个是hash_tag的开始，另外一个是hash_tag的结束，在hash_tag的开始和结束之间，是将用于计算key的hash值的部分，计算的结果会用于选择服务器。 例如：如果hash_tag被定义为”&#123;&#125;”，那么key值为&quot;user:&#123;user1&#125;:ids&quot;和&quot;user:&#123;user1&#125;:tweets&quot;的hash值都是基于”user1”，最终会被映射到相同的服务器。而&quot;user:user1:ids&quot;将会使用整个key来计算hash，可能会被映射到不同的服务器。 distribution存在ketama、modula和random3种可选的配置。其含义如下： (1)ketama ketama一致性hash算法，会根据服务器构造出一个hash ring，并为ring上的节点分配hash范围。ketama的优势在于单个节点添加、删除之后，会最大程度上保持整个群集中缓存的key值可以被重用。 (2)modula modula非常简单，就是根据key值的hash值取模，根据取模的结果选择对应的服务器。 (3)random random是无论key值的hash是什么，都随机的选择一个服务器作为key值操作的目标。 timeout单位是毫秒，是连接到server的超时值。默认是永久等待。 backlog监听TCP 的backlog（连接等待队列）的长度，默认是512。 preconnect是一个boolean值，指示twemproxy是否应该预连接pool中的server。默认是false。 redis是一个boolean值，用来识别到服务器的通讯协议是redis还是memcached。默认是false。 server_connections每个server可以被打开的连接数。默认，每个服务器开一个连接。 auto_eject_hosts是一个boolean值，用于控制twemproxy是否应该根据server的连接状态重建群集。这个连接状态是由server_failure_limit 阀值来控制。默认是false。 server_retry_timeout单位是毫秒，控制服务器连接的时间间隔，在auto_eject_host被设置为true的时候产生作用。默认是30000 毫秒。 server_failure_limit控制连接服务器的次数，在auto_eject_host被设置为true的时候产生作用。默认是2。 servers一个pool中的服务器的地址、端口和权重的列表，包括一个可选的服务器的名字，如果提供服务器的名字，将会使用它决定server的次序，从而提供对应的一致性hash的hash ring。否则，将使用server被定义的次序","categories":[{"name":"memcached","slug":"memcached","permalink":"https://blog.xiemx.com/categories/memcached/"},{"name":"redis","slug":"memcached/redis","permalink":"https://blog.xiemx.com/categories/memcached/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://blog.xiemx.com/tags/redis/"},{"name":"memcache","slug":"memcache","permalink":"https://blog.xiemx.com/tags/memcache/"},{"name":"twemproxy","slug":"twemproxy","permalink":"https://blog.xiemx.com/tags/twemproxy/"}],"author":"xiemx"},{"title":"twemproxy构建redis/memcache集群","slug":"2017-02-09-twemproxu","date":"2017-02-08T21:02:12.000Z","updated":"2019-10-18T08:43:51.186Z","comments":false,"path":"/2017/02/09/2017-02-09-twemproxu/","link":"","permalink":"https://blog.xiemx.com/2017/02/09/2017-02-09-twemproxu/","excerpt":"","text":"twemproxy构建memcache集群，也可以用来构建reids集群。方法类似只是配置文件不同。 官网描述： A fast, light-weight proxy for memcached and redis 安装配置123456789101112131415161718192021222324252627282930$ apt-get install libtool automake -y$ git clone https://github.com/twitter/twemproxy.git$ cd twemproxy$ autoreconf -fvi$ ./configure$ make$ src/nutcracker -hroot@vagrant-ubuntu-trusty-64:~# nutcracker -hThis is nutcracker-0.4.1Usage: nutcracker [-?hVdDt] [-v verbosity level] [-o output file] [-c conf file] [-s stats port] [-a stats addr] [-i stats interval] [-p pid file] [-m mbuf size]Options: -h, --help : this help -V, --version : show version and exit -t, --test-conf : test configuration for syntax errors and exit -d, --daemonize : run as a daemon -D, --describe-stats : print stats description and exit -v, --verbose=N : set logging level (default: 5, min: 0, max: 11) -o, --output=S : set logging file (default: stderr) -c, --conf-file=S : set configuration file (default: conf/nutcracker.yml) -s, --stats-port=N : set stats monitoring port (default: 22222) -a, --stats-addr=S : set stats monitoring ip (default: 0.0.0.0) -i, --stats-interval=N : set stats aggregation interval in msec (default: 30000 msec) -p, --pid-file=S : set pid file (default: off) -m, --mbuf-size=N : set size of mbuf chunk in bytes (default: 16384 bytes) 创建集群12345678910111213141516171819202122232425262728293031323334#用tweaproxy启动代理一组memcached／redis非常简单，只需要指定配置文件即可，官方也提供了范本在仓库的conf/下vagrant@memcache-server:~$ cat nutcracker.ymlmemcache: listen: 127.0.0.1:11211 hash: fnv1a_64 distribution: ketama auto_eject_hosts: true server_retry_timeout: 2000 server_failure_limit: 1 servers: - 127.0.0.1:11221:1 - 127.0.0.1:11222:1 - 127.0.0.1:11223:1#启动后端的memcachedvagrant@memcache-server:~$ ps -ef | grep memcachevagrant 17934 1 0 09:07 ? 00:00:00 memcached -d -p 11221vagrant 17942 1 0 09:07 ? 00:00:00 memcached -d -p 11222vagrant 17950 1 0 09:07 ? 00:00:00 memcached -d -p 11223vagrant 18295 18083 0 09:20 pts/0 00:00:00 grep --color=auto memcachevagrant@memcache-server:~$ nutcracker -c nutcracker.yml -dvagrant@memcache-server:~$ telnet localhost 11211Trying 127.0.0.1...Connected to localhost.Escape character is '^]'.set test 0 0 11STOREDget testVALUE test 0 11END","categories":[{"name":"memcached","slug":"memcached","permalink":"https://blog.xiemx.com/categories/memcached/"},{"name":"redis","slug":"memcached/redis","permalink":"https://blog.xiemx.com/categories/memcached/redis/"}],"tags":[{"name":"cluster","slug":"cluster","permalink":"https://blog.xiemx.com/tags/cluster/"},{"name":"redis","slug":"redis","permalink":"https://blog.xiemx.com/tags/redis/"},{"name":"memcached","slug":"memcached","permalink":"https://blog.xiemx.com/tags/memcached/"},{"name":"twemproxy","slug":"twemproxy","permalink":"https://blog.xiemx.com/tags/twemproxy/"}],"author":"xiemx"},{"title":"Vagrantfile配置","slug":"2017-02-09-vagrantfile","date":"2017-02-08T21:02:02.000Z","updated":"2019-09-27T06:52:36.185Z","comments":false,"path":"/2017/02/09/2017-02-09-vagrantfile/","link":"","permalink":"https://blog.xiemx.com/2017/02/09/2017-02-09-vagrantfile/","excerpt":"","text":"删除原始文档，参考官方文档：https://www.vagrantup.com/docs/vagrantfile/","categories":[{"name":"vagrant","slug":"vagrant","permalink":"https://blog.xiemx.com/categories/vagrant/"}],"tags":[{"name":"vagrant","slug":"vagrant","permalink":"https://blog.xiemx.com/tags/vagrant/"}],"author":"xiemx"},{"title":"vcenter扩容Linux虚拟机磁盘","slug":"2017-01-19-vcenter-expand-disk-to-linux","date":"2017-01-19T03:01:34.000Z","updated":"2019-09-27T06:52:36.184Z","comments":false,"path":"/2017/01/19/2017-01-19-vcenter-expand-disk-to-linux/","link":"","permalink":"https://blog.xiemx.com/2017/01/19/2017-01-19-vcenter-expand-disk-to-linux/","excerpt":"","text":"磁盘扩容100G1.vcenter扩容磁盘vcenter增加步骤参考：vcenter扩容window磁盘 2.Linux需要rescan磁盘，重新读到大小 1234方法1:reboot方法2:[root@localhost ~]# echo 1 &gt; /sys/block/sdc/device/rescan 3.fdisk创建新分区 注意创建新的磁盘后内核可能无法识别到分区表，需要运行partprobe刷新下分区表信息 4.lvm动态扩容lvm参考：lvm动态磁盘管理 完成后：","categories":[{"name":"vcenter","slug":"vcenter","permalink":"https://blog.xiemx.com/categories/vcenter/"}],"tags":[{"name":"vcenter","slug":"vcenter","permalink":"https://blog.xiemx.com/tags/vcenter/"}],"author":"xiemx"},{"title":"nginx server_name_hash_bucket_size","slug":"2017-01-08-nginx-server_name_hash_bucket_size","date":"2017-01-18T03:01:15.000Z","updated":"2019-10-19T10:18:22.426Z","comments":false,"path":"/2017/01/18/2017-01-08-nginx-server_name_hash_bucket_size/","link":"","permalink":"https://blog.xiemx.com/2017/01/18/2017-01-08-nginx-server_name_hash_bucket_size/","excerpt":"","text":"nginx默认的 server_name_hash_bucket_size 一般为32/64，如果配置了超长的server_name建议适当增大此参数 123http &#123; server_names_hash_bucket_size 512;&#125;","categories":[{"name":"nginx","slug":"nginx","permalink":"https://blog.xiemx.com/categories/nginx/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"https://blog.xiemx.com/tags/nginx/"}],"author":"xiemx"},{"title":"nginx防盗链配置","slug":"2017-01-18-nginx-referer","date":"2017-01-17T18:01:18.000Z","updated":"2019-10-19T10:12:29.779Z","comments":true,"path":"/2017/01/18/2017-01-18-nginx-referer/","link":"","permalink":"https://blog.xiemx.com/2017/01/18/2017-01-18-nginx-referer/","excerpt":"","text":"123456location ~* \\.(gif|jpg|jpeg|png|ico)$ &#123; valid_referers none blocked server_names *.xiemx.com; if ($invalid_referer) &#123; return 444 &#125;&#125;","categories":[{"name":"nginx","slug":"nginx","permalink":"https://blog.xiemx.com/categories/nginx/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"https://blog.xiemx.com/tags/nginx/"}],"author":"xiemx"},{"title":"docker privileged","slug":"2017-01-18-docker-privileged","date":"2017-01-17T17:01:49.000Z","updated":"2019-10-19T15:35:23.355Z","comments":false,"path":"/2017/01/18/2017-01-18-docker-privileged/","link":"","permalink":"https://blog.xiemx.com/2017/01/18/2017-01-18-docker-privileged/","excerpt":"","text":"docker 特权模式1234$ docker help run ...--privileged=false Give extended privileges to this container... 使用该参数，container内的root拥有真正的root权限。否则，container内的root只是外部的一个普通用户权限。privileged启动的容器，可以看到很多host上的设备，并且可以执行mount,iptables等操作。甚至允许你在docker容器中启动docker容器。 未设置privileged启动的容器： 1234567[root@localhost ~]# docker run -t -i centos:latest bash[root@65acccbba42f /]# ls /devconsole fd full fuse kcore null ptmx pts random shm stderr stdin stdout tty urandom zero[root@65acccbba42f /]# mkdir /home/test/[root@65acccbba42f /]# mkdir /home/test2/[root@65acccbba42f /]# mount -o bind /home/test /home/test2mount: permission denied 设置privileged启动的容器： 1234567891011121314[root@localhost ~]# docker run -t -i --privileged centos:latest bash[root@c39330902b45 /]# ls /dev/autofs dm-1 hidraw0 loop1 null ptp3 sg0 shm tty10 tty19 tty27 tty35 tty43 tty51 tty6 ttyS1 usbmon3 vcs5 vfiobsg dm-2 hidraw1 loop2 nvram pts sg1 snapshot tty11 tty2 tty28 tty36 tty44 tty52 tty60 ttyS2 usbmon4 vcs6 vga_arbiterbtrfs-control dm-3 hpet loop3 oldmem random sg2 snd tty12 tty20 tty29 tty37 tty45 tty53 tty61 ttyS3 usbmon5 vcsa vhost-netbus dm-4 input mapper port raw sg3 stderr tty13 tty21 tty3 tty38 tty46 tty54 tty62 uhid usbmon6 vcsa1 watchdogconsole dm-5 kcore mcelog ppp rtc0 sg4 stdin tty14 tty22 tty30 tty39 tty47 tty55 tty63 uinput vcs vcsa2 watchdog0cpu dm-6 kmsg mem ptmx sda sg5 stdout tty15 tty23 tty31 tty4 tty48 tty56 tty7 urandom vcs1 vcsa3 zerocpu_dma_latency fd kvm net ptp0 sda1 sg6 tty tty16 tty24 tty32 tty40 tty49 tty57 tty8 usbmon0 vcs2 vcsa4crash full loop-control network_latency ptp1 sda2 sg7 tty0 tty17 tty25 tty33 tty41 tty5 tty58 tty9 usbmon1 vcs3 vcsa5dm-0 fuse loop0 network_throughput ptp2 sda3 sg8 tty1 tty18 tty26 tty34 tty42 tty50 tty59 ttyS0 usbmon2 vcs4 vcsa6[root@c39330902b45 /]# mkdir /home/test/[root@c39330902b45 /]# mkdir /home/test2/[root@c39330902b45 /]# mount -o bind /home/test /home/test2","categories":[{"name":"docker","slug":"docker","permalink":"https://blog.xiemx.com/categories/docker/"}],"tags":[],"author":"xiemx"},{"title":"vcenter扩容window磁盘","slug":"2017-01-12-vcenter-expand-disk-to-windows","date":"2017-01-11T19:01:41.000Z","updated":"2019-09-27T06:52:36.184Z","comments":false,"path":"/2017/01/12/2017-01-12-vcenter-expand-disk-to-windows/","link":"","permalink":"https://blog.xiemx.com/2017/01/12/2017-01-12-vcenter-expand-disk-to-windows/","excerpt":"","text":"1.vcenter中增加磁盘大小，原本是80G现在扩容100G 2.Linux系统中重新扫描磁盘","categories":[{"name":"windows","slug":"windows","permalink":"https://blog.xiemx.com/categories/windows/"},{"name":"vcenter","slug":"windows/vcenter","permalink":"https://blog.xiemx.com/categories/windows/vcenter/"}],"tags":[{"name":"windows","slug":"windows","permalink":"https://blog.xiemx.com/tags/windows/"},{"name":"vcenter","slug":"vcenter","permalink":"https://blog.xiemx.com/tags/vcenter/"}],"author":"xiemx"},{"title":"zabbix监控redis","slug":"2016-12-19-zabbix-monitor-redis","date":"2016-12-18T21:12:43.000Z","updated":"2019-09-27T06:52:36.184Z","comments":false,"path":"/2016/12/19/2016-12-19-zabbix-monitor-redis/","link":"","permalink":"https://blog.xiemx.com/2016/12/19/2016-12-19-zabbix-monitor-redis/","excerpt":"","text":"可配合zabbix自动发现，来完成自动监控 scripts1234567891011121314151617181920212223242526cat redis.sh#!/bin/bashredis_conn=\"/usr/local/bin/redis-cli\"port=$1case $2 in \"used_memory\") used_memory=`$redis_conn -p $port info | grep used_memory:|awk -F':' '&#123;print $2&#125;'` echo $used_memory ;; \"ops_sec\") ops=`$redis_conn -p $port info|grep instantaneous_ops_per_sec:|awk -F':' '&#123;print $2&#125;'` echo $ops ;; \"connected_clients\") connected_clients=`$redis_conn -p $port info|grep connected_clients: | awk -F ':' '&#123;print $2&#125;'` echo $connected_clients ;; \"blocked_clients\") blocked_clients=`$redis_conn -p $port info|grep blocked_clients:|awk -F':' '&#123;print $2&#125;'` echo $blocked_clients ;; *) echo \"please input used_memory|ops_sec|connected_clients|blocked_clients\" ;; esac conf文件123cat zabbix_redis.confUserParameter=redis[*],/opt/script/zabbix/redis.sh $1 $2","categories":[{"name":"zabbix","slug":"zabbix","permalink":"https://blog.xiemx.com/categories/zabbix/"}],"tags":[{"name":"zabbix","slug":"zabbix","permalink":"https://blog.xiemx.com/tags/zabbix/"},{"name":"scripts","slug":"scripts","permalink":"https://blog.xiemx.com/tags/scripts/"}],"author":"xiemx"},{"title":"zabbix监控mongo","slug":"2016-12-19-zabbix-monitor-mongo","date":"2016-12-18T21:12:27.000Z","updated":"2019-09-27T06:52:36.187Z","comments":false,"path":"/2016/12/19/2016-12-19-zabbix-monitor-mongo/","link":"","permalink":"https://blog.xiemx.com/2016/12/19/2016-12-19-zabbix-monitor-mongo/","excerpt":"","text":"可配合zabbix自动发现，自动监控服务 script12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485cat mongo.sh#!/bin/bashcase $1 in # use_memory)# used_memory=`echo \"db.serverStatus().mem\"|mongo admin|grep resident|awk -F':' '&#123;print $2&#125;'|tr -d \" ,\"`# echo $used_memory# ;;# # use_vmemory)# used_vmemory=`echo \"db.serverStatus().mem\"|mongo admin|grep virtual|awk -F':' '&#123;print $2&#125;'|tr -d \" ,\"`# echo $used_vmemory# ;;# # used_conn)# used_conn=`echo \"db.serverStatus().connections\"|mongo admin|grep current|awk -F':' '&#123;print $2&#125;'|tr -d ' ,'`# echo $used_conn# ;;# # available_conn)# available=`echo \"db.serverStatus().connections\"|mongo admin|grep available|awk -F':' '&#123;print $2&#125;'|tr -d ' ,'`# echo $availabe# ;; insert) insert=`mongostat -n1 | tail -n 1|awk '&#123;print $1&#125;'|tr -d ' *,'` echo $insert ;; query) query=`mongostat -n1 | tail -n 1|awk '&#123;print $2&#125;'|tr -d ' *,'` echo $query ;; update) update=`mongostat -n1 | tail -n 1|awk '&#123;print $3&#125;'|tr -d ' *,'` echo $update ;; delete) delete=`mongostat -n1 | tail -n 1|awk '&#123;print $4&#125;'|tr -d ' *,'` echo $delete ;; getmore) getmore=`mongostat -n1 | tail -n 1|awk '&#123;print $5&#125;'|tr -d ' *,'` echo $getmore ;; command) command=`mongostat -n1 | tail -n 1|awk '&#123;print $6&#125;'|awk -F'|' '&#123;print $1&#125;'|tr -d ' *,'` echo $command ;; vsize) vsize=`mongostat -n1 | tail -n 1|awk '&#123;print $10&#125;'|tr -d ' *,G'` echo $vsize ;; res) res=`mongostat -n1 | tail -n 1|awk '&#123;print $11&#125;'|tr -d ' *,G'` echo $res ;; qr) qr=`mongostat -n1 | tail -n 1|awk '&#123;print $12&#125;'|awk -F'|' '&#123;print $1&#125;'|tr -d ' *,'` echo $qr ;; qw) qw=`mongostat -n1 | tail -n 1|awk '&#123;print $12&#125;'|awk -F'|' '&#123;print $2&#125;'|tr -d ' *,'` echo $qw ;; ar) ar=`mongostat -n1 | tail -n 1|awk '&#123;print $13&#125;'|awk -F'|' '&#123;print $1&#125;'|tr -d ' *,'` echo $ar ;; aw) aw=`mongostat -n1 | tail -n 1|awk '&#123;print $13&#125;'|awk -F'|' '&#123;print $2&#125;'|tr -d ' *,'` echo $aw ;; conn) conn=`mongostat -n1 | tail -n 1|awk '&#123;print $16&#125;'|tr -d ' *,'` echo $conn ;; *) echo \"please input insert|query|update|delete\" ;;esac conf配置文件12cat zabbix_mongo.confUserParameter=mongo[*],/opt/script/zabbix/mongo.sh $1","categories":[{"name":"zabbix","slug":"zabbix","permalink":"https://blog.xiemx.com/categories/zabbix/"}],"tags":[{"name":"zabbix","slug":"zabbix","permalink":"https://blog.xiemx.com/tags/zabbix/"},{"name":"mongo","slug":"mongo","permalink":"https://blog.xiemx.com/tags/mongo/"}],"author":"xiemx"},{"title":"zabbix监控memcache","slug":"2016-12-19-zabbix-monitor-memcache","date":"2016-12-18T21:12:02.000Z","updated":"2019-09-27T06:52:36.184Z","comments":false,"path":"/2016/12/19/2016-12-19-zabbix-monitor-memcache/","link":"","permalink":"https://blog.xiemx.com/2016/12/19/2016-12-19-zabbix-monitor-memcache/","excerpt":"","text":"script12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152cat memcached.sh#!/bin/bashport=$1mem_conn=\"/bin/nc 127.0.0.1 $port\"case $2 in conn) conn=`echo -e \"stats\\nquit\"|$mem_conn|grep curr_connections | awk '&#123;print $3&#125;' ` echo $conn ;; bytes) bytes=`echo -e \"stats\\nquit\"|$mem_conn|grep bytes|awk '&#123;print $3&#125;'` echo `echo $bytes |tr -d \" \"` ;; cmd_get) cmd_get=`echo -e \"stats\\nquit\"|$mem_conn|grep cmd_get| awk '&#123;print $3&#125;' ` echo $cmd_get ;; cmd_set) cmd_set=`echo -e \"stats\\nquit\"|$mem_conn|grep cmd_set| awk '&#123;print $3&#125;' ` echo $cmd_set ;; get_hits) get_hits=`echo -e \"stats\\nquit\"|$mem_conn|grep get_hits| awk '&#123;print $3&#125;' ` echo $get_hits ;; read_qps_sec) count1=`echo -e \"stats\\nquit\"|$mem_conn|grep cmd_get| awk '&#123;print $3&#125;'|tr -d '\\r' ` sleep 1 count2=`echo -e \"stats\\nquit\"|$mem_conn|grep cmd_get| awk '&#123;print $3&#125;'|tr -d '\\r' ` count=` expr $count2 - $count1` echo $count ;; write_qps_sec) count1=`echo -e \"stats\\nquit\"|$mem_conn|grep cmd_set| awk '&#123;print $3&#125;' ` sleep 1 count2=`echo -e \"stats\\nquit\"|$mem_conn|grep cmd_set| awk '&#123;print $3&#125;' ` count=`echo $count2 $count1|awk '&#123;printf($1-$2)&#125;'` echo $count ;; hit_target) cmd_get=`echo -e \"stats\\nquit\"|$mem_conn|grep cmd_get| awk '&#123;print $3&#125;' ` get_hits=`echo -e \"stats\\nquit\"|$mem_conn|grep get_hits| awk '&#123;print $3&#125;' ` hit_target=`echo $get_hits $cmd_get|awk '&#123;printf($1*100/$2)&#125;'` echo $hit_target ;; *) echo \"please input conn|bytes|cmd_get|cmd_set|get_hits|read_qps_sec|write_qps_sec|hit_target\" ;;esac 配置文件放到zabbix的conf.d/目录下，12cat zabbix_memcache.confUserParameter=memcache[*],/opt/script/zabbix/memcached.sh $1 $2","categories":[{"name":"zabbix","slug":"zabbix","permalink":"https://blog.xiemx.com/categories/zabbix/"}],"tags":[{"name":"zabbix","slug":"zabbix","permalink":"https://blog.xiemx.com/tags/zabbix/"},{"name":"monitor","slug":"monitor","permalink":"https://blog.xiemx.com/tags/monitor/"},{"name":"memcache","slug":"memcache","permalink":"https://blog.xiemx.com/tags/memcache/"}],"author":"xiemx"},{"title":"nginx websocket proxying","slug":"2016-12-14-nginx-websocket-proxying","date":"2016-12-13T17:12:43.000Z","updated":"2019-10-19T10:15:38.238Z","comments":false,"path":"/2016/12/14/2016-12-14-nginx-websocket-proxying/","link":"","permalink":"https://blog.xiemx.com/2016/12/14/2016-12-14-nginx-websocket-proxying/","excerpt":"","text":"server { ... location /chat/ { proxy_pass http://backend; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; } } 官方关于websocket proxying的文档：http://nginx.org/en/docs/http/websocket.html","categories":[{"name":"nginx","slug":"nginx","permalink":"https://blog.xiemx.com/categories/nginx/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"https://blog.xiemx.com/tags/nginx/"},{"name":"websocket","slug":"websocket","permalink":"https://blog.xiemx.com/tags/websocket/"}],"author":"xiemx"},{"title":"zabbix 监控ssl证书过期时间","slug":"2016-12-05-zabbix-monitor-ssl-cert","date":"2016-12-05T02:12:45.000Z","updated":"2019-09-27T06:52:36.187Z","comments":false,"path":"/2016/12/05/2016-12-05-zabbix-monitor-ssl-cert/","link":"","permalink":"https://blog.xiemx.com/2016/12/05/2016-12-05-zabbix-monitor-ssl-cert/","excerpt":"","text":"脚本是参考网上修改过的版本，注意要指定servername参数，以免抓取到其它站点的证书。 123456789mingxu.xie@t-slq-ops-1:/opt/script/zabbix# cat check_ssl_cert.sh #/bin/bashhost=1port=2end_date=openssl s_client -host $host -port $port -servername $host -showcerts &amp;lt;/dev/null 2&gt;/dev/null|sed -n '/BEGIN CERTIFICATE/,/END CERT/p' |openssl x509 -text 2&gt;/dev/null |sed -n 's/ *Not After : *//p'if [ -n \"end_date\" ];then end_date_seconds=`date '+%s' --date \"end_date\"now_seconds=date '+%s'` echo \"(end_date_seconds-now_seconds)/24/3600\" | bcfi","categories":[{"name":"zabbix","slug":"zabbix","permalink":"https://blog.xiemx.com/categories/zabbix/"}],"tags":[],"author":"xiemx"},{"title":"zabbix触发器函数","slug":"2016-12-05-zabbix-trigger-function","date":"2016-12-05T01:12:36.000Z","updated":"2019-09-27T06:52:36.183Z","comments":false,"path":"/2016/12/05/2016-12-05-zabbix-trigger-function/","link":"","permalink":"https://blog.xiemx.com/2016/12/05/2016-12-05-zabbix-trigger-function/","excerpt":"","text":"不再一一列出，参考官方文档 https://www.zabbix.com/documentation/4.0/manual/appendix/triggers/functions","categories":[{"name":"zabbix","slug":"zabbix","permalink":"https://blog.xiemx.com/categories/zabbix/"}],"tags":[{"name":"zabbix","slug":"zabbix","permalink":"https://blog.xiemx.com/tags/zabbix/"}],"author":"xiemx"},{"title":"Zabbix自定义Item Not Supported,页面出现不支持解决\n","slug":"2016-11-23-fix-zabbix-item-not-supported","date":"2016-11-22T20:11:59.000Z","updated":"2019-09-27T06:52:36.185Z","comments":false,"path":"/2016/11/23/2016-11-23-fix-zabbix-item-not-supported/","link":"","permalink":"https://blog.xiemx.com/2016/11/23/2016-11-23-fix-zabbix-item-not-supported/","excerpt":"","text":"使用Zabbix的时候往往会自定义Item。但是经常会遇到自定义的Item Not Supported了。Zabbix Agent默认的超时时间是3秒。往往我们自定义的Item由于各种原因返回时间会比较长。建议修改一个适合自己实际的值。 123456vim /etc/zabbix/zabbix_agent.conf#Range: 1-30Timeout=8###修改完毕后重启zabbix-agent/etc/init.d/zabbix-agent restart","categories":[{"name":"zabbix","slug":"zabbix","permalink":"https://blog.xiemx.com/categories/zabbix/"},{"name":"issue","slug":"zabbix/issue","permalink":"https://blog.xiemx.com/categories/zabbix/issue/"}],"tags":[{"name":"zabbix","slug":"zabbix","permalink":"https://blog.xiemx.com/tags/zabbix/"}],"author":"xiemx"},{"title":"zabbix-server.conf文件详解","slug":"2016-09-01-zabbix-server-configure","date":"2016-08-31T21:09:48.000Z","updated":"2019-09-27T06:52:36.187Z","comments":false,"path":"/2016/09/01/2016-09-01-zabbix-server-configure/","link":"","permalink":"https://blog.xiemx.com/2016/09/01/2016-09-01-zabbix-server-configure/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101AlertScriptsPath 默认值：/usr/local/share/zabbix/alertscripts 说明：告警脚本目录AllowRoot 默认值：0 说明：是否允许使用root启动，0:不允许，1:允许，默认情况下她会使用zabbix用户来启动zabbix进程，不推荐使用rootCacheSize 取值范围： 128K-8G 默认值：8M 说明：配置缓存，用于存储host，item，trigger数据，2.2.3版本之前最大支持2G，目前最大支持8G，一般用不了多少的。CacheUpdateFrequency 取值范围：1-3600 默认值：60 说明：多少秒更新一次配置缓存DBHost 默认值：localhost 说明：数据库主机地址DBName 默认值：无 必填：是DBPassword： 默认值：空 说明：数据库密码DBPort 取值范围：1024-65535 默认值:3306 说明：SQLite作为DB，这个选项请忽略，如果使用socket链接，也请忽略。DBSchema 说明：Schema名称. 用于 IBM DB2 、 PostgreSQL.DBSocket 默认值：/tmp/mysql.sock 说明：mysql sock文件路径DebugLevel 取值范围：0-5 默认值：3 说明: 指定debug级别 0 - 基本信息 1 - critical信息 2 - error信息 3 - warnings信息 4 - 调试日志，日志内容很多，慎重使用 5 - 用于调试web和vmware监控ExternalScripts 默认值： /usr/local/share/zabbix/externalscripts 说明： 外部脚本目录Fping6Location 默认值：/usr/sbin/fping6 说明：fping6路径，不懂fping的人可以百度一下，如果zabbix非root启动，请给fping6 SUIDFpingLocation 默认值：/usr/sbin/fping 说明:和上面的一样HistoryCacheSize 取值范围：128K-2G 默认值：8M 说明： 历史记录缓存大小，用于存储历史记录HistoryTextCacheSize 取值范围：128K-2G 默认值：16M 说明：文本类型历史记录的缓存大小，存储character, text 、log历史记录.HousekeepingFrequency 取值范围：0-24 默认值：1 说明：housekeep执行频率，默认每小时回去删除一些过期数据。如果server重启，那么30分钟之后才执行一次，接下来，每隔一小时在执行一次。Include 说明：include配置文件，可以使用正则表达式，例如：/usr/local/zabbix-2.4.4/conf/ttlsa.com/*.confJavaGateway 说明：Zabbix Java gateway的主机名，需要启动Java pollersJavaGatewayPort 取值范围：1024-32767 默认值：10052 Zabbix Java gateway监听端口ListenIP 默认值：0.0.0.0 说明：监听地址，留空则会在所有的地址上监听，可以监听多个IP地址，ip之间使用逗号分隔，例如：127.0.0.1,10.10.0.2ListenPort 取值范围：1024-32767 默认值：10051 说明：监听端口LoadModule 说明：加载模块，格式: LoadModule=，文件必须在指定的LoadModulePath目录下，如果需要加载多个模块，那么写多个即可。LoadModulePath 模块目录，参考上面LogFile 日志文件，例如：/data/logs/zabbix/zabbix-server.logLogFileSize 取值范围：0-1024 默认值：1 0表示禁用日志自动rotation，如果日志达到了限制，并且rotation失败，老日志文件将会被清空掉，重新生成一个新日志。LogSlowQueries 取值范围：0-3600000 默认值：0 多慢的数据库查询将会被记录，单位：毫秒，0表示不记录慢查询。只有在DebugLevel=3时，这个配置才有效。MaxHousekeeperDelete 取值范围： 0-1000000 默认值：5000 housekeeping一次删除的数据不能大于MaxHousekeeperDeletePidFile 默认值：/tmp/zabbix_server.pid PID文件ProxyConfigFrequency 取值范围：1-604800 默认值：3600 proxy被动模式下，server多少秒同步配置文件至proxy。ProxyDataFrequency 取值范围：1-3600 默认值:1 被动模式下，zabbix server间隔多少秒向proxy请求历史数据SenderFrequency 取值范围：5-3600 默认值：30 间隔多少秒，再尝试发送为发送的报警SNMPTrapperFile 默认值：/tmp/zabbix_traps.tmp SNMP trap发送到server的数据临时存放文件。SourceIP 出口IP地址SSHKeyLocation SSH公钥私钥路径SSLCertLocation SSL证书目录，用于web监控SSLKeyLocation SSL认证私钥路径、用于web监控SSLCALocation SSL认证,CA路径，如果为空，将会使用系统默认的CAStartDBSyncers 取值范围：1-100 默认值：4 预先foke DB Syncers的数量，1.8.5以前最大值为64StartDiscoverers 取值范围：0-250 默认值：1 pre-forked discoverers的数量，1.8.5版本以前最大可为255StartHTTPPollers 取值范围：0-1000 默认值：1 pre-forked HTTP pollers的数量，1.8.5以前最大255StartIPMIPollers 取值范围：0-1000 默认值：0 pre-forked IPMI pollers的数量，1.8.5之前，最大为255Timeout 取值范围：1-30 默认值：3 agent，snmp，external check的超时时间，单位为秒TmpDir 默认值：/tmpTrapperTimeout 取值范围：1-300 默认值：300 处理trapper数据的超时时间TrendCacheSize 取值范围：128K-2G 默认值：4M 历史数据缓存大小UnavailableDelay 取值范围：1-3600 默认值：60 间隔多少秒再次检测主机是否可用UnreachableDelay 取值范围：1-3600 默认值：15 间隔多少秒再次检测主机是否可达。UnreachablePeriod 取值范围：1-3600 默认值：45 检测到主机不可用，多久将它置为不可达User 默认值：zabbix 启动zabbix server的用户，在配置禁止root启动，并且当前shell用户是root得情况下有效。如果当前用户是xiemx，那么zabbix server的运行用户是xiemxValueCacheSize 取值范围：0,128K-64G 默认值：8M 0表示禁用，history value缓存大小，当缓存超标了，将会每隔5分钟往server日志里面记录。养成看日志的好习惯。","categories":[{"name":"zabbix","slug":"zabbix","permalink":"https://blog.xiemx.com/categories/zabbix/"}],"tags":[],"author":"xiemx"},{"title":"keepalived不支持IPVS","slug":"2016-08-24-keepalived-not-support-IPVS","date":"2016-08-23T21:08:21.000Z","updated":"2019-10-19T15:18:09.098Z","comments":false,"path":"/2016/08/24/2016-08-24-keepalived-not-support-IPVS/","link":"","permalink":"https://blog.xiemx.com/2016/08/24/2016-08-24-keepalived-not-support-IPVS/","excerpt":"","text":"编译keepalived是遇到keepalived无法支持ip_vs，情况如下： keepalived默认编译时是在/usr/src/linux下找内核源代码。默认安装的源码在/usr/src/kernels/$(uname -r)/目录下（如果没有这个目录可以安装下kernel-devel包)。 解决办法： 123ln -s /usr/src/kernels/`uname -r`/ /usr/src/linux或者编译时指定 --with-kernel-dir=/usr/src/kernels/`uname -r`/然后重新编译keepalived，现在官方的最新版已经很少出现这种情况，暂时不知道是不是版本问题。","categories":[{"name":"keepalived","slug":"keepalived","permalink":"https://blog.xiemx.com/categories/keepalived/"}],"tags":[{"name":"cluster","slug":"cluster","permalink":"https://blog.xiemx.com/tags/cluster/"},{"name":"keepalived","slug":"keepalived","permalink":"https://blog.xiemx.com/tags/keepalived/"}],"author":"xiemx"},{"title":"keepalived配置多实例","slug":"2016-08-24-keepalived-multi-server","date":"2016-08-23T20:08:05.000Z","updated":"2019-10-19T15:23:47.802Z","comments":false,"path":"/2016/08/24/2016-08-24-keepalived-multi-server/","link":"","permalink":"https://blog.xiemx.com/2016/08/24/2016-08-24-keepalived-multi-server/","excerpt":"","text":"安装keepalived 1、 下载 wget http://www.keepalived.org/software/keepalived-1.2.23.tar.gz 2、 解包 tar xf keepalived-1.2.23.tar.gz 3、 切换目录 cd keepalived-1.2.23 4、 配置 ./configure –prefix=/opt/keepalived 因为keepalived 运行在ipvs 之上，因此这两 个软件一定要安装在一个系统里面。如果configure 操作能正常进行，运行完毕后将有如下的汇总输出： 1234567891011Keepalived configuration------------------------Keepalived version : 1.2.23Compiler : gccCompiler flags : -g -O2Extra Lib : -lpopt -lssl -lcryptoUse IPVS Framework : YesIPVS sync daemon support : YesUse VRRP Framework : YesUse LinkWatch : NoUse Debug flags : No 5、 编译和安装make&amp;&amp;make install6、 环境配置 123456cd /opt/keepalived/mkdir /etc/keepalived/cp sbin/keepalived /usr/sbin/cp etc/keepalived/keepalived.conf /etc/keepalived/cp etc/sysconfig/keepalived /etc/sysconfig/cp etc/rc.d/init.d/keepalived /etc/init.d/ 7、启动keepalived查看是否正常 1234567[root@cluster-node1 keepalived]# /etc/init.d/keepalived startStarting keepalived (via systemctl): [ OK ][root@cluster-node1 keepalived]# ps -ef | grep keepalivedroot 4662 1 0 01:22 ? 00:00:00 keepalived -Droot 4664 4662 0 01:22 ? 00:00:00 keepalived -Droot 4665 4662 0 01:22 ? 00:00:00 keepalived -Droot 16435 2705 0 01:37 pts/0 00:00:00 grep --color=auto keepalived 8、修改配置文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@cluster-node1 keepalived]# cat /etc/keepalived/keepalived.conf ! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id node1 vrrp_skip_check_adv_addr vrrp_strict vrrp_garp_interval 0 vrrp_gna_interval 0&#125;vrrp_instance VI_1 &#123; state MASTER interface eno16777736 virtual_router_id 51 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 10.0.0.1/24 &#125;&#125;vrrp_instance VI_2 &#123; state BACKUP interface eno16777736 virtual_router_id 52 priority 50 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 10.0.1.1/24 &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@cluster-node2 keepalived]# cat /etc/keepalived/keepalived.conf ! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id node2 vrrp_skip_check_adv_addr vrrp_strict vrrp_garp_interval 0 vrrp_gna_interval 0&#125;vrrp_instance VI_1 &#123; state BACKUP interface eno16777736 virtual_router_id 51 priority 50 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 10.0.0.1/24 &#125;&#125;vrrp_instance VI_2 &#123; state MASTER interface eno16777736 virtual_router_id 52 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 10.0.1.1/24 &#125;&#125; 9、重启keepalived，查看vip是否正常绑定 12345678[root@cluster-node1 keepalived]# ip add | egrep \"0.1|1.1\" inet 127.0.0.1/8 scope host lo inet6 ::1/128 scope host inet 10.0.0.1/24 scope global eno16777736[root@cluster-node2 keepalived]# ip add | egrep \"0.1|1.1\" inet 127.0.0.1/8 scope host lo inet6 ::1/128 scope host inet 10.0.1.1/24 scope global eno16777736","categories":[{"name":"keepalived","slug":"keepalived","permalink":"https://blog.xiemx.com/categories/keepalived/"}],"tags":[{"name":"cluster","slug":"cluster","permalink":"https://blog.xiemx.com/tags/cluster/"},{"name":"keepalived","slug":"keepalived","permalink":"https://blog.xiemx.com/tags/keepalived/"}],"author":"xiemx"},{"title":"Tomcat修改网站根目录","slug":"2016-08-18-tomcat-change-webroot","date":"2016-08-17T20:08:40.000Z","updated":"2019-10-19T07:24:46.748Z","comments":true,"path":"/2016/08/18/2016-08-18-tomcat-change-webroot/","link":"","permalink":"https://blog.xiemx.com/2016/08/18/2016-08-18-tomcat-change-webroot/","excerpt":"","text":"1.tomcat原来的默认根目录是http://localhost:8080，如果想修改访问的根目录，可以这样： 找到tomcat的server.xml（在conf目录下），找到: 123&lt;Host name=\"localhost\" appBase=\"webapps\" unpackWARs=\"true\" autoDeploy=\"true\" xmlValidation=\"false\" xmlNamespaceAware=\"false\"&gt;&lt;/Host&gt; 在前插入: 1&lt;Context path=\"\" docBase=\"/home/tomcat/\" debug=\"0\"/&gt; 其中/home/tomcat/就是我想设置的网站根目录，然后重启tomcat。再次访问http://localhost:8080时，就是直接访问/home/tomcat/目录下的文件了。 2.tomcat的web.xml（在conf目录下），在该文件中找到 12345&lt;welcome-file-list&gt; &lt;welcome-file&gt;index.html&lt;/welcome-file&gt; &lt;welcome-file&gt;index.htm&lt;/welcome-file&gt; &lt;welcome-file&gt;index.jsp&lt;/welcome-file&gt;&lt;/welcome-file-list&gt; 这是tomcat默认的3个文件，当你输入指定路径后，tomcat会自动查找这3个页面。如果你想让tomcat自动找到自己的页面，比如main.jsp。可以修改上面信息为： 123456&lt;welcome-file-list&gt; &lt;welcome-file&gt;main.jsp&lt;/welcome-file&gt; &lt;welcome-file&gt;index.html&lt;/welcome-file&gt; &lt;welcome-file&gt;index.htm&lt;/welcome-file&gt; &lt;welcome-file&gt;index.jsp&lt;/welcome-file&gt;&lt;/welcome-file-list&gt;","categories":[{"name":"tomcat","slug":"tomcat","permalink":"https://blog.xiemx.com/categories/tomcat/"}],"tags":[{"name":"http","slug":"http","permalink":"https://blog.xiemx.com/tags/http/"},{"name":"tomcat","slug":"tomcat","permalink":"https://blog.xiemx.com/tags/tomcat/"},{"name":"webserver","slug":"webserver","permalink":"https://blog.xiemx.com/tags/webserver/"}],"author":"xiemx"},{"title":"watch重复执行某个命令","slug":"2016-08-11-shell-command-watch","date":"2016-08-10T20:08:29.000Z","updated":"2019-09-27T06:52:36.185Z","comments":false,"path":"/2016/08/11/2016-08-11-shell-command-watch/","link":"","permalink":"https://blog.xiemx.com/2016/08/11/2016-08-11-shell-command-watch/","excerpt":"","text":"当需要重复执行一个命令时，可使用watch 123456789101112131415161718192021222324➜ ~ watchUsage: watch [options] commandOptions: -b, --beep beep if command has a non-zero exit -c, --color interpret ANSI color and style sequences -d, --differences[=&lt;permanent&gt;] highlight changes between updates -e, --errexit exit if command has a non-zero exit -g, --chgexit exit when output from command changes -n, --interval &lt;secs&gt; seconds to wait between updates -p, --precise attempt run command in precise intervals -t, --no-title turn off header -x, --exec pass command to exec instead of \"sh -c\" -h, --help display this help and exit -v, --version output version information and exitFor more details see watch(1).###每秒执行一次ls命令watch -n 1 ls","categories":[{"name":"command","slug":"command","permalink":"https://blog.xiemx.com/categories/command/"}],"tags":[{"name":"command","slug":"command","permalink":"https://blog.xiemx.com/tags/command/"},{"name":"shell","slug":"shell","permalink":"https://blog.xiemx.com/tags/shell/"}],"author":"xiemx"},{"title":"zabbix Less than 25% free in the configuration cache解决","slug":"2016-08-09-zabbix-less-than-25-free-in-the-configuration-cache","date":"2016-08-08T19:08:20.000Z","updated":"2019-09-27T06:52:36.185Z","comments":false,"path":"/2016/08/09/2016-08-09-zabbix-less-than-25-free-in-the-configuration-cache/","link":"","permalink":"https://blog.xiemx.com/2016/08/09/2016-08-09-zabbix-less-than-25-free-in-the-configuration-cache/","excerpt":"","text":"在zabbix server默认配置下，出现告警 1Less than 25% free in the configuration cache 可增加zabbix配置缓存解决 123456vim zabbix_server.conf##将缓存从8M提升到16M，可以调到最高8GCacheSize=16M##重启zabbix serverservice zabbix_server restart","categories":[{"name":"zabbix","slug":"zabbix","permalink":"https://blog.xiemx.com/categories/zabbix/"}],"tags":[{"name":"zabbix","slug":"zabbix","permalink":"https://blog.xiemx.com/tags/zabbix/"},{"name":"issue","slug":"issue","permalink":"https://blog.xiemx.com/tags/issue/"}],"author":"xiemx"},{"title":"mysql锁表解决","slug":"2016-08-02-mysql-lock-table","date":"2016-08-01T18:08:56.000Z","updated":"2019-10-19T10:25:02.821Z","comments":false,"path":"/2016/08/02/2016-08-02-mysql-lock-table/","link":"","permalink":"https://blog.xiemx.com/2016/08/02/2016-08-02-mysql-lock-table/","excerpt":"","text":"1、查询是否锁表 1show OPEN TABLES where In_use &gt; 0; 2、查询进程 1show processlist 查询到相对应的ID 3、杀死进程 1kill id other: 12345查看正在锁的事务SELECT * FROMINFORMATION_SCHEMA.INNODB_LOCKS; 查看等待锁的事务SELECT * FROMINFORMATION_SCHEMA.INNODB_LOCK_WAITS;","categories":[{"name":"mysql","slug":"mysql","permalink":"https://blog.xiemx.com/categories/mysql/"}],"tags":[{"name":"database","slug":"database","permalink":"https://blog.xiemx.com/tags/database/"},{"name":"debug","slug":"debug","permalink":"https://blog.xiemx.com/tags/debug/"},{"name":"mysql","slug":"mysql","permalink":"https://blog.xiemx.com/tags/mysql/"}],"author":"xiemx"},{"title":"域名状态含义","slug":"2016-07-27-domain-status-code","date":"2016-07-27T03:07:26.000Z","updated":"2019-09-27T06:52:36.186Z","comments":false,"path":"/2016/07/27/2016-07-27-domain-status-code/","link":"","permalink":"https://blog.xiemx.com/2016/07/27/2016-07-27-domain-status-code/","excerpt":"","text":"新注册的域名，可能出现以下状态： 域名状态 含义 addPeriod 注册局设置域名新注册期（域名新注册 5 天内会出现的状态，不影响域名使用，5 天后自动解除）。 ·ok 普通状态（可正常使用。没有需要立即进行的操作，也没有设置任何保护措施。有其他状态时，OK 状态不显示，但并不代表不正常。 出于对域名注册信息的保护，域名在进行某些安全锁定后，会出现以下状态： 域名状态 含义 ·clientDeleteProhibited 注册商设置禁止删除（保护域名的一种状态，域名不能被删除）。 serverDeleteProhibited 注册局设置禁止删除（保护域名的一种状态，域名不能被删除）。 ·clientUpdateProhibited 注册商设置禁止更新（域名信息，包括注册人/管理联系人/技术联系人/付费联系人/DNS 等不能被修改，但可设置或修改解析记录）。 ·serverUpdateProhibited 注册局设置禁止更新（域名信息，包括注册人/管理联系人/技术联系人/付费联系人/DNS 等不能被修改，但可设置或修改解析记录）。 ·clientTransferProhibited 注册商设置禁止转移（保护域名的一种状态，域名不能转移注册商）。 ·serverTransferProhibited 注册局设置禁止转移（保护域名的一种状态，域名不能转移注册商。有的域名新注册及转移注册商 60 天内会被注册局设置成该状态，60 天后自动解除；有的则为域名涉及仲裁或诉讼案被注册局设置，仲裁或诉讼案结束会被解除）。 其他禁止解析、禁止续费的状态： 域名状态 含义 ·pendingVerification 注册信息审核期（该域名注册后未进行实名审核，需尽早在域名付费后 5 天内提交资料审核，5 天后仍未实名审核的，将进入 ServerHold 状态）。 ·clientHold 注册商设置暂停解析（处于该状态域名解析暂停，需联系注册商解除该状态）。 ·serverHold 注册局设置暂停解析（处于该状态域名解析暂停，.cn 国内中英文域名注册成功后未通过实名审核时多出现该种状态，需在域名有效期内完成实名审核后解除）。 ·inactive 非激活状态（注册时未填写域名 DNS，不能进行解析，需在注册商处设置域名 DNS）。 ·clientRenewProhibited/serverRenewProhibited 注册商或注册局设置禁止续费（域名不能续费，处于该状态通常表示该域名处于仲裁或法院争议期，需联系注册商确认原因）。 ·pendingTransfer 注册局设置转移过程中（域名正在转移注册商过程中）。 ·redemptionPeriod 注册局设置赎回期（域名处于赎回期，可联系注册商高价赎回）。 ·pendingDelete 注册局设置待删除/赎回期（对于国际域名，该状态表示域名已过赎回期等待被删除，删除后开放重新注册；对于国内域名，该状态表示域名处于赎回期，可联系注册商高价赎回）。","categories":[{"name":"domain","slug":"domain","permalink":"https://blog.xiemx.com/categories/domain/"}],"tags":[{"name":"domain","slug":"domain","permalink":"https://blog.xiemx.com/tags/domain/"},{"name":"statusCode","slug":"statusCode","permalink":"https://blog.xiemx.com/tags/statusCode/"}],"author":"xiemx"},{"title":"uwsgi 报错 invalid option --“x” getopt_long() error","slug":"2016-07-26-uwsgi-invalid-option-x-getopt_long-error","date":"2016-07-26T03:07:34.000Z","updated":"2019-09-27T06:52:36.184Z","comments":false,"path":"/2016/07/26/2016-07-26-uwsgi-invalid-option-x-getopt_long-error/","link":"","permalink":"https://blog.xiemx.com/2016/07/26/2016-07-26-uwsgi-invalid-option-x-getopt_long-error/","excerpt":"","text":"报错：12uwsgi: invalid option — ‘x’getopt_long() error 解决这个问题是因为编译uwsgi的时候少了libxml2库导致的，应该先安装库再编译，否则会少了xml的支持。 uninstall uwsgi install libxml* install uwsgi","categories":[{"name":"uwsgi","slug":"uwsgi","permalink":"https://blog.xiemx.com/categories/uwsgi/"},{"name":"python","slug":"uwsgi/python","permalink":"https://blog.xiemx.com/categories/uwsgi/python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://blog.xiemx.com/tags/python/"},{"name":"uwsgi","slug":"uwsgi","permalink":"https://blog.xiemx.com/tags/uwsgi/"},{"name":"issue","slug":"issue","permalink":"https://blog.xiemx.com/tags/issue/"}],"author":"xiemx"},{"title":"nginx+uwsgi部署django项目","slug":"2016-07-26-nginx-uwsgi-deploy-django","date":"2016-07-25T22:07:48.000Z","updated":"2019-10-19T10:05:20.687Z","comments":false,"path":"/2016/07/26/2016-07-26-nginx-uwsgi-deploy-django/","link":"","permalink":"https://blog.xiemx.com/2016/07/26/2016-07-26-nginx-uwsgi-deploy-django/","excerpt":"","text":"安装基本软件12sudo apt-get install python-dev nginxpip install uwsgi 配置uwsgi和django的集成1234567891011vim test.py 创建test.py,添加如下代码def application(env, start_response): start_response('200 OK', [('Content-Type','text/html')]) return \"Hello World\"然后执行shell命令：uwsgi –http :8001 –wsgi-file test.py访问网页：curl http://127.0.0.1:8001/Hello World #编写django_wsgi.py文件，将其放在与文件manage.py同一个目录下： 12345678910111213### vim django_wsgi.py 添加如下代码：#!/usr/bin/env python# coding: utf-8import osimport sys# 将系统的编码设置为UTF8reload(sys)sys.setdefaultencoding('utf8')os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"yoursite.settings\")from django.core.handlers.wsgi import WSGIHandlerapplication = WSGIHandler() 连接django和uwsgi，实现简单的WEB服务器。我们假设你的Django项目的地址是/home/work/src/sites/testdjango1/testdjango/mysite， 然后，就可以执行以下命令：uwsgi –http :8000 –chdir /home/work/src/sites/testdjango1/testdjango/mysite –module django_wsgi这样，你就可以在浏览器中访问你的Django程序了。所有的请求都是经过uwsgi传递给Django程序的。 集成django,uwsgi和nginx部署： 在django项目根目录创建启动uwsgi的xml文件： 1234567&lt;uwsgi&gt; &lt;socket&gt;:8077&lt;/socket&gt; &lt;chdir&gt;/home/work/src/sites/testdjango1/testdjango/mysite&lt;/chdir&gt; &lt;module&gt;django_wsgi&lt;/module&gt; &lt;processes&gt;4&lt;/processes&gt; &lt;!-- 进程数 --&gt; &lt;daemonize&gt;uwsgi.log&lt;/daemonize&gt;&lt;/uwsgi&gt; 配置Nginx服务器： 1234567891011121314151617181920212223242526272829303132333435363738394041#备份nginx配置文件：sudo cp /etc/nginx/sites-available/default /etc/nginx/sites-available/default.bakvim /etc/nginx/sites-available/default 修改如下：server &#123; listen 80; server_name localhost; access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; #charset koi8-r; #access_log logs/host.access.log main; location / &#123; include uwsgi_params; uwsgi_pass 127.0.0.1:8077; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; #method first location ~ ^/static/ &#123; root /home/hz/PycharmProjects/myscrapy/check_ip/; expires 24h; access_log off; &#125; #method second #location ~* ^.+\\.(css|js|json|ttf|woff|map|woff2)$&#123; # root /home/hz/PycharmProjects/myscrapy/check_ip/; # access_log off; # expires 24h; #&#125; &#125; 如果不能访问日志文件，修改相关文件的权限即可 验证测试各步骤结果 123456789重启Nginx服务器，以使Nginx的配置生效。nginx -s reload重启后检查Nginx日志是否有异常。启动uWSGI服务器cd /home/work/src/sites/testdjango1/testdjango/mysiteuwsgi -x djangochina_socket.xml django搜集静态文件 1234settings.py文件中指定STATIC_ROOT路径python manage.py collectstatic修改nginx配置文件指定目录到STATIC_ROOT","categories":[{"name":"python","slug":"python","permalink":"https://blog.xiemx.com/categories/python/"},{"name":"uwsgi","slug":"python/uwsgi","permalink":"https://blog.xiemx.com/categories/python/uwsgi/"}],"tags":[{"name":"python","slug":"python","permalink":"https://blog.xiemx.com/tags/python/"},{"name":"uwsgi","slug":"uwsgi","permalink":"https://blog.xiemx.com/tags/uwsgi/"},{"name":"django","slug":"django","permalink":"https://blog.xiemx.com/tags/django/"},{"name":"nginx","slug":"nginx","permalink":"https://blog.xiemx.com/tags/nginx/"}],"author":"xiemx"},{"title":"linux自带的限制策略","slug":"2016-07-11-linux-limit-tools","date":"2016-07-11T01:07:34.000Z","updated":"2019-10-19T12:22:44.475Z","comments":false,"path":"/2016/07/11/2016-07-11-linux-limit-tools/","link":"","permalink":"https://blog.xiemx.com/2016/07/11/2016-07-11-linux-limit-tools/","excerpt":"","text":"[root@iZ11eqvuvnqZ ~]# cat /etc/security/limits.conf # /etc/security/limits.conf # #This file sets the resource limits for the users logged in via PAM. #It does not affect resource limits of the system services. # #Also note that configuration files in /etc/security/limits.d directory, #which are read in alphabetical order, override the settings in this #file in case the domain is the same or more specific. #That means for example that setting a limit for wildcard domain here #can be overriden with a wildcard setting in a config file in the #subdirectory, but a user specific setting here can be overriden only #with a user specific setting in the subdirectory. # #Each line describes a limit for a user in the form: # #domain&gt; type&gt; item&gt; value&gt; # #Where: #domain&gt; can be: # - a user name # - a group name, with @group syntax # - the wildcard *, for default entry # - the wildcard %, can be also used with %group syntax, # for maxlogin limit # #type&gt; can have the two values: # - &quot;soft&quot; for enforcing the soft limits # - &quot;hard&quot; for enforcing hard limits # #item&gt; can be one of the following: # - core - limits the core file size (KB) # - data - max data size (KB) # - fsize - maximum filesize (KB) # - memlock - max locked-in-memory address space (KB) # - nofile - max number of open files # - rss - max resident set size (KB) # - stack - max stack size (KB) # - cpu - max CPU time (MIN) # - nproc - max number of processes # - as - address space limit (KB) # - maxlogins - max number of logins for this user # - maxsyslogins - max number of logins on the system # - priority - the priority to run user process with # - locks - max number of file locks the user can hold # - sigpending - max number of pending signals # - msgqueue - max memory used by POSIX message queues (bytes) # - nice - max nice priority allowed to raise to values: [-20, 19] # - rtprio - max realtime priority # #domain&gt; &amp;lt;type&gt; &amp;lt;item&gt; &amp;lt;value&gt; # #* soft core 0 #* hard rss 10000 #@student hard nproc 20 #@faculty soft nproc 20 #@faculty hard nproc 50 #ftp hard nproc 0 #@student - maxlogins 4 # End of file * soft nofile 65535 * hard nofile 65535","categories":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/tags/linux/"},{"name":"command","slug":"command","permalink":"https://blog.xiemx.com/tags/command/"}],"author":"xiemx"},{"title":"PHP_CURL 使用代理访问服务器","slug":"2016-07-06-php_curl","date":"2016-07-05T19:07:24.000Z","updated":"2019-10-19T09:53:04.909Z","comments":false,"path":"/2016/07/06/2016-07-06-php_curl/","link":"","permalink":"https://blog.xiemx.com/2016/07/06/2016-07-06-php_curl/","excerpt":"","text":"12345678910111213141516171819202122#使用代码时请加上php文件的括号function curl_string ($url,$user_agent,$proxy)&#123; $ch = curl_init(); curl_setopt ($ch, CURLOPT_PROXY, $proxy); curl_setopt ($ch, CURLOPT_URL, $url); curl_setopt ($ch, CURLOPT_USERAGENT, $user_agent); curl_setopt ($ch, CURLOPT_HEADER, 1); curl_setopt ($ch, CURLOPT_RETURNTRANSFER, 1); curl_setopt ($ch, CURLOPT_FOLLOWLOCATION, 1); curl_setopt ($ch, CURLOPT_TIMEOUT, 120); $result = curl_exec ($ch); curl_close($ch); return $result;&#125;$url_page = \"http://www.google.com\";$user_agent = \"Mozilla/4.0\";$proxy = \"http://192.11.222.124:8000\";$string = curl_string($url_page,$user_agent,$proxy);echo $string;","categories":[{"name":"php","slug":"php","permalink":"https://blog.xiemx.com/categories/php/"}],"tags":[{"name":"php","slug":"php","permalink":"https://blog.xiemx.com/tags/php/"}],"author":"xiemx"},{"title":"zabbix微信报警脚本","slug":"2016-06-29-zabbix-alert-to-wechat-script","date":"2016-06-29T02:06:55.000Z","updated":"2019-09-27T06:52:36.185Z","comments":false,"path":"/2016/06/29/2016-06-29-zabbix-alert-to-wechat-script/","link":"","permalink":"https://blog.xiemx.com/2016/06/29/2016-06-29-zabbix-alert-to-wechat-script/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142#!/usr/bin/python__author__ = 'xiemx'import sysimport json,requestsimport osimport loggingclass Weixin(object): def get_token(self): CorpID = '-------4fa4' Secret = 'Aew6oxx-----------FaTClkjXlmw_zH' token_url = 'https://qyapi.weixin.qq.com/cgi-bin/gettoken?corpid=&#123;&#125;&amp;corpsecret=&#123;&#125;'.format(CorpID, Secret) response = requests.get(token_url, verify=False).content p = json.loads(response) token = p['access_token'] return token def send_msg(self, user_id, msg): token = self.get_token() url = 'https://qyapi.weixin.qq.com/cgi-bin/message/send?access_token=&#123;&#125;'.format(token) send_content=&#123; \"touser\": user_id, \"msgtype\": \"text\", \"agentid\": \"2\", \"text\": &#123; \"content\": msg &#125;, \"safe\":\"0\" &#125; p = requests.post(url, verify=False, data=json.dumps(send_content)) print p.content logging.debug(\"weixin send success\")if __name__ == \"__main__\": user_id = sys.argv[1] msg = sys.argv[2] + '\\n' + sys.argv[3] weixin = Weixin() weixin.send_msg(user_id, msg)","categories":[{"name":"zabbix","slug":"zabbix","permalink":"https://blog.xiemx.com/categories/zabbix/"}],"tags":[],"author":"xiemx"},{"title":"zabbix监控指定端口","slug":"2016-06-29-zabbix-monitor-port","date":"2016-06-29T02:06:32.000Z","updated":"2019-09-27T06:52:36.185Z","comments":false,"path":"/2016/06/29/2016-06-29-zabbix-monitor-port/","link":"","permalink":"https://blog.xiemx.com/2016/06/29/2016-06-29-zabbix-monitor-port/","excerpt":"","text":"1.登陆zabbix主界面 选择：配置-模板 选择模板组，这里我选择的是Template App Agentless，原因该自带模板组内包含各种常用服务的模板 单击该模板组 项目 点击右上角的 创建项目 这里添加的是mysql服务，按照如图配置 单击下方 存档 保存。 如图点击 触发器 如图，点击右上方 创建触发器 按照如图配置 点击 存档 保存，完毕。 以上是将自定义模板服务添加到Template App Agentless模板组 最后将Template App Agentless模板组添加到你需要监控的客户端主机内","categories":[{"name":"zabbix","slug":"zabbix","permalink":"https://blog.xiemx.com/categories/zabbix/"}],"tags":[],"author":"xiemx"},{"title":"shell 变量特殊操作","slug":"2016-06-29-shell-advance-variable","date":"2016-06-29T02:06:08.000Z","updated":"2019-10-19T08:09:37.989Z","comments":false,"path":"/2016/06/29/2016-06-29-shell-advance-variable/","link":"","permalink":"https://blog.xiemx.com/2016/06/29/2016-06-29-shell-advance-variable/","excerpt":"","text":"替换运算符1234$&#123;var:-word&#125; 如果var存在且非null，返回它的值；否则返回word$&#123;var:=word&#125; 如果var存在且非null，返回它的值；否则将word赋值给var，并返回var的值$&#123;var:?word&#125; 如果var存在且非null，返回它的值；否则显示var:word$&#123;var:+word&#125; 如果var存在且非null，返回word；否则返回null 模式匹配运算符1234$&#123;var#pattern&#125; 匹配前缀（最小匹配），并返回余下内容$&#123;var##pattern&#125; 匹配前缀（最大匹配），并返回余下内容$&#123;var%pattern&#125; 匹配结尾（最小匹配），并返回余下内容$&#123;var%%pattern&#125; 匹配结尾（最大匹配），并返回余下内容 注：pattern为正则表达式匹配","categories":[{"name":"shell","slug":"shell","permalink":"https://blog.xiemx.com/categories/shell/"},{"name":"linux","slug":"shell/linux","permalink":"https://blog.xiemx.com/categories/shell/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/tags/linux/"},{"name":"shell","slug":"shell","permalink":"https://blog.xiemx.com/tags/shell/"}],"author":"xiemx"},{"title":"shell 特殊变量","slug":"2016-06-29-shell-special-env-variable","date":"2016-06-29T01:06:36.000Z","updated":"2019-10-19T08:06:22.355Z","comments":true,"path":"/2016/06/29/2016-06-29-shell-special-env-variable/","link":"","permalink":"https://blog.xiemx.com/2016/06/29/2016-06-29-shell-special-env-variable/","excerpt":"","text":"特殊变量 12345678910111213141516$# 表示变量的个数，常用于循环$@ 当前命令行所有参数$* 当前命令行所有参数，将命令行所有参数当一个单独参数获取$? 表示上一个命令退出的状态$$ 表示当前进程编号$0 表示当前程序名称$! 表示最近一个后台命令的进程编号$IFS 表示内部的字段分隔符$?的参考值0 成功退出&gt;0 退出失败1-125 命令退出失败，失败返回的相关值由程序定义（譬如，程序内退出只执行 exit 2，则返回为2）126 命令找到了，但无法执行127 命令找不到&gt;128 命令因受到信号而死亡","categories":[{"name":"shell","slug":"shell","permalink":"https://blog.xiemx.com/categories/shell/"},{"name":"linux","slug":"shell/linux","permalink":"https://blog.xiemx.com/categories/shell/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/tags/linux/"},{"name":"shell","slug":"shell","permalink":"https://blog.xiemx.com/tags/shell/"}],"author":"xiemx"},{"title":"rbash创建只读用户","slug":"2016-06-02-rbash-add-readonly-user","date":"2016-06-01T18:06:03.000Z","updated":"2019-10-19T09:18:07.219Z","comments":false,"path":"/2016/06/02/2016-06-02-rbash-add-readonly-user/","link":"","permalink":"https://blog.xiemx.com/2016/06/02/2016-06-02-rbash-add-readonly-user/","excerpt":"","text":"网上看到一篇用rbash来创建受限用户的文章，来源忘记了，mark在这里 受限bash如果 bash 以 rbash 为程序名启动或者命令行参数有 -r 选项，则启动的这个 shell 会在某些功能上受限制．具体表现为如下操作都不能做： 12345678910111213通过 cd 来改变工作目录设置或取消环境变量： SHELL， PATH， ENV， BASH_ENV命令名中不能包含目录分隔符 ‘/’包含有 ‘/’ 的文件名作为内置命令 ‘.’ 的参数hash 内置命令有 -p 选项时的文件名参数包含 '/'在启动时通过 shell 环境导入函数定义在启动时通过 shell 环境解析 SHELLOPTS 的值使用 &gt;，&gt;|， &lt;&gt;， &gt;&amp;， &amp;&gt;， &gt;&gt; 等重定向操作符使用 exec 内置命令通过 enable 内置命令的 -f 和 -d 选项增加或删除内置命令使用 enable 内置命令来禁用或启用 shell 内置命令执行 command 内置命令时加上 -p 选项通过 set +r 或 set +o restricted 关闭受限模式 步骤123456789101112131415161718ln -s /bin/bash /bin/rbashuseradd -s /bin/rbash rttlsapasswd rttlsamkdir /home/rttlsa/binchown root. /home/rttlsa/.bash_profile chmod 755 /home/rttlsa/.bash_profilevi /home/rttlsa/.bash_profile .bash_profile# Get the aliases and functionsif [ -f ~/.bashrc ]; then . ~/.bashrcfi# User specific environment and startup programsPATH=$HOME/binexport PATHln -s /bin/cat /home/rttlsa/bin/cat 将允许执行的命令链接到$HOME/bin目录 如此即可创建只允许查看日志的只读用户。","categories":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/tags/linux/"},{"name":"command","slug":"command","permalink":"https://blog.xiemx.com/tags/command/"},{"name":"rbash","slug":"rbash","permalink":"https://blog.xiemx.com/tags/rbash/"},{"name":"bash","slug":"bash","permalink":"https://blog.xiemx.com/tags/bash/"}],"author":"xiemx"},{"title":"redis主从复制","slug":"2016-05-23-redis-slave-replica","date":"2016-05-23T03:05:17.000Z","updated":"2019-10-19T08:43:41.709Z","comments":true,"path":"/2016/05/23/2016-05-23-redis-slave-replica/","link":"","permalink":"https://blog.xiemx.com/2016/05/23/2016-05-23-redis-slave-replica/","excerpt":"","text":"redis主从复制特点: master可以拥有多个slave 多个slave可以连接同一个master外，还可以连接到其他slave 主从复制不会阻塞master，在同步数据时，master可以继续处理client请求 提高系统的伸缩性 可以在master禁用数据持久化，注释掉master配置文件中的所有save配置，只需在slave上配置数据持久化 redis主从复制过程: 当配置好slave后，slave与master建立连接，然后发送sync命令。无论是第一次连接还是重新连接，master都会启动一个后台进程，将数据库快照保存到文件中，同时master主进程会开始收集新的写命令并缓存。后台进程完成写文件后，master就发送文件给slave，slave将文件保存到硬盘上，再加载到内存中，接着master就会把缓存的命令转发给slave，后续master将收到的写命令发送给slave。如果master同时收到多个slave发来的同步连接命令，master只会启动一个进程来写数据库镜像，然后发送给所有的slave。 redis主从配置:1234567891011121314151617181920212223242526272829303132### masterdaemonize yespidfile /var/run/redis.pidport 6379timeout 300loglevel verboselogfile /usr/local/xiemx-redis/var/log/redis.logdatabases 16save 900 1save 300 10save 60 10000rdbcompression yesdbfilename dump.rdbdir /usr/local/xiemx-redis/var/datarequirepass redisappendonly noappendfsync everysecno-appendfsync-on-rewrite noslowlog-log-slower-than 10000slowlog-max-len 1024vm-enabled novm-swap-file /tmp/redis.swapvm-max-memory 0vm-page-size 32vm-pages 134217728vm-max-threads 4hash-max-zipmap-entries 512hash-max-zipmap-value 64list-max-ziplist-entries 512list-max-ziplist-value 64set-max-intset-entries 512activerehashing yes 123456789101112131415161718192021222324252627282930313233### slavedaemonize yespidfile /var/run/redis.pidport 6379timeout 300loglevel verboselogfile /usr/local/xiemx-redis/var/log/redis.logdatabases 16save 900 1save 300 10save 60 10000rdbcompression yesdbfilename dump.rdbdir /usr/local/xiemx-redis/var/dataappendonly noappendfsync everysecno-appendfsync-on-rewrite noslowlog-log-slower-than 10000slowlog-max-len 1024vm-enabled novm-swap-file /tmp/redis.swapvm-max-memory 0vm-page-size 32vm-pages 134217728vm-max-threads 4hash-max-zipmap-entries 512hash-max-zipmap-value 64list-max-ziplist-entries 512list-max-ziplist-value 64set-max-intset-entries 512activerehashing yesslaveof 192.168.1.189 6379masterauth redis redis复制测试 12345678910111213141516171819查看master端日志:[8930] 31 Jul 19:16:09 - Accepted 192.168.1.136:54774[8930] 31 Jul 19:16:09 * Slave ask for synchronization[8930] 31 Jul 19:16:09 * Starting BGSAVE for SYNC[8930] 31 Jul 19:16:09 * Background saving started by pid 10782[10782] 31 Jul 19:16:09 * DB saved on disk[8930] 31 Jul 19:16:09 * Background saving terminated with success[8930] 31 Jul 19:16:09 * Synchronization with slave succeeded[8930] 31 Jul 19:16:14 - DB 0: 1 keys (0 volatile) in 4 slots HT.[8930] 31 Jul 19:16:14 - 1 clients connected (1 slaves), 807320 bytes in use查看slave端日志:[24398] 01 Aug 10:16:10 * Connecting to MASTER...[24398] 01 Aug 10:16:10 * MASTER SLAVE sync started: SYNC sent[24398] 01 Aug 10:16:10 * MASTER SLAVE sync: receiving 25 bytes from master[24398] 01 Aug 10:16:10 * MASTER SLAVE sync: Loading DB in memory[24398] 01 Aug 10:16:10 * MASTER SLAVE sync: Finished with success[24398] 01 Aug 10:16:15 - DB 0: 1 keys (0 volatile) in 4 slots HT.[24398] 01 Aug 10:16:15 - 1 clients connected (0 slaves), 798960 bytes in use 1234567master端操作:redis 127.0.0.1:6379&gt; set k_m masterOKslave端操作:redis 127.0.0.1:6379&gt; get k_m\"master\"","categories":[{"name":"redis","slug":"redis","permalink":"https://blog.xiemx.com/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://blog.xiemx.com/tags/redis/"}],"author":"xiemx"},{"title":"Linux内核调优部分参数说明","slug":"2016-05-23-linux-kernal-paramenter","date":"2016-05-23T03:05:07.000Z","updated":"2019-10-19T14:56:09.511Z","comments":false,"path":"/2016/05/23/2016-05-23-linux-kernal-paramenter/","link":"","permalink":"https://blog.xiemx.com/2016/05/23/2016-05-23-linux-kernal-paramenter/","excerpt":"","text":"#接收套接字缓冲区大小的默认值(以字节为单位)。net.core.rmem_default = 262144 #接收套接字缓冲区大小的最大值(以字节为单位)。 net.core.rmem_max = 16777216 #发送套接字缓冲区大小的默认值(以字节为单位)。 net.core.wmem_default = 262144 #发送套接字缓冲区大小的最大值(以字节为单位)。 net.core.wmem_max = 16777216 #用来限制监听(LISTEN)队列最大数据包的数量，超过这个数量就会导致链接超时或者触发重传机制。 net.core.somaxconn = 262144 #当网卡接收数据包的速度大于内核处理的速度时，会有一个队列保存这些数据包。这个参数表示该队列的最大值。 net.core.netdev_max_backlog = 262144 #表示系统中最多有多少TCP套接字不被关联到任何一个用户文件句柄上。如果超过这里设置的数字，连接就会复位并输出警告信息。这个限制仅仅是为了防止简单的DoS攻击。此值不能太小。 net.ipv4.tcp_max_orphans = 262144 #表示那些尚未收到客户端确认信息的连接（SYN消息）队列的长度，默认为1024，加大队列长度为262144，可以容纳更多等待连接的网络连接数。 net.ipv4.tcp_max_syn_backlog = 262144 #表示系统同时保持TIME_WAIT套接字的最大数量。如果超过此数，TIME_WAIT套接字会被立刻清除并且打印警告信息。之所以要设定这个限制，纯粹为了抵御那些简单的DoS攻击，不过，过多的TIME_WAIT套接字也会消耗服务器资源，甚至死机。 net.ipv4.tcp_max_tw_buckets = 10000 #表示允许系统打开的端口范围。 net.ipv4.ip_local_port_range = 1024 65500 #以下两参数可解决生产场景中大量连接的服务器中TIME_WAIT过多问题。 #表示开启TCP连接中TIME_WAIT套接字的快速回收，默认为0，表示关闭。 net.ipv4.tcp_tw_recycle = 1 #表示允许重用TIME_WAIT状态的套接字用于新的TCP连接,默认为0，表示关闭。 net.ipv4.tcp_tw_reuse = 1 #当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭。 net.ipv4.tcp_syncookies = 1 #表示系统允许SYN连接的重试次数。为了打开对端的连接，内核需要发送一个SYN并附带一个回应前面一个SYN的ACK包。也就是所谓三次握手中的第二次握手。这个设置决定了内核放弃连接之前发送SYN+ACK包的数量。 net.ipv4.tcp_synack_retries = 1 #表示在内核放弃建立连接之前发送SYN包的数量。 net.ipv4.tcp_syn_retries = 1 #减少处于FIN-WAIT-2连接状态的时间，使系统可以处理更多的连接。 net.ipv4.tcp_fin_timeout = 30 #这个参数表示当keepalive启用时，TCP发送keepalive消息的频度。默认是2小时，若将其设置得小一些，可以更快地清理无效的连接。 net.ipv4.tcp_keepalive_time = 600 #探测消息未获得响应时，重发该消息的间隔时间（秒）。系统默认75秒。 net.ipv4.tcp_keepalive_intvl = 30 #在认定连接失效之前，发送多少个TCP的keepalive探测包。系统默认值是9。这个值乘以tcp_keepalive_intvl之后决定了，一个连接发送了keepalive探测包之后可以有多少时间没有回应。 net.ipv4.tcp_keepalive_probes = 3 #确定TCP栈应该如何反映内存使用，每个值的单位都是内存页（通常是4KB）。第一个值是内存使用的下限；第二个值是内存压力模式开始对缓冲区使用应用压力的上限；第三个值是内存使用的上限。在这个层次上可以将报文丢弃，从而减少对内存的使用。示例中第一个值为7864324/1024/1024=3G，第二个值为10485764/1024/1024=4G，第三个值为1572864*4/1024/1024=6G。 net.ipv4.tcp_mem = 786432 1048576 1572864 #此参数限制并发未完成的异步请求数目，应该设置避免I/O子系统故障。 fs.aio-max-nr = 1048576 #该参数决定了系统中所允许的文件句柄最大数目，文件句柄设置代表linux系统中可以打开的文件的数量。 fs.file-max = 6815744 #表示尽量使用内存，减少使用磁盘swap交换分区，内存速度明显高于磁盘一个数量级。 vm.swappiness = 0","categories":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/tags/linux/"},{"name":"kernal","slug":"kernal","permalink":"https://blog.xiemx.com/tags/kernal/"}],"author":"xiemx"},{"title":"python socket编程","slug":"2016-04-28-python-code-socket","date":"2016-04-27T18:04:14.000Z","updated":"2019-10-19T07:54:45.618Z","comments":false,"path":"/2016/04/28/2016-04-28-python-code-socket/","link":"","permalink":"https://blog.xiemx.com/2016/04/28/2016-04-28-python-code-socket/","excerpt":"","text":"需要在多台服务器上运行日志分析脚本，分析完成后每台机器直接发送邮件会出现大量邮件同时过来，现在想将多台机器的分析结果收集起来，通过网络发送到服务端，在服务端收集所有的日志，在统一发送一封邮件，实现的socket代码如下。 接收端（server）123456789101112131415161718192021222324#!/usr/bin/env pythonimport socketdef socket_server(host,port): self_host = host self_port = port s = socket.socket(socket.AF_INET,socket.SOCK_STREAM) s.bind((self_host,self_port)) s.listen(4) f = open(\"log\", \"a\") while True: conn,addr=s.accept() data = conn.recv(1024) print 'data:', data f.write(data) f.close() s.close()if __name__ == \"__main__\": host = '127.0.0.1' port = 65530 socket_server(host,port) 发送端（client）12345678910111213141516171819202122#!/usr/bin/env pythonimport socketdef socket_client(host,port,content): self_host = host self_port = port self_content = content s = socket.socket(socket.AF_INET,socket.SOCK_STREAM) s.connect((self_host,self_port)) s.sendall(content) s.close()if __name__ == \"__main__\": host = \"127.0.0.1\" port = 65530 f = open(\"sendfile\",\"r\") content = f.read() f.close() socket_client(host, port, content)","categories":[{"name":"python","slug":"python","permalink":"https://blog.xiemx.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://blog.xiemx.com/tags/python/"},{"name":"socket","slug":"socket","permalink":"https://blog.xiemx.com/tags/socket/"}],"author":"xiemx"},{"title":"window系统上部署Zabbix_agent","slug":"2016-04-13-deploy-zabbix_agent-to-windows","date":"2016-04-13T01:04:53.000Z","updated":"2019-09-27T06:52:36.187Z","comments":false,"path":"/2016/04/13/2016-04-13-deploy-zabbix_agent-to-windows/","link":"","permalink":"https://blog.xiemx.com/2016/04/13/2016-04-13-deploy-zabbix_agent-to-windows/","excerpt":"","text":"1.获取windows的agent客户端，解压文件至指定位置 ![img](/images/img_570da13048078.png)2.修改conf文件中的 12345Server=server端的IP地址ServerActive=server端的IP地址HostName=服务器在监控端上的监控名，可见名称可以不写默认为主机名称 3.命令行模式下安装Zabbix_agent监控程序 1234567891011# 进入解压后的文件夹下cd C:\\zabbix_agents\\bin\\win64#安装程序zabbix_agentd.exe –c c:\\zabbix\\zabbix_agentd.conf -i#启动程序zabbix_agentd.exe –c c:\\zabbix\\zabbix_agentd.conf -s 123456参数含义：-c 指定配置文件所在位置-i 安装客户端-s 启动客户端-x 停止客户端-d 卸载客户端 4.监控端添加agent服务器注意主机名为conf文件中定义的名称，否则会导致某些监控项目异常","categories":[{"name":"zabbix","slug":"zabbix","permalink":"https://blog.xiemx.com/categories/zabbix/"}],"tags":[{"name":"windows","slug":"windows","permalink":"https://blog.xiemx.com/tags/windows/"},{"name":"zabbix","slug":"zabbix","permalink":"https://blog.xiemx.com/tags/zabbix/"}],"author":"xiemx"},{"title":"Tomcat虚拟主机别名设置","slug":"2016-04-13-tomcat-server-name-alias","date":"2016-04-13T01:04:35.000Z","updated":"2019-10-18T09:13:06.614Z","comments":false,"path":"/2016/04/13/2016-04-13-tomcat-server-name-alias/","link":"","permalink":"https://blog.xiemx.com/2016/04/13/2016-04-13-tomcat-server-name-alias/","excerpt":"","text":"一个虚拟空间需要绑定多个域名时可以通过alias标签来设置别名，详见如下配置文件部分截图 123456789101112131415161718192021222324&lt;Engine name=\"Catalina\" defaultHost=\"localhost\"&gt; &lt;!--For clustering, please take a look at documentation at: /docs/cluster-howto.html (simple how to) /docs/config/cluster.html (reference documentation) --&gt; &lt;!-- &lt;Cluster className=\"org.apache.catalina.ha.tcp.SimpleTcpCluster\"/&gt; --&gt; &lt;!-- Use the LockOutRealm to prevent attempts to guess user passwords via a brute-force attack --&gt; &lt;Realm className=\"org.apache.catalina.realm.LockOutRealm\"&gt; &lt;!-- This Realm uses the UserDatabase configured in the global JNDI resources under the key \"UserDatabase\". Any edits that are performed against this UserDatabase are immediately available for use by the Realm. --&gt; &lt;Realm className=\"org.apache.catalina.realm.UserDatabaseRealm\" resourceName=\"UserDatabase\"/&gt; &lt;/Realm&gt; &lt;Host name=\"www.xiemx.com\" debug=\"0\" appBase=\"webapps\" unpackWARs=\"true\" autoDeploy=\"true\" xmlValidation=\"false\" xmlNamespaceAware=\"false\"&gt; &lt;Alias&gt;xiemx.com&lt;/Alias&gt; &lt;Alias&gt;xx.xiemx.com&lt;/Alias&gt; &lt;Alias&gt;yy.xiemx.com&lt;/Alias&gt; &lt;/Host&gt; &lt;/Engine&gt;","categories":[{"name":"tomcat","slug":"tomcat","permalink":"https://blog.xiemx.com/categories/tomcat/"}],"tags":[{"name":"http","slug":"http","permalink":"https://blog.xiemx.com/tags/http/"},{"name":"tomcat","slug":"tomcat","permalink":"https://blog.xiemx.com/tags/tomcat/"},{"name":"webserver","slug":"webserver","permalink":"https://blog.xiemx.com/tags/webserver/"}],"author":"xiemx"},{"title":"tomcat出现PermGen Space问题","slug":"2016-03-29-tomcat-permgen-space","date":"2016-03-28T21:03:32.000Z","updated":"2019-10-19T07:19:23.489Z","comments":false,"path":"/2016/03/29/2016-03-29-tomcat-permgen-space/","link":"","permalink":"https://blog.xiemx.com/2016/03/29/2016-03-29-tomcat-permgen-space/","excerpt":"","text":"现象tomcat服务器运行一段时间，总是会自动报异常：java.lang.OutOfmemoryError:PermGen Space的错误，导致项目无法正常运行。 出现这个错误是由于内存泄漏。PermGen Space指的是内存的永久保存区，该块内存主要是被JVM存放class和mete信息的，当class被加载loader的时候就会被存储到该内存区中，与存放类的实例的heap区不同，java中的垃圾回收器GC不会在主程序运行期对PermGen space进行清理，所以当我们的应用中有很多的class时，很可能就会出现PermGen space的错误。 解决方法手动设置MaxPermSize的大小 修改 TOMCAT_HOME/bin/catalina.bat(Linux上为catalina.sh)文件，在echo “using CATALINA_BASE：$CATALINA_BASE”上面加入这一行内容： 1set JAVA_OPTS=%JAVA_OPTS% -server -XX:PermSize=128m -XX:MaxPermSize=512m","categories":[{"name":"tomcat","slug":"tomcat","permalink":"https://blog.xiemx.com/categories/tomcat/"}],"tags":[{"name":"http","slug":"http","permalink":"https://blog.xiemx.com/tags/http/"},{"name":"tomcat","slug":"tomcat","permalink":"https://blog.xiemx.com/tags/tomcat/"},{"name":"webserver","slug":"webserver","permalink":"https://blog.xiemx.com/tags/webserver/"}],"author":"xiemx"},{"title":"python中is和==的区别","slug":"2016-03-29-python-is","date":"2016-03-28T18:03:08.000Z","updated":"2019-10-19T09:24:58.389Z","comments":false,"path":"/2016/03/29/2016-03-29-python-is/","link":"","permalink":"https://blog.xiemx.com/2016/03/29/2016-03-29-python-is/","excerpt":"","text":"Python中的对象包含三要素：id、type、value id用来唯一标识一个对象 type标识对象的类型 value是对象的值。 is 通过id来判断对象是否相等 == 通过value来判断值是否相等 例： 1234567&gt;&gt;&gt; a = &#123;1:2&#125;&gt;&gt;&gt; b = a.copy()&gt;&gt;&gt; a is bFalse&gt;&gt;&gt; a == bTrue&gt;&gt;&gt;","categories":[{"name":"python","slug":"python","permalink":"https://blog.xiemx.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://blog.xiemx.com/tags/python/"}],"author":"xiemx"},{"title":"Zentaopms部署","slug":"2016-03-25-zentaopms-deployment","date":"2016-03-25T03:03:57.000Z","updated":"2019-09-27T06:52:36.187Z","comments":false,"path":"/2016/03/25/2016-03-25-zentaopms-deployment/","link":"","permalink":"https://blog.xiemx.com/2016/03/25/2016-03-25-zentaopms-deployment/","excerpt":"","text":"获取软件 1wget http://sourceforge.net/projects/zentao/files/8.1.3/ZenTaoPMS.8.1.3.zip/download 将软件解压放到web服务器的目录下 创建数据库，授权用户 1create database zentao;grant all zentao.* to zentao@localhost identified by 'zentap'; Web访问页面开启安装进程 访问http://localhost/zentaopms/www/install.php，同dz等程序安装相同注意配置时使用授权的账户密码和数据库 安装完成后需要破解zentaopms方可正常登录 通过下面的地址下载loader-wizard：http://www.ioncube.com/loader-wizard/loader-wizard.zip 下载之后，将其解压缩，到web服务器的DocumentRoot下。 使用浏览器访问loader-wizard.php文件。该文件会检测当前环境，给出提示解决方法依照步骤处理 重新web服务启动之后，再次访问loader.php，如果安装成功，系统会提示。 看到这个界面，就表示解密软件已经安装成功了。 再次访问zentaopms的首页测试是否可以正常登录","categories":[{"name":"zentaopms","slug":"zentaopms","permalink":"https://blog.xiemx.com/categories/zentaopms/"},{"name":"deployment","slug":"zentaopms/deployment","permalink":"https://blog.xiemx.com/categories/zentaopms/deployment/"}],"tags":[{"name":"zentaopms","slug":"zentaopms","permalink":"https://blog.xiemx.com/tags/zentaopms/"},{"name":"deployment","slug":"deployment","permalink":"https://blog.xiemx.com/tags/deployment/"}],"author":"xiemx"},{"title":"python tab自动补全模块","slug":"2016-03-04-python-tab","date":"2016-03-03T23:03:49.000Z","updated":"2019-10-19T09:27:16.679Z","comments":true,"path":"/2016/03/04/2016-03-04-python-tab/","link":"","permalink":"https://blog.xiemx.com/2016/03/04/2016-03-04-python-tab/","excerpt":"","text":"12345678910111213141516171819#vi tab.py#!/usr/bin/env python # python startup file import sysimport readlineimport rlcompleterimport atexitimport os# tab completion readline.parse_and_bind('tab: complete')# history file histfile = os.path.join(os.environ['HOME'], '.pythonhistory')try: readline.read_history_file(histfile)except IOError: passatexit.register(readline.write_history_file, histfile)del os, histfile, readline, rlcompleter","categories":[{"name":"python","slug":"python","permalink":"https://blog.xiemx.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://blog.xiemx.com/tags/python/"}],"author":"xiemx"},{"title":"Mysql数据库代理amoeba","slug":"2016-02-18-mysql-proxy-amoeba","date":"2016-02-17T21:02:36.000Z","updated":"2019-10-19T10:44:22.737Z","comments":false,"path":"/2016/02/18/2016-02-18-mysql-proxy-amoeba/","link":"","permalink":"https://blog.xiemx.com/2016/02/18/2016-02-18-mysql-proxy-amoeba/","excerpt":"","text":"基本环境： 12345A（172.25.16.10）：客户端 B（172.25.16.11）：主数据库 C（172.25.16.12）：从数据库 D（172.25.16.13）：从数据库J（172.25.16.19）：代理数据库 基于amoeba的读写分离 在server j 安装JDK 新建目录/usr/local/amoeba 1mkdir /usr/local/amoeba 把压缩包解压到该目录/usrl/local/amoeba 在conf目录下有amoeba的配置文件需要修改的两个：amoeba.xml 和 dbServers.xml amoeba.xml(3个地方要改):监听的端口号：12345&lt;proxy&gt;&lt;!-- service class must implements com.meidusa.amoeba.service.Service --&gt; &lt;service name=\"Amoeba for Mysql\" class=\"com.meidusa.amoeba.net.ServerableConnectionManager\"&gt;&lt;!-- port --&gt;&lt;property name=\"port\"&gt;3306&lt;/property&gt; 客户端访问时用的用户名123456789101112&lt;property name=\"authenticator\"&gt; &lt;bean class=\"com.meidusa.amoeba.mysql.server.MysqlClientAuthenticator\"&gt; &lt;property name=\"user\"&gt;amoeba&lt;/property&gt; &lt;property name=\"password\"&gt;amoeba&lt;/property&gt; &lt;property name=\"filter\"&gt; &lt;bean class=\"com.meidusa.amoeba.server.IPAccessController\"&gt; &lt;property name=\"ipFile\"&gt;$&#123;amoeba.home&#125;/conf/access_list.conf&lt;/property&gt; &lt;/bean&gt; &lt;/property&gt; &lt;/bean&gt;&lt;/property&gt; read/write pool123456&lt;property name=\"sqlFunctionFile\"&gt;$&#123;amoeba.home&#125;/conf/functionMap.xml&lt;/property&gt;&lt;property name=\"LRUMapSize\"&gt;1500&lt;/property&gt; &lt;property name=\"defaultPool\"&gt;serverb#默认访问的数据库#&lt;/property&gt; &lt;property name=\"writePool\"&gt;serverb#指定可写池#&lt;/property&gt; &lt;property name=\"readPool\"&gt;readgroup1#指定只读池#&lt;/property&gt;&lt;property name=\"needParse\"&gt;true&lt;/property&gt; dbServers.xml文件：修改端口号，用户名，密码。1234567891011121314&lt;dbServer name=\"abstractServer\" abstractive=\"true\"&gt;&lt;factoryConfig class=\"com.meidusa.amoeba.mysql.net.MysqlServerConnectionFactory\"&gt; &lt;property name=\"manager\"&gt;$&#123;defaultManager&#125;&lt;/property&gt; &lt;property name=\"sendBufferSize\"&gt;64&lt;/property&gt; &lt;property name=\"receiveBufferSize\"&gt;128&lt;/property&gt; &lt;!-- mysql port --&gt; &lt;property name=\"port\"&gt;3306&lt;/property&gt; &lt;!-- mysql schema --&gt; &lt;property name=\"schema\"&gt;test&lt;/property&gt; &lt;!-- mysql user --&gt; &lt;property name=\"user\"&gt;dbproxy&lt;/property&gt; &lt;!-- mysql password --&gt; &lt;property name=\"password\"&gt;xiemx&lt;/property&gt;&lt;/factoryConfig&gt; 指定可以代理的数据库1234567891011121314151617181920&lt;dbServer name=\"serverb\" parent=\"abstractServer\"&gt; &lt;factoryConfig&gt; &lt;!-- mysql ip --&gt; &lt;property name=\"ipAddress\"&gt;172.25.16.11&lt;/property&gt; &lt;/factoryConfig&gt;&lt;/dbServer&gt;&lt;dbServer name=\"serverc\" parent=\"abstractServer\"&gt; &lt;factoryConfig&gt; &lt;!-- mysql ip --&gt; &lt;property name=\"ipAddress\"&gt;172.25.16.12&lt;/property&gt; &lt;/factoryConfig&gt;&lt;/dbServer&gt;&lt;dbServer name=\"serverd\" parent=\"abstractServer\"&gt; &lt;factoryConfig&gt; &lt;!-- mysql ip --&gt; &lt;property name=\"ipAddress\"&gt;172.25.16.13&lt;/property&gt; &lt;/factoryConfig&gt;&lt;/dbServer&gt; 指定只读组12345678&lt;dbServer name=\"readgroup1\" virtual=\"true\"&gt; &lt;poolConfig class=\"com.meidusa.amoeba.server.MultipleServerPool\"&gt; &lt;!-- Load balancing strategy: 1=ROUNDROBIN , 2=WEIGHTBASED , 3=HA--&gt; &lt;property name=\"loadbalance\"&gt;1&lt;/property&gt; &lt;!-- Separated by commas,such as: server1,server2,server1 --&gt; &lt;property name=\"poolNames\"&gt;serverc,serverd&lt;/property&gt; &lt;/poolConfig&gt;&lt;/dbServer&gt; 启动脚本在/bin下启动时会报错，需要修改文件即可：DEFAULT_OPTS=&quot;-server -Xms256m -Xmx256m -Xss228k&quot;(最后一个参数改为228即可) 1./amoeba start 在BCD上授权允许J访问数据库 1grant all on db1.* to dbproxy@'172.25.16.19' identified by 'xiemx';","categories":[{"name":"mysql","slug":"mysql","permalink":"https://blog.xiemx.com/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://blog.xiemx.com/tags/mysql/"},{"name":"amoeba","slug":"amoeba","permalink":"https://blog.xiemx.com/tags/amoeba/"},{"name":"proxy","slug":"proxy","permalink":"https://blog.xiemx.com/tags/proxy/"}],"author":"xiemx"},{"title":"UDEV规则","slug":"2016-02-18-udev","date":"2016-02-17T21:02:23.000Z","updated":"2019-10-18T07:37:58.286Z","comments":false,"path":"/2016/02/18/2016-02-18-udev/","link":"","permalink":"https://blog.xiemx.com/2016/02/18/2016-02-18-udev/","excerpt":"","text":"udev根据系统中的硬件设备的状态动态更新设备，通过对内核产生的设备名增加别名的方式来达到不管设备连接的顺序而维持一个统一的设备名的目的。udev可以根据设备的其他信息如总线（bus），生产商（vendor）等不同来区分不同的设备，并产生设备文件。udev是硬件平台无关的，属于user space的进程，它脱离驱动层的关联而建立在操作系统之上，遵循Linux Standard Base （LSB）设备命名方法，但也可以自定义命名 工作流程图 配置文件、rule参数12345678910111213141516171819202122232425262728293031323334353637383940[mingxu.xie@cn-aux-cc udev]$ ll -R.:total 8drwxr-xr-x 2 root root 4096 Jun 6 02:58 rules.d-rw-r--r-- 1 root root 218 Mar 19 2014 udev.conf./rules.d:total 32-rw-r--r-- 1 root root 640 Jul 31 2016 51-ec2-hvm-devices.rules-rw-r--r-- 1 root root 641 Jul 31 2016 52-ec2-vcpu.rules-rw-r--r-- 1 root root 740 Jul 31 2016 53-ec2-network-interfaces.rules-rw-r--r-- 1 root root 680 Jul 31 2016 60-cdrom_id.rules-rw-r--r-- 1 root root 326 Apr 26 2016 60-raw.rules-rw-r--r-- 1 root root 1343 Jul 31 2016 70-ec2-nvme-devices.rules-rw-r--r-- 1 root root 1424 Jul 31 2016 75-persistent-net-generator.rules-rwxr-xr-x 1 root root 343 Aug 21 2016 80-docker.rules# 主配置 /etc/udev/udev.conf udev_root=&quot;/dev/&quot;udev_rules=&quot;/etc/udev/rules.d/&quot;#rule规则的文件命名第一段为执行顺序，同rc脚本[mingxu.xie@cn-aux-cc udev]$ cat rules.d/53-ec2-network-interfaces.rules# Copyright (C) 2012 Amazon.com, Inc. or its affiliates.# All Rights Reserved.## Licensed under the Apache License, Version 2.0 (the &quot;License&quot;).# You may not use this file except in compliance with the License.# A copy of the License is located at## http://aws.amazon.com/apache2.0/## or in the &quot;license&quot; file accompanying this file. This file is# distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS# OF ANY KIND, either express or implied. See the License for the# specific language governing permissions and limitations under the# License.ACTION==&quot;add&quot;, SUBSYSTEM==&quot;net&quot;, KERNEL==&quot;eth*&quot;, IMPORT&#123;program&#125;=&quot;/bin/sleep 1&quot;SUBSYSTEM==&quot;net&quot;, KERNEL==&quot;eth*&quot;, RUN+=&quot;/etc/sysconfig/network-scripts/ec2net.hotplug&quot; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102#监听一个docker run -it --rm nginx bash容器启动的设备变化情况，如下[mingxu.xie@cn-aux-cc udev]$ udevadm monitormonitor will print the received events for:UDEV - the event which udev sends out after rule processingKERNEL - the kernel ueventKERNEL[240932.229041] add /devices/virtual/bdi/253:8 (bdi)KERNEL[240932.229579] add /devices/virtual/block/dm-8 (block)UDEV [240932.229612] add /devices/virtual/bdi/253:8 (bdi)UDEV [240932.229641] add /devices/virtual/block/dm-8 (block)KERNEL[240932.248388] change /devices/virtual/block/dm-8 (block)UDEV [240932.272904] change /devices/virtual/block/dm-8 (block)KERNEL[240932.425971] remove /devices/virtual/block/dm-8 (block)UDEV [240932.428073] remove /devices/virtual/block/dm-8 (block)KERNEL[240932.456141] remove /devices/virtual/bdi/253:8 (bdi)KERNEL[240932.456261] remove /devices/virtual/block/dm-8 (block)UDEV [240932.456380] remove /devices/virtual/bdi/253:8 (bdi)UDEV [240932.456419] remove /devices/virtual/block/dm-8 (block)KERNEL[240932.469307] add /devices/virtual/bdi/253:8 (bdi)KERNEL[240932.469384] add /devices/virtual/block/dm-8 (block)UDEV [240932.469657] add /devices/virtual/bdi/253:8 (bdi)UDEV [240932.469691] add /devices/virtual/block/dm-8 (block)KERNEL[240932.488228] change /devices/virtual/block/dm-8 (block)UDEV [240932.504203] change /devices/virtual/block/dm-8 (block)KERNEL[240932.636785] remove /devices/virtual/block/dm-8 (block)UDEV [240932.638755] remove /devices/virtual/block/dm-8 (block)KERNEL[240932.676137] remove /devices/virtual/bdi/253:8 (bdi)KERNEL[240932.676245] remove /devices/virtual/block/dm-8 (block)UDEV [240932.676367] remove /devices/virtual/bdi/253:8 (bdi)UDEV [240932.676420] remove /devices/virtual/block/dm-8 (block)KERNEL[240932.683098] add /devices/virtual/bdi/253:8 (bdi)KERNEL[240932.683204] add /devices/virtual/block/dm-8 (block)UDEV [240932.683384] add /devices/virtual/bdi/253:8 (bdi)UDEV [240932.683582] add /devices/virtual/block/dm-8 (block)KERNEL[240932.696227] change /devices/virtual/block/dm-8 (block)UDEV [240932.712517] change /devices/virtual/block/dm-8 (block)KERNEL[240932.793677] add /devices/virtual/net/veth4d1712b (net)KERNEL[240932.793699] add /devices/virtual/net/veth4d1712b/queues/rx-0 (queues)KERNEL[240932.793727] add /devices/virtual/net/veth4d1712b/queues/tx-0 (queues)KERNEL[240932.793908] add /devices/virtual/net/veth17f6e3e (net)KERNEL[240932.793930] add /devices/virtual/net/veth17f6e3e/queues/rx-0 (queues)KERNEL[240932.793942] add /devices/virtual/net/veth17f6e3e/queues/tx-0 (queues)UDEV [240932.827598] add /devices/virtual/net/veth4d1712b (net)UDEV [240932.827843] add /devices/virtual/net/veth4d1712b/queues/rx-0 (queues)UDEV [240932.828121] add /devices/virtual/net/veth4d1712b/queues/tx-0 (queues)KERNEL[240932.849171] add /kernel/slab/:A-0000200/cgroup/vm_area_struct(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)KERNEL[240932.849196] add /kernel/slab/:A-0000064/cgroup/anon_vma_chain(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)UDEV [240932.849211] add /kernel/slab/:A-0000200/cgroup/vm_area_struct(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)KERNEL[240932.849225] add /kernel/slab/anon_vma/cgroup/anon_vma(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)KERNEL[240932.849238] add /kernel/slab/:aA-0000192/cgroup/dentry(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)KERNEL[240932.849252] add /kernel/slab/proc_inode_cache/cgroup/proc_inode_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)UDEV [240932.849269] add /kernel/slab/:A-0000064/cgroup/anon_vma_chain(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)KERNEL[240932.849436] add /kernel/slab/:0000064/cgroup/kmalloc-64(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)KERNEL[240932.849455] add /kernel/slab/shmem_inode_cache/cgroup/shmem_inode_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)UDEV [240932.849709] add /kernel/slab/proc_inode_cache/cgroup/proc_inode_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)UDEV [240932.850033] add /kernel/slab/anon_vma/cgroup/anon_vma(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)UDEV [240932.850374] add /kernel/slab/shmem_inode_cache/cgroup/shmem_inode_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)UDEV [240932.850527] add /kernel/slab/:aA-0000192/cgroup/dentry(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)UDEV [240932.852368] add /kernel/slab/:0000064/cgroup/kmalloc-64(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)KERNEL[240932.854849] add /kernel/slab/:0000192/cgroup/kmalloc-192(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)KERNEL[240932.854871] add /kernel/slab/:0001024/cgroup/kmalloc-1024(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)KERNEL[240932.855103] add /kernel/slab/radix_tree_node/cgroup/radix_tree_node(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)UDEV [240932.855120] add /kernel/slab/:0000192/cgroup/kmalloc-192(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)KERNEL[240932.855133] add /kernel/slab/:A-0000192/cgroup/cred_jar(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)KERNEL[240932.855145] add /kernel/slab/:A-0002048/cgroup/mm_struct(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)KERNEL[240932.855158] add /kernel/slab/mqueue_inode_cache/cgroup/mqueue_inode_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)KERNEL[240932.855170] add /kernel/slab/sock_inode_cache/cgroup/sock_inode_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)KERNEL[240932.855359] add /kernel/slab/:A-0009664/cgroup/task_struct(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)UDEV [240932.855395] add /kernel/slab/:A-0002048/cgroup/mm_struct(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)KERNEL[240932.855410] add /kernel/slab/:A-0000704/cgroup/files_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)UDEV [240932.855445] add /kernel/slab/mqueue_inode_cache/cgroup/mqueue_inode_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)KERNEL[240932.855461] add /kernel/slab/sighand_cache/cgroup/sighand_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)UDEV [240932.855474] add /kernel/slab/:0001024/cgroup/kmalloc-1024(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)KERNEL[240932.855487] add /kernel/slab/:A-0001024/cgroup/signal_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)KERNEL[240932.855499] add /kernel/slab/:A-0000128/cgroup/pid(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)UDEV [240932.855512] add /kernel/slab/radix_tree_node/cgroup/radix_tree_node(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)UDEV [240932.856010] add /kernel/slab/sock_inode_cache/cgroup/sock_inode_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)UDEV [240932.856029] add /kernel/slab/:A-0009664/cgroup/task_struct(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)UDEV [240932.856045] add /kernel/slab/sighand_cache/cgroup/sighand_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)KERNEL[240932.856077] add /kernel/slab/:0000256/cgroup/kmalloc-256(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)UDEV [240932.856092] add /kernel/slab/:A-0000704/cgroup/files_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)KERNEL[240932.856104] add /kernel/slab/:0000512/cgroup/kmalloc-512(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)UDEV [240932.856188] add /kernel/slab/:A-0000128/cgroup/pid(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)UDEV [240932.857080] add /kernel/slab/:A-0000192/cgroup/cred_jar(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)UDEV [240932.857105] add /kernel/slab/:A-0001024/cgroup/signal_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)UDEV [240932.857126] add /kernel/slab/:0000256/cgroup/kmalloc-256(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)UDEV [240932.857140] add /kernel/slab/:0000512/cgroup/kmalloc-512(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)KERNEL[240932.862157] add /kernel/slab/xfs_inode/cgroup/xfs_inode(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)UDEV [240932.862348] add /kernel/slab/xfs_inode/cgroup/xfs_inode(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)KERNEL[240932.863008] add /kernel/slab/inode_cache/cgroup/inode_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)UDEV [240932.863179] add /kernel/slab/inode_cache/cgroup/inode_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)UDEV [240932.932564] add /devices/virtual/net/veth17f6e3e (net)UDEV [240932.932779] add /devices/virtual/net/veth17f6e3e/queues/rx-0 (queues)UDEV [240932.932880] add /devices/virtual/net/veth17f6e3e/queues/tx-0 (queues)KERNEL[240932.960183] remove /devices/virtual/net/veth4d1712b (net)UDEV [240932.982851] remove /devices/virtual/net/veth4d1712b (net)KERNEL[240933.089679] add /kernel/slab/:0000008/cgroup/kmalloc-8(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)UDEV [240933.089933] add /kernel/slab/:0000008/cgroup/kmalloc-8(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)KERNEL[240933.150833] add /kernel/slab/:0002048/cgroup/kmalloc-2048(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)KERNEL[240933.150863] add /kernel/slab/:0000096/cgroup/kmalloc-96(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)UDEV [240933.151052] add /kernel/slab/:0002048/cgroup/kmalloc-2048(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)UDEV [240933.151130] add /kernel/slab/:0000096/cgroup/kmalloc-96(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)","categories":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/tags/linux/"},{"name":"udev","slug":"udev","permalink":"https://blog.xiemx.com/tags/udev/"}],"author":"xiemx"},{"title":"三种共享存储比较","slug":"2016-02-18-Three-shared-storage-comparisons","date":"2016-02-17T20:02:44.000Z","updated":"2019-09-27T06:52:36.186Z","comments":false,"path":"/2016/02/18/2016-02-18-Three-shared-storage-comparisons/","link":"","permalink":"https://blog.xiemx.com/2016/02/18/2016-02-18-Three-shared-storage-comparisons/","excerpt":"","text":"共享存储（Share Storage）类型 NAS(Network Attached Storage)网络附加存储 SAN (Storage Area Network)储存区域网络 iSAN (internet Storage Area Network )以太网存储区域网络,基于以太网的san NAS(Network Attached Storage) 基于tcp/ip网络 以文件为单位进行操作（文件锁） SAN (Storage Area Network) 基于硬盘驱动协议（sisc）传输的是磁道/扇区信息 基于扇区/block锁 iSAN (internet Storage Area Network ) 基于tcp/ip网络 , 通过以太网数据包传递scsi协议数据 基于扇区/block锁","categories":[{"name":"storage","slug":"storage","permalink":"https://blog.xiemx.com/categories/storage/"}],"tags":[],"author":"xiemx"},{"title":"ISCSI存储网络构建","slug":"2016-02-18-iscsi-store","date":"2016-02-17T20:02:11.000Z","updated":"2019-10-19T15:52:01.849Z","comments":false,"path":"/2016/02/18/2016-02-18-iscsi-store/","link":"","permalink":"https://blog.xiemx.com/2016/02/18/2016-02-18-iscsi-store/","excerpt":"","text":"iscsi的结构和san的结构 iSCSI 通信端 发起 I/O 请求的启动设备(Initiator) 响应请求并执行实际 I/O 操作的目标设备(Target) iSCSI工作过程 target端导出共享设备 initiator端发现设备 initiator端导入设备 initiator端分区、格式化、挂接设备 iSCSI实现套件服务端（target）：scsi-target-utils软件包客户端（initiator）：iscsi-initiator-utils软件包 部署过程：12yum insitall scsi-target-utilsyum insitall iscsi-initiator-utils 服务器端导出共享设备： 配置文件如下 12345[root@rhel6 ~]# grep -v '#\\|^$' /etc/tgt/targets.confdefault-driver iscsi&lt;target iqn.2015-10.com.example-f30:vdb1-1g&gt; backing-store /dev/vdb1&lt;/target&gt; 启动tgtd服务 12[root@rhel6 ~]# /etc/init.d/tgtd startStarting SCSI target daemon: [ OK ] 查看客户端本地有无/dev/sda设备 12[root@rhel6 ~]# ll /dev/sdals: cannot access /dev/sda: No such file or directory 搜索target端的共享设备 12[root@rhel6 ~]# iscsiadm -m discovery -t st -p 172.25.30.10172.25.30.10:3260,1 iqn.2015-10.com.example-f30:vdb1-1g 导入target端的共享设备 123[root@rhel6 ~]# iscsiadm -m node -lLogging in to [iface: default, target: iqn.2015-10.com.example-f30:vdb1-1g, portal: 172.25.30.10,3260] (multiple)Login to [iface: default, target: iqn.2015-10.com.example-f30:vdb1-1g, portal: 172.25.30.10,3260] successful. 查看本地/dev/sda设备 12[root@rhel6 ~]# ll /dev/sdabrw-rw----. 1 root disk 8, 0 Jan 14 14:51 /dev/sda 卸载导入的设备 1234567891011[root@rhel6 nodes]# ll /dev/sd*brw-rw----. 1 root disk 8, 0 Jan 14 14:53 /dev/sdabrw-rw----. 1 root disk 8, 16 Jan 14 15:35 /dev/sdbbrw-rw----. 1 root disk 8, 17 Jan 14 15:35 /dev/sdb1[root@rhel6 nodes]# iscsiadm -m node -T iqn.2016-01-14.com.example.node4-f24:vdb1-1G -uLogging out of session [sid: 2, target: iqn.2016-01-14.com.example.node4-f24:vdb1-1G, portal: 172.25.24.13,3260]Logout of [sid: 2, target: iqn.2016-01-14.com.example.node4-f24:vdb1-1G, portal: 172.25.24.13,3260] successful.[root@rhel6 nodes]# ll /dev/sd*brw-rw----. 1 root disk 8, 0 Jan 14 14:53 /dev/sda 配置iSCSI的acl和验证在target主配置文件中添加如下两行 1234567891011[root@rhel6 ~]# grep -v '#\\|^$' /etc/tgt/targets.confdefault-driver iscsi&lt;target iqn.2015-10.com.example-f30:vdb1-1g&gt; backing-store /dev/vdb1&lt;/target&gt;&lt;target iqn.2015-10.com.example-f30:vdb2-2g&gt; backing-store /dev/vdb2 initiator-address 172.25.30.0/24 incominguser xiemx uplooking&lt;/target&gt; 重新加载配置文件 12/etc/init.d/tgtd reload/etc/init.d/tgtd force-reload 查看配置导出的设备信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768[root@rhel6 ~]# tgtadm --lld iscsi --mode target --op showTarget 1: iqn.2015-10.com.example-f30:vdb1-1g System information:​ Driver: iscsi​ State: ready I_T nexus information: LUN information:​ LUN: 0​ Type: controller​ SCSI ID: IET 00010000​ SCSI SN: beaf10​ Size: 0 MB, Block size: 1​ Online: Yes​ Removable media: No​ Prevent removal: No​ Readonly: No​ Backing store type: null​ Backing store path: None​ Backing store flags:​ LUN: 1​ Type: disk​ SCSI ID: IET 00010001​ SCSI SN: beaf11​ Size: 1074 MB, Block size: 512​ Online: Yes​ Removable media: No​ Prevent removal: No​ Readonly: No​ Backing store type: rdwr​ Backing store path: /dev/vdb1​ Backing store flags: Account information: ACL information:​ ALL Target 2: iqn.2015-10.com.example-f30:vdb2-2g System information:​ Driver: iscsi​ State: ready I_T nexus information: LUN information:​ LUN: 0​ Type: controller​ SCSI ID: IET 00020000​ SCSI SN: beaf20​ Size: 0 MB, Block size: 1​ Online: Yes​ Removable media: No​ Prevent removal: No​ Readonly: No​ Backing store type: null​ Backing store path: None​ Backing store flags:​ LUN: 1​ Type: disk​ SCSI ID: IET 00020001​ SCSI SN: beaf21​ Size: 2148 MB, Block size: 512​ Online: Yes​ Removable media: No​ Prevent removal: No​ Readonly: No​ Backing store type: rdwr​ Backing store path: /dev/vdb2​ Backing store flags: Account information:​ xiemx ACL information:​ 172.25.30.0/24 initiator端开启CHAP验证并配置用户密码 1234567891011121314151617181920212223242526272829[root@rhel6 ~]grep -v '^#\\|^$' /etc/iscsi/iscsid.confiscsid.startup = /etc/rc.d/init.d/iscsid force-startnode.startup = automaticnode.leading_login = Nonode.session.auth.authmethod = CHAPnode.session.auth.username = xiemxnode.session.auth.password = uplookingnode.session.timeo.replacement_timeout = 120node.conn[0].timeo.login_timeout = 15node.conn[0].timeo.logout_timeout = 15node.conn[0].timeo.noop_out_interval = 5node.conn[0].timeo.noop_out_timeout = 5node.session.err_timeo.abort_timeout = 15node.session.err_timeo.lu_reset_timeout = 30node.session.err_timeo.tgt_reset_timeout = 30node.session.initial_login_retry_max = 8node.session.cmds_max = 128node.session.queue_depth = 32node.session.xmit_thread_priority = -20node.session.iscsi.InitialR2T = Nonode.session.iscsi.ImmediateData = Yesnode.session.iscsi.FirstBurstLength = 262144node.session.iscsi.MaxBurstLength = 16776192node.conn[0].iscsi.MaxRecvDataSegmentLength = 262144node.conn[0].iscsi.MaxXmitDataSegmentLength = 0discovery.sendtargets.iscsi.MaxRecvDataSegmentLength = 32768node.conn[0].iscsi.HeaderDigest = Nonenode.session.nr_sessions = 1node.session.iscsi.FastAbort = Yes initiator重新发现并导入共享设备 123456789101112131415[root@rhel6 nodes]# iscsiadm -m discovery -t st -p 172.25.30.10172.25.30.10:3260,1 iqn.2015-10.com.example-f30:vdb1-1g172.25.30.10:3260,1 iqn.2015-10.com.example-f30:vdb2-2g[root@rhel6 nodes]# iscsiadm -m node -T iqn.2015-10.com.example-f30:vdb1-1g -lLogging in to [iface: default, target: iqn.2015-10.com.example-f30:vdb1-1g, portal: 172.25.30.10,3260] (multiple)Login to [iface: default, target: iqn.2015-10.com.example-f30:vdb1-1g, portal: 172.25.30.10,3260] successful.[root@rhel6 nodes]# iscsiadm -m node -T iqn.2015-10.com.example-f30:vdb2-2g -lLogging in to [iface: default, target: iqn.2015-10.com.example-f30:vdb2-2g, portal: 172.25.30.10,3260] (multiple)Login to [iface: default, target: iqn.2015-10.com.example-f30:vdb2-2g, portal: 172.25.30.10,3260] successful.[root@rhel6 nodes]# ll /dev/sd*brw-rw----. 1 root disk 8, 16 Jan 14 16:04 /dev/sdbbrw-rw----. 1 root disk 8, 32 Jan 14 16:04 /dev/sdc","categories":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/tags/linux/"},{"name":"iscsi","slug":"iscsi","permalink":"https://blog.xiemx.com/tags/iscsi/"},{"name":"store","slug":"store","permalink":"https://blog.xiemx.com/tags/store/"}],"author":"xiemx"},{"title":"LVS-DR集群构建","slug":"2016-02-18-lvs-dr","date":"2016-02-17T20:02:11.000Z","updated":"2019-10-19T11:41:03.491Z","comments":false,"path":"/2016/02/18/2016-02-18-lvs-dr/","link":"","permalink":"https://blog.xiemx.com/2016/02/18/2016-02-18-lvs-dr/","excerpt":"","text":"设置端口标记的规则 12345#!/bin/bashVIP=$1IPTABLES=/sbin/iptables$IPTABLES -t mangle -A PREROUTING -p tcp -d $VIP --dport 80 -j MARK --set-mark 100$IPTABLES -t mangle -A PREROUTING -p tcp -d $VIP --dport 443 -j MARK --set-mark 100 ARP防火墙设置 1234567891011#!/bin/bashVIP=192.168.0.100RIP=192.168.0.xDGW=172.25.0.254DGWMAC=52:54:00:00:00:fearptables -Farptables -A IN -d $VIP -j DROParptables -A OUT -s $VIP -j mangle --mangle-ip-s $RIP/sbin/ifconfig eth0:1 $VIP broadcast $VIP netmask 255.255.255.0 uparp -s $DGW $DGWMAC/sbin/route add default gw $DGW 检测脚本 1234567#!/bin/bash/usr/bin/links -dump 1 $1 &gt;/dev/null 2&gt;&amp;1if [ 0 -eq $? ] ; then echo okelse echo failfi 主配置文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980[root@lvs-f30 ~]# cat /etc/sysconfig/ha/lvs.cfserial_no = 25primary = 172.25.30.14primary_private = 192.168.122.246service = lvsbackup_active = 1backup = 172.25.30.15backup_private = 192.168.122.247heartbeat = 1heartbeat_port = 539keepalive = 6deadtime = 18network = directdebug_level = NONEmonitor_links = 0syncdaemon = 0syncd_iface = eth0virtual http &#123; active = 1 address = 172.25.30.100 eth0:1 vip_nmask = 255.255.255.0 fwmark = 100 port = 80 send = &quot;GET / HTTP/1.0\\r\\n\\r\\n&quot; expect = &quot;ok&quot; use_regex = 0 send_program = &quot;/bin/testlink %h&quot; load_monitor = none scheduler = wlc protocol = tcp timeout = 6 reentry = 15 quiesce_server = 0 server node1 &#123; address = 192.168.122.224 active = 1 port = 80 weight = 1 &#125; server node2 &#123; address = 192.168.122.245 active = 1 port = 80 weight = 1 &#125;&#125;virtual https &#123; active = 1 address = 172.25.30.100 eth0:1 vip_nmask = 255.255.255.0 fwmark = 100 port = 443 send = &quot;GET / HTTP/1.0\\r\\n\\r\\n&quot; expect = &quot;HTTP&quot; use_regex = 0 load_monitor = none scheduler = wlc protocol = tcp timeout = 6 reentry = 15 quiesce_server = 0 server node1 &#123; address = 192.168.122.224 active = 1 port = 443 weight = 1 &#125; server node2 &#123; address = 192.168.122.245 active = 1 port = 443 weight = 1 &#125;&#125;","categories":[{"name":"lvs","slug":"lvs","permalink":"https://blog.xiemx.com/categories/lvs/"}],"tags":[{"name":"cluster","slug":"cluster","permalink":"https://blog.xiemx.com/tags/cluster/"},{"name":"lvs","slug":"lvs","permalink":"https://blog.xiemx.com/tags/lvs/"}],"author":"xiemx"},{"title":"LVS-NAT集群构建","slug":"2016-02-18-lvs-nat-cluster","date":"2016-02-17T19:02:13.000Z","updated":"2019-10-19T11:51:41.152Z","comments":false,"path":"/2016/02/18/2016-02-18-lvs-nat-cluster/","link":"","permalink":"https://blog.xiemx.com/2016/02/18/2016-02-18-lvs-nat-cluster/","excerpt":"","text":"由于lvs基于内核实现的负载均衡技术，因此主要是在内核层面配置，软件层面需要配置的东西很少。我们是通过软件生成配置文件，再讲配置文件刷到内核中。 1234567891011121314#安装组包yum groupinstall \"Load Balancer\"#启动图形化服务/etc/init.d/piranha-gui start#创建piranha用户密码piranha-passwd#通过图形化创建的配置文件存放在/etc/sysconfig/ha/lvs.cf#将配置刷到内核模块中ip_vs和ip_vs*/etc/init.d/pulse start 123456789#测试脚本/bin/testlink[root@lvs-f30 ~]# cat /bin/testlink#!/bin/bash/usr/bin/links -dump 1 $1 &gt;/dev/null 2&gt;&amp;1if [ 0 -eq $? ] ; thenecho okelseecho failfi 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[root@lvs-f30 ~]# cat /etc/sysconfig/ha/lvs.cfserial_no = 30primary = 172.25.30.14primary_private = 192.168.122.246service = lvsbackup_active = 1backup = 172.25.30.15backup_private = 192.168.122.247heartbeat = 1heartbeat_port = 539keepalive = 6deadtime = 18network = natnat_router = 192.168.122.254 eth2nat_nmask = 255.255.255.255debug_level = NONEmonitor_links = 0syncdaemon = 0syncd_iface = eth2virtual http &#123; active = 1 address = 172.25.30.100 eth0:1 vip_nmask = 255.255.255.0 port = 80 send = &quot;GET / HTTP/1.0\\r\\n\\r\\n&quot; expect = &quot;ok&quot; use_regex = 0 send_program = &quot;/bin/testlink %h&quot; load_monitor = none scheduler = wlc protocol = tcp timeout = 6 reentry = 15 quiesce_server = 0 server node1 &#123; address = 192.168.122.224 active = 1 port = 80 weight = 1 &#125; server node2 &#123; address = 192.168.122.245 active = 1 port = 80 weight = 1 ​&#125;&#125;","categories":[{"name":"lvs","slug":"lvs","permalink":"https://blog.xiemx.com/categories/lvs/"}],"tags":[{"name":"cluster","slug":"cluster","permalink":"https://blog.xiemx.com/tags/cluster/"},{"name":"lvs","slug":"lvs","permalink":"https://blog.xiemx.com/tags/lvs/"}],"author":"xiemx"},{"title":"LVS的三种路由技术","slug":"2016-02-18-lvs-three-mode","date":"2016-02-17T19:02:10.000Z","updated":"2019-10-19T11:50:16.100Z","comments":false,"path":"/2016/02/18/2016-02-18-lvs-three-mode/","link":"","permalink":"https://blog.xiemx.com/2016/02/18/2016-02-18-lvs-three-mode/","excerpt":"","text":"目前LVS的三种路由方式 Virtual Server via Network Address Translation（VS-NAT） Virtual Server via Direct Routing（VS-DR） Virtual Server via IP Tunneling（VS-TUN）","categories":[{"name":"lvs","slug":"lvs","permalink":"https://blog.xiemx.com/categories/lvs/"}],"tags":[{"name":"cluster","slug":"cluster","permalink":"https://blog.xiemx.com/tags/cluster/"},{"name":"lvs","slug":"lvs","permalink":"https://blog.xiemx.com/tags/lvs/"}],"author":"xiemx"},{"title":"rhcs搭建HA集群","slug":"2016-02-01-rhcs-ha-cluster","date":"2016-02-01T04:02:16.000Z","updated":"2019-10-19T08:27:09.995Z","comments":true,"path":"/2016/02/01/2016-02-01-rhcs-ha-cluster/","link":"","permalink":"https://blog.xiemx.com/2016/02/01/2016-02-01-rhcs-ha-cluster/","excerpt":"","text":"YUM配置HA集群(图形化) RHEL和Centos的光盘中自带有红帽的conga套件。将安装源指向光盘即可yum来安装，也可以将luci和ricci拷贝出来直接通过”yum localinstall luci ricci”来安装。其中luci是web界面的图形化配置工具，ricci为同步配置文件的工具。ricci运行需要用到ricci账户权限，安装ricci时设定下ricci密码。 安装设置步骤 1.所有节点进行环境初始化 时间日期（date） elinux 防火墙（iptables） 主机名（/etc/hosts和/etc/sysconfig/network） 网卡网络(stop NetworkManager) yum源 2.在所有节点上安装ricci，在其中一台节点上安装luci，并配置ricci密码。 123[root@localhost ~]# yum install luci ricci -y[root@localhost ~]# yum install ricci -y[root@localhost ~]# echo 123123123 | passwd --stdin ricci 3.启动luci进入web配置界面，可使用系统root账户登录luci 12345678910111213141516[root@localhost ~]# /etc/init.d/luci startStart luci... [ OK ]Point your web browser to https://localhost.localdomain:8084 (or equivalent) to access luci[![ha](/images/ha.png)](http://www.xiemx.com/wp-content/uploads/2016/01/ha.png)[![hainstall](/images/hainstall.png)](http://www.xiemx.com/wp-content/uploads/2016/02/hainstall.png)在双机集群的基础上添加一个节点进去实现多机集群，这里使用非图形化安装安装 High Availability 组包[root@node3 ~]# yum groupinstall \"High Availability\"设置ricci用户密码:[root@node1 ~]# echo uplooking | passwd --stdin ricciChanging password for user ricci.passwd: all authentication tokens updated successfully. 修改配置文件/etc/cluster/cluster.conf并同步到所有结点上 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;cluster config_version=\"12\" name=\"f30Cluster\"&gt; &lt;clusternodes&gt; &lt;clusternode name=\"node1.xiemx.com\" nodeid=\"1\"&gt; &lt;fence&gt; &lt;method name=\"FenceMethod\"&gt; &lt;device domain=\"node1\" name=\"Fence1\"/&gt; &lt;/method&gt; &lt;/fence&gt; &lt;/clusternode&gt; &lt;clusternode name=\"node2.xiemx.com\" nodeid=\"2\"&gt; &lt;fence&gt; &lt;method name=\"FenceMethod\"&gt; &lt;device domain=\"node2\" name=\"Fence2\"/&gt; &lt;/method&gt; &lt;/fence&gt; &lt;/clusternode&gt; &lt;clusternode name=\"node3.xiemx.com\" nodeid=\"3\"&gt; &lt;fence&gt; &lt;method name=\"FenceMethod\"&gt; &lt;device domain=\"node3\" name=\"Fence3\"/&gt; &lt;/method&gt; &lt;/fence&gt; &lt;/clusternode&gt; &lt;/clusternodes&gt; &lt;cman expected_votes=\"1\"/&gt; &lt;fencedevices&gt; &lt;fencedevice agent=\"fence_xvm\" name=\"Fence1\"/&gt; &lt;fencedevice agent=\"fence_xvm\" name=\"Fence2\"/&gt; &lt;fencedevice agent=\"fence_xvm\" name=\"Fence3\"/&gt; &lt;/fencedevices&gt; &lt;rm&gt; &lt;failoverdomains&gt; &lt;failoverdomain name=\"Domain1\" restricted=\"1\"&gt; &lt;failoverdomainnode name=\"node1.xiemx.com\"/&gt; &lt;failoverdomainnode name=\"node2.xiemx.com\"/&gt; &lt;failoverdomainnode name=\"node3.xiemx.com\"/&gt; &lt;/failoverdomain&gt; &lt;/failoverdomains&gt; &lt;resources&gt; &lt;ip address=\"172.25.30.100/24\" sleeptime=\"10\"/&gt; &lt;script file=\"/etc/init.d/httpd\" name=\"httpd\"/&gt; &lt;/resources&gt; &lt;service domain=\"Domain1\" name=\"httpd\" recovery=\"restart\"&gt; &lt;ip ref=\"172.25.30.100/24\"/&gt; &lt;script ref=\"httpd\"/&gt; &lt;/service&gt; &lt;/rm&gt; &lt;/cluster&gt; 同步配置:要保证所有节点的ricci服务已启动，且ricci账户都配置密码 1234567[root@node1-f30 ~]# cman_tool version -rYou have not authenticated to the ricci daemon on node1-f30.example.comPassword:You have not authenticated to the ricci daemon on node3-f30.example.comPassword:You have not authenticated to the ricci daemon on node2-f30.example.comPassword: 启动cman，rgmanager，modcluster服务 1234567891011121314151617181920212223[root@node3-f30 ~]# /etc/init.d/cman startStarting cluster:Checking if cluster has been disabled at boot... [ OK ]Checking Network Manager... [ OK ]Global setup... [ OK ]Loading kernel modules... [ OK ]Mounting configfs... [ OK ]Starting cman... [ OK ]Waiting for quorum... [ OK ]Starting fenced... [ OK ]Starting dlm_controld... [ OK ]Tuning DLM kernel config... [ OK ]Starting gfs_controld... [ OK ]Unfencing self... [ OK ]Joining fence domain... [ OK ][root@node3-f30 ~]# /etc/init.d/rgmanager startStarting Cluster Service Manager: [ OK ][root@node3-f30 ~]# /etc/init.d/modclusterd startStarting Cluster Module - cluster monitor: Setting verbosity level to LogBasic[ OK ] 切换资源到node3节点上去运行，测试节点是否正常: 12345678910111213[root@node1-f30 ~]# clusvcadm -r httpd -m node3-f30.example.comTrying to relocate service:httpd to node3-f30.example.com...Successservice:httpd is now running on node3-f30.example.com同步fence_xvm.key:[root@node1-f30 ~]# scp /etc/cluster/fence_xvm.key 172.25.30.12:/etc/cluster/fence_xvm.keyThe authenticity of host '172.25.30.12 (172.25.30.12)' can't be established.RSA key fingerprint is cf:7c:26:aa:4f:41:7b:21:5e:09:ce:8a:15:2c:97:32.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added '172.25.30.12' (RSA) to the list of known hosts.root@172.25.30.12's password:fence_xvm.key 100% 512 0.5KB/s 00:00 Qdisk仲裁盘添加： 在新的机器上共享一个磁盘出来加入到集群中,共享磁盘需要用到scsi-target-utils工具包，可yum安装获得 12345678910111213141516171819202122232425262728293031323334[root@rhel6 ~]# yum install scsi*将配置文件中的如下3行配置取消注释修改需要添加磁盘[root@rhel6 ~]# vi /etc/tgt/targets.conf&lt;target iqn.2008-09.com.example:server.target1&gt;backing-store /dev/vdb1&lt;/target&gt;集群的节点中安装包 iscsi-initiator-utils[root@node1-f30 ~]# yum install scsi*添加仲裁盘到本地,格式化仲裁盘iscsiadm -m discovery -t st -p 172.25.30.13iscsiadm -m node -lmkqdisk -c /dev/sda -l myqdisk[root@node3 ~]# iscsiadm -m discovery -t st -p 172.25.30.13Starting iscsid: [ OK ]172.25.30.13:3260,1 iqn.2008-09.com.example:server.target1[root@node3 ~]# iscsiadm -m node -lLogging in to [iface: default, target: iqn.2008-09.com.example:server.target1, portal: 172.25.30.13,3260] (multiple)Login to [iface: default, target: iqn.2008-09.com.example:server.target1, portal: 172.25.30.13,3260] successful.[root@node3 ~]# mkqdisk -c /dev/sda -l myqdiskmkqdisk v3.0.12.1Writing new quorum disk label 'myqdisk' to /dev/sda.WARNING: About to destroy all data on /dev/sda; proceed [N/y] ? yWarning: Initializing previously initialized partitionInitializing status block for node 1...Initializing status block for node 2... 在luci图形界面中添加仲裁到集群 也可以通过修改配置文件然后通过cman_tool version -r来同步到集群中的每个节点上 配置文件: 123&lt;quorumd label=\"myqdisk\" min_score=\"1\"&gt;&lt;heuristic interval=\"5\" program=\"ping 172.25.30.254 -c 1\" tko=\"2\"/&gt;&lt;/quorumd&gt; 同步配置文件时要注意修改版本号","categories":[{"name":"cluster","slug":"cluster","permalink":"https://blog.xiemx.com/categories/cluster/"}],"tags":[{"name":"rhcs","slug":"rhcs","permalink":"https://blog.xiemx.com/tags/rhcs/"},{"name":"cluster","slug":"cluster","permalink":"https://blog.xiemx.com/tags/cluster/"}],"author":"xiemx"},{"title":"mysql master-slave","slug":"2016-01-31-mysql-master-slave","date":"2016-01-31T04:01:41.000Z","updated":"2019-10-19T11:05:37.792Z","comments":false,"path":"/2016/01/31/2016-01-31-mysql-master-slave/","link":"","permalink":"https://blog.xiemx.com/2016/01/31/2016-01-31-mysql-master-slave/","excerpt":"","text":"Mysql复制（replication）是一个异步的复制，从一个Mysql 实例（Master）复制到另一个Mysql 实例（Slave）。实现整个主从复制，需要由Master服务器上的IO进程，和Slave服务器上的Sql进程和IO进程共从完成。要实现主从复制，首先必须打开Master端的binary log（bin-log）功能，因为整个 MySQL 复制过程实际上就是Slave从Master端获取相应的二进制日志，然后再在自己slave端完全顺序的执行日志中所记录的各种操作。 （二进制日志几乎记录了除select以外的所有针对数据库的sql操作语句）主从同步也称之为AB复制。 基本过程： Slave端的IO进程连接上Master，向Master请求指定日志文件的指定position后的日志内容； Master接收到来自Slave的IO进程的请求后，负责复制的IO进程根据Slave的请求信息，读取相应日志内容，返回给Slave的IO进程。并将本次请求读取的bin-log文件名及位置一起返回给Slave端。 Slave的IO进程接收到信息后，将接收到的日志内容依次添加到Slave端的relay-log文件的最末端，并将读取到的Master端的bin-log的文件名和位置记录到master-info文件中，以便在下一次读取的时候能够清楚的告诉Master“我需要从某个bin-log的哪个位置开始往后的日志内容，请发给我”。 Slave的Sql进程检测到relay-log中新增加了内容后，会解析relay-log的内容成为在Master端真实执行时候的那些可执行的内容，并在自身执行。 ###数据库主从同步： 12主：servera 172.25.30.10 从：serverb 172.25.30.11 数据库的主从同步（AB复制），要保证数据的一致性，首先将主上的数据库备份出来，在从机上恢复一下保证基础的数据相同。从上述过程中我们可以得知slave是通过获取master的二进制日志来重演来达到数据同步的，因此master需要开启二进制日志，slave来获取二进制日志时使用什么账户，账号具有什么权限需要在master端定义，salve需要知道谁是master，用什么账户、同步哪个二进制文件、从二进制文件的哪个position开始同步。另外还需要设置下主从的server id。 由上推导出操作步骤 master/slave先设置server-id，master在开启二进制日志功能log-bin 123456789101112[root@localhost mysql]# cat /etc/my.cnf[mysqld]server-id=1 server-id数字任选，但不可重复datadir=/var/lib/mysqllog-bin=/var/log/mysql/binlog slave不需要开启此功能socket=/var/lib/mysql/mysql.sockuser=mysqllog-slow-queries=/var/log/mysql/slowlogsymbolic-links=0[mysqld_safe]log-error=/var/log/mysqld.logpid-file=/var/run/mysqld/mysqld.pid master创建同步用户，并授权 1mysql&gt;grant replication slave on *.* to xiemx@‘172.25.30.%’ identified by '123123';flush privileges; slave端指定master信息 123mysql&gt;change master to master_host='172.25.30.10',master_user='xiemx',master_password='123123',master_log_file=\"binlog.000003\"，master_log_pos=245;mysql&gt;slave start； 查看salve状态信息； 1234567891011121314151617181920212223242526272829303132333435mysql&gt; show slave status \\G; \\G表示将行和列翻转过来显示*************************** 1. row ***************************Slave_IO_State: Waiting for master to send eventMaster_Host: 172.25.30.11Master_User: xiemxMaster_Port: 4331Connect_Retry: 60Master_Log_File: binlog.000003Read_Master_Log_Pos: 245Relay_Log_File: mysqld-relay-bin.000001 中继日志Relay_Log_Pos: 245Slave_IO_Running: Yes slave的io是否正常运行Slave_SQL_Running: Yes slave的sql是否正常运行，需要注意两个都为yes也不一定正常。但有一个no肯定不正常。我们需要在测试下读写。Replicate_Do_DB:Replicate_Ignore_DB: mysqlReplicate_Do_Table:Replicate_Ignore_Table:Replicate_Wild_Do_Table: photo.%Replicate_Wild_Ignore_Table: mysql.%Last_Errno: 0Last_Error:Skip_Counter: 0Exec_Master_Log_Pos: 13456620Relay_Log_Space: 36764898503Until_Condition: NoneUntil_Log_File:Until_Log_Pos: 0Master_SSL_Allowed: NoMaster_SSL_CA_File:Master_SSL_CA_Path:Master_SSL_Cert:Master_SSL_Cipher:Master_SSL_Key:Seconds_Behind_Master: 249904×××××××××××××××××××××××××××××××××××××××××××××××××××××××××× 测试主从是否同步 由于slave是从master获取二进制日志来同步数据的，因此在slave上不应该有任何非查询类的sql语句执行，如果slave执行了sql语句操作了某个数据，当master操作这个数据时，salve通过二进制日志重演去操作时发生报错，会导致主从断开。如果主从断开我们就需要重新change master来建立主从。因此切记在单向主从同步的环境中，不可操作slave去执行非查询类操作。 我们在master上创建一个xiemx数据库，查看下slave是否会自动创建出来。 12345678910mysql&gt; create database xiemx；show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || test || xiemx |+--------------------+4 rows in set (0.00 sec) 如果如果在slave上show databases；结果和上面相同的则说明主从同步设置成功。","categories":[{"name":"mysql","slug":"mysql","permalink":"https://blog.xiemx.com/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://blog.xiemx.com/tags/mysql/"}],"author":"xiemx"},{"title":"mysql master-master","slug":"2016-01-31-mysql-master-master","date":"2016-01-31T04:01:23.000Z","updated":"2019-10-19T11:00:27.494Z","comments":false,"path":"/2016/01/31/2016-01-31-mysql-master-master/","link":"","permalink":"https://blog.xiemx.com/2016/01/31/2016-01-31-mysql-master-master/","excerpt":"","text":"原理同单向主从相同，都是salve获取master的二进制日志来重演数据，在互为主从的架构中，servera既是master又是slave，通用的serverb也是master和slave，servera回去同步serverb的数据，serverb也会去同步servera的操作。已达到数据的一致性。 由原理可以得知： servera需要开启二进制日志授权serverb来同步，servera需要change master指向serverb serverb需要开启二进制日志授权servera来同步，serverb需要change master指向servera，另外serverb的数据是通过master来同步到中继日志中来重演生成的，因此需要在开启将中继日志写入二进制日志功能log_slave_update=1 需要注意的是，在设置双向主从时，我们是先设置一条单向主从(servera-serverb)，在设置另一条单向主从(serverb-servera)。我们在设置完第一条单向主从(servera-serverb)成功时，slave(serverb)上是不能执行任何非查询语句的，因此第二条单向主从(serverb-servera)在serverb上设置servera的同步账户时grant授权语句需要通过(servera-serverb)这条单向主从的master(servera)来传递给salve(serverb)。 操作步骤 设置server-id，开启二进制日志，serverb需要开启中继日志写入二进制日志功能 123456789101112[root@localhost mysql]# cat /etc/my.cnf[mysqld]server-id=1 master/slave数字不可重复datadir=/var/lib/mysqllog-bin=/var/log/mysql/binlogsocket=/var/lib/mysql/mysql.socklog_slave_update=1 serverb上添加此项log-slow-queries=/var/log/mysql/slowlogsymbolic-links=0[mysqld_safe]log-error=/var/log/mysqld.logpid-file=/var/run/mysqld/mysqld.pid servera创建同步用户，并授权 1mysql&gt;grant replication slave on *.* to xiemx1@‘172.25.30.%’ identified by '123123';flush privileges; serverb指定master信息 12345mysql&gt;change master to master_host='172.25.30.10',master_user='xiemx1',master_password='123123',master_log_file=\"binlog.000003\"，master_log_pos=245;mysql&gt;slave start；mysql&gt;show slave status\\G;查看io和sql是否yes，在测试主从是否生效，在保障主从生效的情况下执行下列操作。 servera上授权第二条单向的同步账户 1mysql&gt;grant replication slave on *.* to xiemx2@‘172.25.30.%’ identified by '123123';flush privileges; servera指定master信息 12345mysql&gt;change master to master_host='172.25.30.11',master_user='xiemx2',master_password='123123',master_log_file=\"binlog.000001\"，master_log_pos=245;mysql&gt;slave start；mysql&gt;show slave status\\G;查看io和sql是否为yes 在servera上创建一个库看serverb是否会自动生成，在serverb上创建库看servera是否会自动生成。如果都没有问题的话说明设置成功。show master status\\G; 可以查看当前的二进制日志文件和position号","categories":[{"name":"mysql","slug":"mysql","permalink":"https://blog.xiemx.com/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://blog.xiemx.com/tags/mysql/"}],"author":"xiemx"},{"title":"四种事务隔离级别","slug":"2016-01-31-transaction-isolation-level","date":"2016-01-30T21:01:30.000Z","updated":"2019-09-27T06:52:36.183Z","comments":false,"path":"/2016/01/31/2016-01-31-transaction-isolation-level/","link":"","permalink":"https://blog.xiemx.com/2016/01/31/2016-01-31-transaction-isolation-level/","excerpt":"","text":"数据库系统提供了四种事务隔离级别： A.Serializable（串行化）：一个事务在执行过程中完全看不到其他事务对数据库所做的更新（事务执行的时候不允许别的事务并发执行。事务串行化执行，事务只能一个接着一个地执行，而不能并发执行。 B.Repeatable Read（可重复读）：一个事务在执行过程中可以看到其他事务已经提交的新插入的记录，但是不能看到其他其他事务对已有记录的更新。 C.Read Commited（读已提交数据）：一个事务在执行过程中可以看到其他事务已经提交的新插入的记录，而且能看到其他事务已经提交的对已有记录的更新。 D.Read Uncommitted（读未提交数据）：一个事务在执行过程中可以看到其他事务没有提交的新插入的记录，而且能看到其他事务没有提交的对已有记录的更新。","categories":[{"name":"mysql","slug":"mysql","permalink":"https://blog.xiemx.com/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://blog.xiemx.com/tags/mysql/"}],"author":"xiemx"},{"title":"mysql binlog恢复数据","slug":"2016-01-30-mysql-binlog-recovery-data","date":"2016-01-30T03:01:53.000Z","updated":"2019-10-19T10:48:03.517Z","comments":false,"path":"/2016/01/30/2016-01-30-mysql-binlog-recovery-data/","link":"","permalink":"https://blog.xiemx.com/2016/01/30/2016-01-30-mysql-binlog-recovery-data/","excerpt":"","text":"123456789101112131415[root@localhost mysql]# cat /etc/my.cnf[mysqld]datadir=/var/lib/mysqllog-bin=/var/log/mysql/binlog 开启二进制日志socket=/var/lib/mysql/mysql.sockuser=mysqllog-slow-queries=/var/log/mysql/slowlogsymbolic-links=0[mysqld_safe]log-error=/var/log/mysqld.logpid-file=/var/run/mysqld/mysqld.pid[root@localhost mysql]# cd /var/log/mysql[root@localhost mysql]# lsbinlog.000001 binlog.000002 binlog.000003 binlog.index 二进制生成的文件 查看数据库的日志相关设置： 1234567891011121314151617181920212223242526272829303132333435363738394041424344mysql&gt; SHOW GLOBAL VARIABLES LIKE '%log%';+-----------------------------------------+----------------------------+| Variable_name | Value |+-----------------------------------------+----------------------------+| back_log | 50 || binlog_cache_size | 32768 || binlog_direct_non_transactional_updates | OFF || binlog_format | STATEMENT || expire_logs_days | 0 || general_log | OFF || general_log_file | /var/run/mysqld/mysqld.log || innodb_flush_log_at_trx_commit | 1 || innodb_locks_unsafe_for_binlog | OFF || innodb_log_buffer_size | 1048576 || innodb_log_file_size | 5242880 || innodb_log_files_in_group | 2 || innodb_log_group_home_dir | ./ || innodb_mirrored_log_groups | 1 || log | OFF || log_bin | ON | 查看二进制日志是否开启| log_bin_trust_function_creators | OFF || log_bin_trust_routine_creators | OFF || log_error | /var/log/mysqld.log | 错误日志| log_output | FILE || log_queries_not_using_indexes | OFF || log_slave_updates | OFF || log_slow_queries | ON || log_warnings | 1 || max_binlog_cache_size | 18446744073709547520 || max_binlog_size | 1073741824 || max_relay_log_size | 0 || relay_log | | 中继日志| relay_log_index | || relay_log_info_file | relay-log.info || relay_log_purge | ON || relay_log_space_limit | 0 || slow_query_log | ON | 慢查询日志| slow_query_log_file | /var/log/mysql/slowlog || sql_log_bin | ON || sql_log_off | OFF || sql_log_update | ON || sync_binlog | 0 |+-----------------------------------------+----------------------------+38 rows in set (0.00 sec) 查看二进制日志文件：二进制文件不是文本文档无法使用cat等于都文本的命令查看，需使用mysqlbinlog命令查看。 1234567891011121314151617181920212223242526272829[root@localhost mysql]# mysqlbinlog binlog.000003/*!40019 SET @@session.max_insert_delayed_threads=0*/;/*!50003 SET @OLD_COMPLETION_TYPE=@@COMPLETION_TYPE,COMPLETION_TYPE=0*/;DELIMITER /*!*/;# at 4#160110 16:41:49 server id 1 end_log_pos 106 Start: binlog v 4, server v 5.1.73-log created 160110 16:41:49 at startup# Warning: this binlog is either in use or was not closed properly.ROLLBACK/*!*/;BINLOG 'TRmSVg8BAAAAZgAAAGoAAAABAAQANS4xLjczLWxvZwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABNGZJWEzgNAAgAEgAEBAQEEgAAUwAEGggAAAAICAgC'/*!*/;# at 106 position号标记#160110 17:26:15 server id 1 end_log_pos 191 Query thread_id=3 exec_time=0 error_code=0 时间标记160110表示2016-01-10SET TIMESTAMP=1452417975/*!*/;SET @@session.pseudo_thread_id=3/*!*/;SET @@session.foreign_key_checks=1, @@session.sql_auto_is_null=1, @@session.unique_checks=1, @@session.autocommit=1/*!*/;SET @@session.sql_mode=0/*!*/;SET @@session.auto_increment_increment=1, @@session.auto_increment_offset=1/*!*/;/*!\\C latin1 *//*!*/;SET @@session.character_set_client=8,@@session.collation_connection=8,@@session.collation_server=8/*!*/;SET @@session.lc_time_names=0/*!*/;SET @@session.collation_database=DEFAULT/*!*/;create database xiemx 对数据库的操作记录/*!*/;DELIMITER ;# End of log fileROLLBACK /* added by mysqlbinlog */;/*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/; 二进制恢复命令： 1234567[root @server1 mysql ] #mysqlbinlog --start-date=\"2015-06-18 9:55:00\" --stop-date=\"2015-06-18 10:05:00\" /var/log/mysql/binlog.000006 | mysql -uroot -p从二进制日志binlog.000006的2015-06-18 9:55:00这个时间到2015-06-18 10:05:00之间的操作[root @server1 mysql ] #mysqlbinlog --stop-position=\"368312\" /var/log/mysql/binlog.000006 | mysql -u root -p 从二进制日志binlog.000006的开头恢复到368312这个position标记结束[root @server1 mysql ] #mysqlbinlog --start-position=\"368315\" /var/log/mysql/bin.000006 | mysql -u root -p 从二进制日志binlog.000006的368315这个position标记恢复到日志结束 在使用二进制恢复数据库时可以使用position标记和时间标记，假设我们在数据库操作中误执行了错误的操作我们可以用position和时间标记来跳 过错误的操作不执行，恢复出数据库。二进制日志恢复数据库是基于某个完整备份的基础上。因此在备份数据库时我们可以使用flush log命令来刷新二进制文件，记录下文件名。这样下次数据库出现问题我们就可以用手上的二进制文件和备份来恢复数据。","categories":[{"name":"mysql","slug":"mysql","permalink":"https://blog.xiemx.com/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://blog.xiemx.com/tags/mysql/"},{"name":"binlog","slug":"binlog","permalink":"https://blog.xiemx.com/tags/binlog/"}],"author":"xiemx"},{"title":"Tomcat多实例运行","slug":"2016-01-30-multi-tomcat-server","date":"2016-01-29T21:01:29.000Z","updated":"2019-10-18T09:36:40.408Z","comments":false,"path":"/2016/01/30/2016-01-30-multi-tomcat-server/","link":"","permalink":"https://blog.xiemx.com/2016/01/30/2016-01-30-multi-tomcat-server/","excerpt":"","text":"一个程序默认在系统中都是维护一个进程（或一个主进程多个子进程），这样的结构在进程出错终止时会导致该进程下所有的网站都会打不开。但tomcat我们可以通过调整启动时指定的tomcat程序位置来启动多个tomcat程序，分属不同的进程这样在某个进程终止时也不会影响到其他进程，不会导致其它网站无法访问。但一个端口只能被一个进程监听，当我们启动多进程就不能同时监听8080端口，我们可以依次监听8081，8082等，如果要实现直接输入域名访问可以在前段增加nginx来做代理即可。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[root@serverc ~]# /etc/init.d/tomcat stop[root@serverc ~]# cd /home/tomcat/[root@serverc tomcat]# mkdir tomcat1 tomcat2[root@serverc tomcat]# cd apache-tomcat-8.0.24/[root@serverc apache-tomcat-8.0.24]# cp -rp logs/ temp/ tomcat1.com/ work/ webapps/ conf/ ../tomcat1/[root@serverc apache-tomcat-8.0.24]# cp -rp logs/ temp/ tomcat2.com/ work/ webapps/ conf/ ../tomcat2/[root@serverc apache-tomcat-8.0.24]# rm -rf LICENSE NOTICE RELEASE-NOTES RUNNING.txt conf logs tomcat1.com tomcat2.com webapps[root@serverc apache-tomcat-8.0.24]# lsbin lib temp work[root@serverc apache-tomcat-8.0.24]# cd ../tomcat1/[root@serverc tomcat1]# vim conf/server.xml&lt;Host name=\"www.tomcat1.com\" appBase=\"tomcat1.com\"unpackWARs=\"true\" autoDeploy=\"true\"&gt;&lt;Valve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs\"prefix=\"localhost_access_log\" suffix=\".txt\"pattern=\"%h %l %u %t &amp;quot;%r&amp;quot; %s %b\" /&gt;&lt;/Host&gt;[root@serverc tomcat1]# cd ../tomcat2/[root@serverc tomcat2]# vim conf/server.xml&lt;Host name=\"www.tomcat2.com\" appBase=\"tomcat2.com\"unpackWARs=\"true\" autoDeploy=\"true\"&gt;&lt;Valve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs\"prefix=\"localhost_access_log\" suffix=\".txt\"pattern=\"%h %l %u %t &amp;quot;%r&amp;quot; %s %b\" /&gt;&lt;/Host&gt;=============================================================================&lt;Connector port=\"8081\" protocol=\"HTTP/1.1\"connectionTimeout=\"20000\"redirectPort=\"8443\" /&gt;=============================================================================&lt;Connector port=\"8010\" protocol=\"AJP/1.3\" redirectPort=\"8443\" /&gt;[root@serverc tomcat2]# cd /etc/init.d/[root@serverc init.d]# mv tomcat tomcat1[root@serverc init.d]# vim tomcat1export CATALINA_HOME=\"/home/tomcat/apache-tomcat-8.0.24/\"export CATALINA_BASE=\"/home/tomcat/tomcat1\"[root@serverc init.d]# cp tomcat1 tomcat2[root@serverc init.d]# vim tomcat2export CATALINA_HOME=\"/home/tomcat/apache-tomcat-8.0.24/\"export CATALINA_BASE=\"/home/tomcat/tomcat2\"[root@serverc init.d]# /etc/init.d/tomcat1 start[root@serverc init.d]# /etc/init.d/tomcat2 start[root@serverc init.d]# netstat -ltunp | grep 8080tcp6 0 0 :::8080 :::* LISTEN 5749/jsvc.exec[root@serverc init.d]# netstat -ltunp | grep 8081tcp6 0 0 :::8081 :::* LISTEN 5782/jsvc.exec","categories":[{"name":"tomcat","slug":"tomcat","permalink":"https://blog.xiemx.com/categories/tomcat/"}],"tags":[{"name":"http","slug":"http","permalink":"https://blog.xiemx.com/tags/http/"},{"name":"tomcat","slug":"tomcat","permalink":"https://blog.xiemx.com/tags/tomcat/"},{"name":"webserver","slug":"webserver","permalink":"https://blog.xiemx.com/tags/webserver/"}],"author":"xiemx"},{"title":"Tomcat创建虚拟主机","slug":"2016-01-30-tomcat-virtual-host","date":"2016-01-29T20:01:28.000Z","updated":"2019-10-19T07:20:12.516Z","comments":true,"path":"/2016/01/30/2016-01-30-tomcat-virtual-host/","link":"","permalink":"https://blog.xiemx.com/2016/01/30/2016-01-30-tomcat-virtual-host/","excerpt":"","text":"1.修改tomcat服务的主配置文件。该文件位于tomcat主程序下conf目录中，文件名称为server.xml 123[root@serverc ~]# cd /home/tomcat/apache-tomcat-8.0.24/conf/ [root@serverc conf]# ls -l server.xml -rw------- 1 tomcat root 6458 Jul 2 04:23 server.xml 2.编辑该文件，添加虚拟主机。host字段为tomcat服务的虚拟主机配置字段。配置两个虚拟主机，www.tomcat1.com和www.tomcat2.com，两台虚拟主机有自己的appbase（即网页文件根目录，下图中使用的是相对路径表示虚拟主机网页根目录，该路径是相对于tomcat服务主程序所在目录来说，该路径现不存在，后续再创建）和相关日志前缀。注name字段为虚拟主机名，appBase为根目录，uppackwars为自动解压是否开启，autoDeploy为自动部署是否开启 123456789101112&lt;Host name=\"www.tomcat1.com\" appBase=\"tomcat1.com\"unpackWARs=\"true\" autoDeploy=\"true\"&gt;&lt;Valve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs\"prefix=\"localhost_access_log\" suffix=\".txt\"pattern=\"%h %l %u %t \"%r\" %s %b\" /&gt;&lt;/Host&gt;&lt;Host name=\"www.tomcat2.com\" appBase=\"tomcat2.com\"unpackWARs=\"true\" autoDeploy=\"true\"&gt;&lt;Valve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs\"prefix=\"localhost_access_log\" suffix=\".txt\"pattern=\"%h %l %u %t \"%r\" %s %b\" /&gt;&lt;/Host&gt; 3.进入tomcat服务主程序所在目录，创建上述步骤中虚拟主机所指定的appBase，分别进入每一个虚拟主机的appBase目录下创建ROOT目录，在ROOT目录写每一个虚拟主机对应的首页文件。 12345678910[root@serverc conf]# cd /home/tomcat/apache-tomcat-8.0.24/ [root@serverc apache-tomcat-8.0.24]# mkdir tomcat1.com/ [root@serverc apache-tomcat-8.0.24]# cd tomcat1.com/ [root@serverc tomcat1.com]# mkdir ROOT [root@serverc tomcat1.com]# echo tomcat1 &gt; ROOT/index.html [root@serverc tomcat1.com]# cd ../ [root@serverc apache-tomcat-8.0.24]# mkdir tomcat2.com [root@serverc apache-tomcat-8.0.24]# cd tomcat2.com [root@serverc tomcat2.com]# mkdir ROOT [root@serverc tomcat2.com]# echo tomcat2 &gt; ROOT/index.html 4.重启tomcat服务 12[root@serverc tomcat2.com]# /etc/init.d/tomcat stop[root@serverc tomcat2.com]# /etc/init.d/tomcat start 5.在client端机器上测试 123456[root@workstation ~]# echo 172.25.41.10 www.tomcat1.com &gt;&gt; /etc/hosts [root@workstation ~]# echo 172.25.41.10 www.tomcat2.com &gt;&gt; /etc/hosts[root@workstation ~]# curl www.tomcat1.com:8080tomcat1[root@workstation ~]# curl www.tomcat2.com:8080tomcat2","categories":[{"name":"tomcat","slug":"tomcat","permalink":"https://blog.xiemx.com/categories/tomcat/"}],"tags":[{"name":"http","slug":"http","permalink":"https://blog.xiemx.com/tags/http/"},{"name":"tomcat","slug":"tomcat","permalink":"https://blog.xiemx.com/tags/tomcat/"},{"name":"webserver","slug":"webserver","permalink":"https://blog.xiemx.com/tags/webserver/"}],"author":"xiemx"},{"title":"Tomcat搭建web服务器","slug":"2016-01-24-tomcat-server","date":"2016-01-24T02:01:58.000Z","updated":"2019-10-18T09:27:39.801Z","comments":false,"path":"/2016/01/24/2016-01-24-tomcat-server/","link":"","permalink":"https://blog.xiemx.com/2016/01/24/2016-01-24-tomcat-server/","excerpt":"","text":"tomcat是一款处理jsp页面的web套件，可以处理html页面和jsp页面。由于java语言的跨平台性，软件只要解压后即可运行，但运行java需要在系统中安装jdk的java虚拟机。tomcat的软件包可以去tomcat官网http://tomcat.apache.org获得，jdk的软件包需要去oracle官网下载`http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html`。 另外在部署tomcat虚拟机时我们一般会创建tomcat用户和组并将tomcat程序放在家目录下，家目录的权限为700，该目录只有tomcat用户有完整权限，其他用户无任何权限，可增加安全性。另一种部署软件的方法同我们平时安装源码软件相同，将软件放在/usr/lib/local目录下。这里就将软件部署在home/tomcat/目录下。 tomcat目录结构1234567891011121314[root@serverc tomcat]# tar xf apache-tomcat-8.0.24.tar.gz -C /home/tomcat/[root@serverc tomcat]# cd /home/tomcat/apache-tomcat-8.0.24/[root@serverc apache-tomcat-8.0.24]# ls -ldrwxr-xr-x 2 root root 4096 Dec 11 14:22 bin #存放命令drwxr-xr-x 2 root root 4096 Jul 2 04:23 conf #存放配置文件drwxr-xr-x 2 root root 4096 Dec 11 14:22 lib #存放库文件，java的库文件后缀为jar-rw-r--r-- 1 root root 57011 Jul 2 04:23 LICENSEdrwxr-xr-x 2 root root 6 Jul 2 04:20 logs #存放相关日志-rw-r--r-- 1 root root 1444 Jul 2 04:23 NOTICE-rw-r--r-- 1 root root 6741 Jul 2 04:23 RELEASE-NOTES-rw-r--r-- 1 root root 16204 Jul 2 04:23 RUNNING.txtdrwxr-xr-x 2 root root 29 Dec 11 14:22 temp #存放临时文件drwxr-xr-x 7 root root 76 Jul 2 04:21 webapps #默认网站网页文件根目录drwxr-xr-x 2 root root 6 Jul 2 04:20 work #工作目录 启动tomcat12345678910111213bin目录下有控制tomcat服务的启动关闭脚本，在启动jdk时我们声明下jdk的安装位置:export JAVA\\_HOME=\"/usr/java/jdk1.7.0\\_79/\" 声明tomcat程序中命令和库文件所在位置export CATALINA\\_HOME=\"/home/tomcat/apache-tomcat-8.0.24/\"声明tomcat程序中配置文件、网站根目录等所在位置export CATALINA\\_BASE=\"/home/tomcat/apache-tomcat-8.0.24/\"启动tomcat服务的脚本名称为startup.sh。执行该脚本可启动tomcat服务。该服务默认监听tcp/8080端口。关闭脚本为shutdown.sh。 [root@serverc ~]# cd /home/tomcat/apache-tomcat-8.0.24/bin/ [root@serverc bin]# ./startup.sh [root@serverc bin]# ps -ef | grep java [root@serverc bin]# netstat -ltunp | grep 8080 tcp6 0 0 :::8080 :::* LISTEN 1102/java 测试tomcat服务器 在浏览器中输入地址访问8080，http://192.168.17.10:8080 来测试tomcat服务器是否正常","categories":[{"name":"tomcat","slug":"tomcat","permalink":"https://blog.xiemx.com/categories/tomcat/"}],"tags":[{"name":"http","slug":"http","permalink":"https://blog.xiemx.com/tags/http/"},{"name":"tomcat","slug":"tomcat","permalink":"https://blog.xiemx.com/tags/tomcat/"},{"name":"webserver","slug":"webserver","permalink":"https://blog.xiemx.com/tags/webserver/"}],"author":"xiemx"},{"title":"PXE批量在线安装操作系统","slug":"2016-01-17-pxe-kickstart","date":"2016-01-16T23:01:10.000Z","updated":"2019-10-19T09:34:37.732Z","comments":false,"path":"/2016/01/17/2016-01-17-pxe-kickstart/","link":"","permalink":"https://blog.xiemx.com/2016/01/17/2016-01-17-pxe-kickstart/","excerpt":"","text":"pxe 通过网络方式安装部署 dhcp:动态管理协议 网卡支持tftp(文件下载使用,不支持验证，安全系数低,只下载些基础文件）使用http下载ks.conf(应答文件)/rpm包 流程123456客户端向服务端申请下载 dhcp.client 服务端返回信息客户端下载，并分配给客户端ip地址客户端拿到ip地址后去tftp服务端下载pxelinux.0 并在客户端安装pxelinux.0(引导安装程序）客户端去下载配置文件 pxelinux.cfg/default 安装后出现安装标签 指引安装（例 install foution0)选择安装标签后 到服务端下载unlinuz(微型运行平台） initrd(基本的命令 程序） ks文件路径（自动安装使用）然后出现安装界面 开始交互式安装如果自动安装去找http服务器，下载应答ks.cfg,下载rpm包 自动安装后会再此运行ks.cfg脚本 原理图：1234567891011121314151617181920212223clients：dhcp client------------------------------&gt; dhcp server&lt;------------------------------分配地址池ip，告知tftp的地址请求pxelinux.0文件------------------------------&gt; tftp server&lt;------------------------------pxelinux.0引导界面-------------------------------&gt; tftp server&lt;------------------------------pxelinux.cfg/default安装界面-------------------------------&gt; tftp server&lt;-------------------------------vmlinuxz initrd ks文件路径-------------------------------&gt; http server&lt;------------------------------ks.cfg rpm包&lt;-------------------------------自动执行脚本安装完成 操作步骤依据以上原理图可以得知PXE过程需要用到的文件有pxelinux.o、default、ks.cfg、vmlinuxz、initrd.img。需要用到的协议tftp、dhcp、http。 pxelinux.0 来源syslinux软件包ks.cfg 来源kickstart，也可以通过安装system-config-kickstart来图形化配置vmlinuz 来源于iso镜像文件default 需要手工配置initrd.img 来源iso镜像文件 1.安装软件 1yum install httpd dhcp tftp-server -y 2.配置dhcp 123456789101112131415161718[root@linux]# cat /etc/dhcpd.confddns-update-style interim;allow booting; #定义能够PXE启动allow bootp; #定义支持bootpnext-server 192.168.0.1; #TFTP Server的IP地址filename \"pxelinux.0\"; #bootstrap 文件(NBP)default-lease-time 1800;max-lease-time 7200;ping-check true;option domain-name-servers 192.168.0.1;subnet 192.168.0.0 netmask 255.255.255.0&#123;range 192.168.0.128 192.168.0.220;option routers 192.168.0.1;option broadcast-address 192.168.0.255;&#125; 3.启动tftp 12345678910111213141516[root@linux]# cat /etc/xinetd.d/tftpservice tftp&#123;socket_type = dgramprotocol = udpwait = yesuser = rootserver = /usr/sbin/in.tftpdserver_args = -s /var/lib/tftpboot tftp服务根目录disable = no 是否关闭tftp服务per_source = 11cps = 100 2flags = IPv4&#125;重启xinetd服务 4.获取pxelinux.0文件 123[root@linux]# rpm -ql syslinux | grep \"pxelinux.0\"/usr/lib/syslinux/pxelinux.0[root@linux]# cp /usr/lib/syslinux/pxelinux.0 /var/lib/tftpboot/ 5.创建/var/lib/tftp/pxelinux.cfg目录、创建default文件 123456789101112131415161718192021将 boot.msg initrd.img splash.png vesamenu.c32 vmlinuz 复制到/var/lib/tftpboot/[root@linux]# cat /tftpboot/pxelinux.cfg/defaultdefault vesamenu.c32 显示图形化引导界面，也可以写成default linux文本化界面timeout 60 等待操作时间display boot.msg 显示一些引导信息menu background splash.jpg 背景图片menu title Welcome to pxe Setup! 界面标题label 1menu label Boot from ^local drive 安装1选项标题menu default 60s无操作默认启动此选项localboot 0xfffflabel 2menu label Install linuxipappend 2kernel vmlinuzappend initrd=initrd.img ks=http://172.25.16.9/ks.cfg 6.将iso文件展开到http目录下 7.生成ks.cfg文件 system-config-kickstart安装此图形化工具，生成自动应答脚本。也可以拷贝已安装系统中自动生成的脚本。 8.关闭防火墙，selinux。测试http、tftp等服务都正常启动且文件都已放置可正常访问下载。","categories":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/tags/linux/"},{"name":"pxe","slug":"pxe","permalink":"https://blog.xiemx.com/tags/pxe/"},{"name":"kickstart","slug":"kickstart","permalink":"https://blog.xiemx.com/tags/kickstart/"}],"author":"xiemx"},{"title":"NTP服务器搭建","slug":"2015-11-29-ntp-server","date":"2015-11-29T00:11:58.000Z","updated":"2019-10-19T09:57:15.251Z","comments":false,"path":"/2015/11/29/2015-11-29-ntp-server/","link":"","permalink":"https://blog.xiemx.com/2015/11/29/2015-11-29-ntp-server/","excerpt":"","text":"NTP即网络时间协议，用来同步计算机时间，提高时间的精确度。 环境：RHEL6.5 安装包：ntp-4.2.6p5-1.el6.x86_64 安装方式： 1yum install ntp ntp主配置文件中并无太多配置，有效的配置默认如下 123456789101112[root@localhost html]# egrep -v \"^$|^#\" /etc/ntp.confdriftfile /var/lib/ntp/driftrestrict default kod nomodify notrap nopeer noquery——拒绝ipv4查询restrict -6 default kod nomodify notrap nopeer noquery——拒绝ipv6查询restrict 127.0.0.1——允许本机查询restrict -6 ::1——允许本机查询server 0.rhel.pool.ntp.org iburst——server指定我们去哪里同步server 1.rhel.pool.ntp.org iburstserver 2.rhel.pool.ntp.org iburstserver 3.rhel.pool.ntp.org iburstincludefile /etc/ntp/crypto/pwkeys /etc/ntp/keys 我们作为服务器端需要授权其他用户过来同步时间，需要在配置行中添加 123restrict 192.168.10.10 mask 255.255.255.0 nomodify ——允许192.168.10.10/24这台机器过来同步restrict 192.168.10.0 mask 255.255.255.0 nomodify ——允许192.168.10.0/24这个网段的机器过来同步server 210.72.145.44 ——我们去这台机器上同步时间 以上设置设置好后我们即可通过ntpdate去同步时间 1ntpdate 210.72.145.44 客户端也可以通过ntpdate向我们请求数据，ntp服务器搭建好后客户端可以通过计划任务固定时间来向我们请求时间。ntp配置简单，但要注意防火墙的限制。 常用ntp服务器：12345678910210.72.145.44 (国家授时中心服务器IP地址)ntp.sjtu.edu.cn 202.120.2.101 (上海交通大学网络中心NTP服务器地址）s1b.time.edu.cn 清华大学s1c.time.edu.cn 北京大学s1d.time.edu.cn 东南大学s1e.time.edu.cn 清华大学s2d.time.edu.cn 西南地区网络中心s2e.time.edu.cn 西北地区网络中心s2f.time.edu.cn 东北地区网络中心s2g.time.edu.cn 华东南地区网络中心","categories":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/tags/linux/"},{"name":"ntp","slug":"ntp","permalink":"https://blog.xiemx.com/tags/ntp/"}],"author":"xiemx"},{"title":"linux磁盘配额","slug":"2015-11-29-linux-disk-quota","date":"2015-11-28T20:11:32.000Z","updated":"2019-10-19T12:48:10.144Z","comments":false,"path":"/2015/11/29/2015-11-29-linux-disk-quota/","link":"","permalink":"https://blog.xiemx.com/2015/11/29/2015-11-29-linux-disk-quota/","excerpt":"","text":"再多用户的模式下，linux中常常需要对用户进行磁盘空间限制，例如虚拟主机需要限制用户的空间。linux大多数的发行版都采用quota来对磁盘配额来进行管理，quota是系统内核中的一个功能，要使用quota需要系统内核支持quota功能。目前使用的发行本中都是支持次功能的，如果内核不支持此功能那么就需要重新编译下内核来开启此功能 12345678### grep CONFIG_QUOTA /boot/config-`uname -r` 来检查下内核是否支持[root@localhost mnt]# grep CONFIG_QUOTA /boot/config-2.6.32-431.el6.x86_64CONFIG_QUOTA=yCONFIG_QUOTA_NETLINK_INTERFACE=y# CONFIG_QUOTA_DEBUG is not setCONFIG_QUOTA_TREE=yCONFIG_QUOTACTL=y 如有以上两条则说明支持，另外quota是针对用户去限制其可使用的block和inode，所以应该是一个基于文件系统的配置，所以我们要在文件系统中开启quota功能，针对已经挂载过的文件系统我们可以使用mount -o remount,usrquota,grpquota,default /mnt来重新挂载激活（mnt只是个举例，实际中先df 命令查看当前挂载信息，在选择要开启的文件系统），也可以写在/etc/fstab文件中umount 后再mount -a 或重启系统 12345678910111213141516171819202122[root@localhost mnt]# mount/dev/mapper/VolGroup-lv_root on / type ext4 (rw)proc on /proc type proc (rw)sysfs on /sys type sysfs (rw)devpts on /dev/pts type devpts (rw,gid=5,mode=620)tmpfs on /dev/shm type tmpfs (rw,rootcontext=\"system_u:object_r:tmpfs_t:s0\")/dev/sda1 on /boot type ext4 (rw)none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)/dev/mapper/vgxiemx-lv1 on /mnt type ext3 (rw)[root@localhost mnt]# mount -o remount,grpquota,usrquota,defaults /mnt[root@localhost mnt]# mount/dev/mapper/VolGroup-lv_root on / type ext4 (rw)proc on /proc type proc (rw)sysfs on /sys type sysfs (rw)devpts on /dev/pts type devpts (rw,gid=5,mode=620)tmpfs on /dev/shm type tmpfs (rw,rootcontext=\"system_u:object_r:tmpfs_t:s0\")/dev/sda1 on /boot type ext4 (rw)none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)/dev/mapper/vgxiemx-lv1 on /mnt type ext3 (rw,usrquota,grpquota)[root@localhost mnt]# 此时已开启/mnt对应的文件系统的quota功能，我们可针对/mnt目录下进行用户配额限制 检查文件系统的quota信息，生成quota数据库，会在文件系统的根目录下生成quota.user和quota.group二个data类型的文件可用file命令查看 123456[root@localhost mnt]# quotacheck -avugquotacheck: Your kernel probably supports journaled quota but you are not using it. Consider switching to journaled quota to avoid running quotacheck after an unclean shutdown.quotacheck: Scanning /dev/mapper/vgxiemx-lv1 [/mnt] donequotacheck: Checked 3 directories and 6 files[root@localhost mnt]# lsaquota.group aquota.user lost+found 启动quotaquotaon -av —a选项启动所有文件系统，也可指定具体某个文件系统。关闭quotaoff命令，参数相同。 123[root@localhost /]# quotaon -av/dev/mapper/vgxiemx-lv1 [/mnt]: group quotas turned on/dev/mapper/vgxiemx-lv1 [/mnt]: user quotas turned on 编辑用户的quota信息edquota -u testuser——会打开一个vi界面 123456789Disk quotas for user testuser (uid 500):Filesystem blocks soft hard inodes soft hard/dev/mapper/vgxiemx-lv1 0 10 100 0 10 100filesystem：目标文件系统block：现在已使用的block数量inode：现在已使用的inode数量soft：软限制，可使用的soft数量，0为不限制，具体数值设定后超过数值进行警告，但依旧可以写入，知道宽限时间到达后无法写入默认宽限7天hard：硬限制，可使用block/inode数量，0为不限制，具体数值设定后超过数值，立即无法写入。 例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[testuser@localhost mnt]$ dd if=/dev/zero of=/mnt/bigfile bs=1M count=20 ——生成一个20M的文件，查过soft block限制dm-2: warning, user block quota exceeded.1+0 records in0+0 records out98304 bytes (98 kB) copied, 0.00147599 s, 66.6 MB/s[testuser@localhost mnt]$ du bigfile ——统计文件大小20M20 bigfile[testuser@localhost mnt]$ dd if=/dev/zero of=/mnt/bigfile bs=1M count=110——生成一个110M的文件，超过hard block限制，最后10M数据无法写入dm-2: warning, user block quota exceeded.dm-2: write failed, user block limit reached.dd: writing `/mnt/bigfile': Disk quota exceeded1+0 records in0+0 records out98304 bytes (98 kB) copied, 0.000795784 s, 124 MB/s[testuser@localhost mnt]$ du bigfile——统计文件大小为100M100 bigfile[testuser@localhost mnt]$ lltotal 132-rw-------. 1 root root 7168 Nov 22 04:01 aquota.group-rw-------. 1 root root 7168 Nov 22 03:58 aquota.user-rw-rw-r--. 1 testuser testuser 98304 Nov 22 04:02 bigfiledrwxrwxrwx. 2 root root 16384 Nov 22 02:31 lost+found[testuser@localhost mnt]$ touch file&#123;1..10&#125;——创建10个文件，此时已有1个bigfile，总共11个文件超过soft inode限制dm-2: warning, user file quota exceeded.[testuser@localhost mnt]$ lsaquota.group bigfile file10 file3 file5 file7 file9aquota.user file1 file2 file4 file6 file8 lost+found[testuser@localhost mnt]$ touch test&#123;1..100&#125;——创建100个文件，此时已有11个文件，所以最后的11个文件应该创建不成功dm-2: write failed, user file limit reached.touch: cannot touch `test90': Disk quota exceededtouch: cannot touch `test91': Disk quota exceededtouch: cannot touch `test92': Disk quota exceededtouch: cannot touch `test93': Disk quota exceededtouch: cannot touch `test94': Disk quota exceededtouch: cannot touch `test95': Disk quota exceededtouch: cannot touch `test96': Disk quota exceededtouch: cannot touch `test97': Disk quota exceededtouch: cannot touch `test98': Disk quota exceededtouch: cannot touch `test99': Disk quota exceededtouch: cannot touch `test100': Disk quota exceeded[testuser@localhost mnt]$ lsaquota.group file7 test15 test24 test33 test42 test51 test60 test7 test79 test88aquota.user file8 test16 test25 test34 test43 test52 test61 test70 test8 test89bigfile file9 test17 test26 test35 test44 test53 test62 test71 test80 test9file1 lost+found test18 test27 test36 test45 test54 test63 test72 test81file10 test1 test19 test28 test37 test46 test55 test64 test73 test82file2 test10 test2 test29 test38 test47 test56 test65 test74 test83file3 test11 test20 test3 test39 test48 test57 test66 test75 test84file4 test12 test21 test30 test4 test49 test58 test67 test76 test85file5 test13 test22 test31 test40 test5 test59 test68 test77 test86file6 test14 test23 test32 test41 test50 test6 test69 test78 test87 4.设置宽限时间 12345edquota -t ——默认7天Grace period before enforcing soft limits for users:Time units may be: days, hours, minutes, or secondsFilesystem Block grace period Inode grace period/dev/mapper/vgxiemx-lv1 7days 7days 5.查询所有用户的磁盘配额情况，用户也可用quota查询当前用户的quota信息 12345678910repquota -a[root@localhost mnt]# repquota -a*** Report for user quotas on device /dev/mapper/vgxiemx-lv1Block grace time: 7days; Inode grace time: 7daysBlock limits File limitsUser used soft hard grace used soft hard grace-------------------------------------------------------------------------------------------root -- 956592 0 0 6 0 0testuser ++ 100 10 100 6days 100 10 100 6days 如果要设置quota，建议在/etc/rc.d/rc.local中写入quotacheck和quotaon 已保证系统启动时quota服务开启且数据都是最新的。分区的quote功能开启建议在/etc/fstab中。 例： 1234567891011121314/etc/fstab# Created by anaconda on Sun May 24 08:16:25 2015## Accessible filesystems, by reference, are maintained under '/dev/disk'# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info#/dev/mapper/VolGroup-lv_root / ext4 defaults 1 1UUID=3fe6311e-b3fe-4f7f-8558-4f011eab1dde /boot ext4 defaults 1 2/dev/mapper/VolGroup-lv_swap swap swap defaults 0 0tmpfs /dev/shm tmpfs defaults 0 0devpts /dev/pts devpts gid=5,mode=620 0 0sysfs /sys sysfs defaults 0 0proc /proc proc defaults 0 0/dev/mapper/vgxiemx-lv1 /mnt ext3 defaults,usrquota,grpquota 0 0","categories":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/tags/linux/"},{"name":"command","slug":"command","permalink":"https://blog.xiemx.com/tags/command/"}],"author":"xiemx"},{"title":"Linux动态磁盘管理（lvm）","slug":"2015-11-29-linux-lvm","date":"2015-11-28T18:11:11.000Z","updated":"2019-10-19T15:12:48.063Z","comments":false,"path":"/2015/11/29/2015-11-29-linux-lvm/","link":"","permalink":"https://blog.xiemx.com/2015/11/29/2015-11-29-linux-lvm/","excerpt":"","text":"lvm是将底层硬件存储屏蔽整合，通过软件组合在对系统开放，此时系统查看到的就不是底层一块块的物理磁盘了而是我们虚拟的逻辑磁盘，我们可以自由的对这些磁盘进行容量管理。lvm从系统层面到物理层面分别对应“文件系统——lv——vg——pv——磁盘”。所有对磁盘的管理都要遵循这个物理结构的顺序来处理。创建时由下而上，删除时由上而下，调整中间部分是也要结合上下部分分析需求具体在进行处理。 pv——物理卷vg——卷组lv——逻辑卷pe——最小存储单元，类似与磁盘block lvm是介于系统与硬件之间的一种磁盘组织方法，下层为上层的基础，下层制约上层。 lvm管理 磁盘分区 1fdisk ／dev/vda 同步内核中的分区表信息（如果是新硬盘一般不需要这一部分） 1partx -a /dev/vda 创建pv 1234567891011121314151617181920212223242526272829303132pvcreate ／dev/vda1 ——有多个分区时可以依次空格间隔写在后面，也可以针对整块硬盘去直接创建pvs和pvdisplay查看pv缩略和详细信息[root@localhost ~]# pvsPV VG Fmt Attr PSize PFree/dev/sda2 VolGroup lvm2 a-- 19.1g 0/dev/sdb lvm2 a-- 10.0g 10.0g[root@localhost ~]# pvdisplay--- Physical volume ---PV Name /dev/sda2VG Name VolGroupPV Size 19.51 GiB / not usable 3.00 MiBAllocatable yes (but full)PE Size 4.00 MiBTotal PE 4994Free PE 0Allocated PE 4994PV UUID fcQLJh-oq2G-adfr-QeEK-sdtl-XND-m1YtPA\"/dev/sdb\" is a new physical volume of \"10.00 GiB\"--- NEW Physical volume ---PV Name /dev/sdbVG NamePV Size 10.00 GiBAllocatable NOPE Size 0Total PE 0Free PE 0Allocated PE 0PV UUID WQpkNi-cN6-QogB-TLB2-R2wT-Agmh-BRBiUN 创建vg 12345678910111213141516171819202122232425262728293031vgcreate -s 8M vgname /dev/vda1 /dev/vda2 —— s选项制定pe的大小默认为4Mvgs和vgdisplay查看vg信息[root@localhost ~]# vgcreate -s 8M vgxiemx /dev/sdbVolume group \"vgxiemx\" successfully created[root@localhost ~]# vgsVG #PV #LV #SN Attr VSize VFreeVolGroup 1 2 0 wz--n- 19.51g 0vgxiemx 1 0 0 wz--n- 9.99g 9.99g[root@localhost ~]# vgdisplay--- Volume group ---VG Name vgxiemxSystem IDFormat lvm2Metadata Areas 1Metadata Sequence No 1VG Access read/writeVG Status resizableMAX LV 0Cur LV 0Open LV 0Max PV 0Cur PV 1Act PV 1VG Size 9.99 GiBPE Size 8.00 MiBTotal PE 1279Alloc PE / Size 0 / 0Free PE / Size 1279 / 9.99 GiBVG UUID 5WZfct-snfo-aEDl-gHuO-QDbz-v83j-L0lwmR 创建lv 123456789101112131415161718192021222324252627282930lvcreate -l 100 -n lvname vgname——l选项指定分配100个pe给lv ，n指定名称，最后从哪个vg创建。lvcreate -L1000M -n lvname vgname——L便是直接分配多大空间单位为K M G。L和l参数还有很多种容量表示方法man查阅lvs和lvdisplay查看lv信息[root@localhost ~]# lvcreate -n lv1 -l 1279 vgxiemxLogical volume \"lv1\" created[root@localhost ~]# lvsLV VG Attr LSize Pool Origin Data% Move Log Cpy%Sync Convertlv_root VolGroup -wi-ao---- 17.51glv_swap VolGroup -wi-ao---- 2.00glv1 vgxiemx -wi-a----- 9.99g[root@localhost ~]# lvdisplay--- Logical volume ---LV Path /dev/vgxiemx/lv1LV Name lv1VG Name vgxiemxLV UUID nViafR-nrnS-REMQ-bGOT-uhDe-Skd0-XUNwmULV Write Access read/writeLV Creation host, time localhost.localdomain, 2015-11-22 01:50:41 +0800LV Status available# open 0LV Size 9.99 GiBCurrent LE 1279Segments 1Allocation inheritRead ahead sectors auto- currently set to 256Block device 253:2 创建文件系统 12345678910111213141516171819202122232425mkfs.ext3 /dev/vgname/lvname ——创建文件系统[root@localhost ~]# mkfs -t ext3 /dev/vgxiemx/lv1mke2fs 1.41.12 (17-May-2010)Filesystem label=OS type: LinuxBlock size=4096 (log=2)Fragment size=4096 (log=2)Stride=0 blocks, Stripe width=0 blocks655360 inodes, 2619392 blocks130969 blocks (5.00%) reserved for the super userFirst data block=0Maximum filesystem blocks=268435456080 block groups32768 blocks per group, 32768 fragments per group8192 inodes per groupSuperblock backups stored on blocks:32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632Writing inode tables: doneCreating journal (32768 blocks): doneWriting superblocks and filesystem accounting information: doneThis filesystem will be automatically checked every 30 mounts or180 days, whichever comes first. Use tune2fs -c or -i to override. 挂载lv 123456789mount /dev/vgname/lvname /mnt[root@localhost ~]# mount /dev/vgxiemx/lv1 /mnt[root@localhost mnt]# df -lFilesystem 1K-blocks Used Available Use% Mounted on/dev/mapper/VolGroup-lv_root 18069936 6767876 10384148 40% /tmpfs 953788 72 953716 1% /dev/shm/dev/sda1 495844 39915 430329 9% /boot/dev/mapper/vgxiemx-lv1 10313016 154232 9634908 2% /mnt lvm调整pv的新增、删除pv的新增就是增加新的空闲分区或者新的硬盘创建成新的空闲pv同上pv创建命令一致 pv删除1234567pvremove /dev/vdb[root@localhost /]# pvremove /dev/sdbLabels on physical volume \"/dev/sdb\" successfully wiped[root@localhost /]# pvsPV VG Fmt Attr PSize PFree/dev/sda2 VolGroup lvm2 a-- 19.51g 0 移动pv中的数据由下图我们可以看到pv/dev/sdb1中的1G数据已经被我们移动到/dev/sdb2中去了，如果是单硬盘做的pv此时我们就可以撤除/dev/sdb1对应的这块硬盘，而数据没有丢失 1234567891011121314151617181920212223pvremove /dev/sdb1 /dev/sdb2[root@localhost mnt]# pvsPV VG Fmt Attr PSize PFree/dev/sda2 VolGroup lvm2 a-- 19.51g 0/dev/sdb1 vgxiemx lvm2 a-- 1.01g 8.00m/dev/sdb2 vgxiemx lvm2 a-- 1.01g 1.01g/dev/sdb3 vgxiemx lvm2 a-- 1.01g 1.01g/dev/sdb5 lvm2 a-- 2.01g 2.01g/dev/sdb6 lvm2 a-- 4.96g 4.96g[root@localhost mnt]# pvmove /dev/sdb1 /dev/sdb2/dev/sdb1: Moved: 1.6%/dev/sdb1: Moved: 45.3%/dev/sdb1: Moved: 89.8%/dev/sdb1: Moved: 100.0%[root@localhost mnt]# pvsPV VG Fmt Attr PSize PFree/dev/sda2 VolGroup lvm2 a-- 19.51g 0/dev/sdb1 vgxiemx lvm2 a-- 1.01g 1.01g/dev/sdb2 vgxiemx lvm2 a-- 1.01g 8.00m/dev/sdb3 vgxiemx lvm2 a-- 1.01g 1.01g/dev/sdb5 lvm2 a-- 2.01g 2.01g/dev/sdb6 lvm2 a-- 4.96g 4.96g vg扩容、缩小、删除vg的扩容vg的容量来自于pv，如果vg的容量不够那么vg扩容其实就是新增空闲的pv到vg中去 1234567891011121314151617181920vgextend /dev/sdb5 vgxiemx[root@localhost /]# pvsPV VG Fmt Attr PSize PFree/dev/sda2 VolGroup lvm2 a-- 19.51g 0/dev/sdb1 vgxiemx lvm2 a-- 1.01g 1.01g/dev/sdb2 vgxiemx lvm2 a-- 1.01g 1.01g/dev/sdb3 vgxiemx lvm2 a-- 1.01g 1.01g/dev/sdb5 lvm2 a-- 2.01g 2.01g/dev/sdb6 lvm2 a-- 4.96g 4.96g[root@localhost /]# vgextend vgxiemx /dev/sdb5Volume group \"vgxiemx\" successfully extended[root@localhost /]# pvsPV VG Fmt Attr PSize PFree/dev/sda2 VolGroup lvm2 a-- 19.51g 0/dev/sdb1 vgxiemx lvm2 a-- 1.01g 1.01g/dev/sdb2 vgxiemx lvm2 a-- 1.01g 1.01g/dev/sdb3 vgxiemx lvm2 a-- 1.01g 1.01g/dev/sdb5 vgxiemx lvm2 a-- 2.00g 2.00g/dev/sdb6 lvm2 a-- 4.96g 4.96g vg的减小也就是将vg中的pv移除，但pv移除是存在pv中的pe都已经被使用存在数据，这是我们就需要移动要删除的数据到新的pv中去，新的pv空间必须要比旧的pv空间大。 1234567891011121314151617181920vgreduce vgxiemx /dev/sdb5[root@localhost /]# pvsPV VG Fmt Attr PSize PFree/dev/sda2 VolGroup lvm2 a-- 19.51g 0/dev/sdb1 vgxiemx lvm2 a-- 1.01g 1.01g/dev/sdb2 vgxiemx lvm2 a-- 1.01g 1.01g/dev/sdb3 vgxiemx lvm2 a-- 1.01g 1.01g/dev/sdb5 vgxiemx lvm2 a-- 2.00g 2.00g/dev/sdb6 lvm2 a-- 4.96g 4.96g[root@localhost /]# vgreduce vgxiemx /dev/sdb5Removed \"/dev/sdb5\" from volume group \"vgxiemx\"[root@localhost /]# pvsPV VG Fmt Attr PSize PFree/dev/sda2 VolGroup lvm2 a-- 19.51g 0/dev/sdb1 vgxiemx lvm2 a-- 1.01g 1.01g/dev/sdb2 vgxiemx lvm2 a-- 1.01g 1.01g/dev/sdb3 vgxiemx lvm2 a-- 1.01g 1.01g/dev/sdb5 lvm2 a-- 2.01g 2.01g/dev/sdb6 lvm2 a-- 4.96g 4.96g vg的删除1234567vgremove vgxiemx[root@localhost /]# vgremove vgxiemxVolume group \"vgxiemx\" successfully removed[root@localhost /]# vgsVG #PV #LV #SN Attr VSize VFreeVolGroup 1 2 0 wz--n- 19.51g 0 lv的扩容、缩小、删除lv的缩小12345678910111213lvreduce -L 9000M /dev/vgxiemx/lv1[root@localhost /]# lvreduce -L 9000M /dev/vgxiemx/lv1WARNING: Reducing active logical volume to 8.79 GiBTHIS MAY DESTROY YOUR DATA (filesystem etc.)Do you really want to reduce lv1? [y/n]: yReducing logical volume lv1 to 8.79 GiBLogical volume lv1 successfully resized[root@localhost /]# lvsLV VG Attr LSize Pool Origin Data% Move Log Cpy%Sync Convertlv_root VolGroup -wi-ao---- 17.51glv_swap VolGroup -wi-ao---- 2.00glv1 vgxiemx -wi-a----- 8.79g lv的扩容12345678910lvextend -L 10000M /dev/vgxiemx/lv1[root@localhost /]# lvextend -L 10000M /dev/vgxiemx/lv1Extending logical volume lv1 to 9.77 GiBLogical volume lv1 successfully resized[root@localhost /]# lvsLV VG Attr LSize Pool Origin Data% Move Log Cpy%Sync Convertlv_root VolGroup -wi-ao---- 17.51glv_swap VolGroup -wi-ao---- 2.00glv1 vgxiemx -wi-a----- 9.77g lv的删除123456789lvremove /dev/vgxiemx/lv1[root@localhost /]# lvremove /dev/vgxiemx/lv1Do you really want to remove active logical volume lv1? [y/n]: yLogical volume \"lv1\" successfully removed[root@localhost /]# lvsLV VG Attr LSize Pool Origin Data% Move Log Cpy%Sync Convertlv_root VolGroup -wi-ao---- 17.51glv_swap VolGroup -wi-ao---- 2.00g 文件系统的扩缩检测文件系统12345678910e2fsck /dev/vgxiemx/lv1[root@localhost /]# e2fsck -f /dev/vgxiemx/lv1e2fsck 1.41.12 (17-May-2010)Pass 1: Checking inodes, blocks, and sizesPass 2: Checking directory structurePass 3: Checking directory connectivityPass 4: Checking reference countsPass 5: Checking group summary information/dev/vgxiemx/lv1: 11/655360 files (0.0% non-contiguous), 79696/2619392 blocks 调整文件系统大小123456resize2fs /dev/vgxiemx/lv1 9000M[root@localhost /]# resize2fs /dev/vgxiemx/lv1 9000Mresize2fs 1.41.12 (17-May-2010)Resizing the filesystem on /dev/vgxiemx/lv1 to 2304000 (4k) blocks.The filesystem on /dev/vgxiemx/lv1 is now 2304000 blocks long.","categories":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/tags/linux/"},{"name":"lvm","slug":"lvm","permalink":"https://blog.xiemx.com/tags/lvm/"}],"author":"xiemx"},{"title":"Linux计划任务","slug":"2015-11-28-linux-crontab-and-at","date":"2015-11-28T03:11:05.000Z","updated":"2019-10-19T12:18:15.033Z","comments":false,"path":"/2015/11/28/2015-11-28-linux-crontab-and-at/","link":"","permalink":"https://blog.xiemx.com/2015/11/28/2015-11-28-linux-crontab-and-at/","excerpt":"","text":"计划任务种类： 一次性计划任务（at）：由进程atd守护 周期性计划任务（cron）：由进程crond守护 一次性计划任务（at）生成的文件存放在／var/spool/at/目录下，任务执行一次之后自动删除。一次性计划任务还可以用batch命令去执行。batch命令会在系统空闲的情况下执行任务，用法和at相同但优先级低延时执行。 命令： 1234567［root@localhost ~]# at now + 5 minutes at&gt; /bin/mail root -s \"testing at job\" &lt; /root/.bashrc at&gt; &lt;EOF&gt; &lt;==输入 [ctrl] + d 结束编辑！此时会在／var/spool/at/目录下生成一个任务文件[root@localhost ~]# atq5 2015-11-25 23:00 a root[root@localhost ~]# atrm 5 一次性计划任务的时间写法支持很多可以查看man page！当我们制定计划任务之后，由于种种原因而要去取消计划任务我们可以使用atq去查询当前系统中有多少为执行的计划，每个计划的编号是多少。在通过atrm＋任务编号来删除这个任务。／etc/at.allow和／etc/at.deny文件规范了哪些用户可以使用at那些用户无法使用at。写在allow文件表明了写在此文件中的用户才可以使用at，deny文件中的用户无法使用。 周期性计划任务（cron）由进程crond守护，生成的任务文件存放在／var/spool/cron/目录下文件会以用户名命名。也采用了／etc/cron.allow和／etc/cron.deny的授权方式。配置文件为／etc/crontab 。周期性计划任务分为两种，一种是用户级别（通过crontab －e来制定），一种为系统级别（写在／etc/crontab文件中），建议使用crontab来制定。 命令用法 1234567crontab 参数-u ：只有 root 才能进行这个任务，亦即帮其他使用者创建/移除 crontab 工作排程；-e ：编辑 crontab 的工作内容，同vi编辑文件相同（其实就是vi了一个新文件到／var/spool/cron下）-l ：查阅 crontab 的工作内容-r ：移除所有的 crontab 的工作内容，若仅要移除一项，请用 -e 去编辑。［root@localhost ～］#crontab -u testuser -e #为testuser用户制定计划任务，会打开vi的编辑界面，每一行就是一个任务 例： 12345###时间的写法基本就是\",\" \"-\" \"/\"三种符号来间隔,“＊”代表所有都匹配。30 * * * * &lt;==每小时的30分，输出hello*/10 ＊ * * * &lt;==每隔10分钟30 12 * * 1,3,5 &lt;==每周1 3 5的12时30分30 12 1 3-5 * &lt;==每年的3 4 5月1日12时30分 同样的／etc/crontab 中的时间写法也是类似但语法不同cron每分钟都会去读一次计划任务同时也会读一次／etc/crontab文件，crontab文件中设置了4个文件夹（配置文件中的run-parts命令部分），系统会在不同的时间去读取运行其中的文件，我们可以将脚本文件放到对应的文件夹下也可以实现脚本的周期性执行。我们的locate数据库同步，logrotate等都是放在这些目录下来实现周期性的工作。 12345678910111213[root@localhost ~]# cat /etc/crontabSHELL=/bin/bashPATH=/sbin:/bin:/usr/sbin:/usr/binMAILTO=rootHOME=/# run-parts01 * * * * root run-parts /etc/cron.hourly &lt;==每小时执行一次02 4 * * * root run-parts /etc/cron.daily &lt;==每天执行一次22 4 * * 0 root run-parts /etc/cron.weekly &lt;==每周日执行一次42 4 1 * * root run-parts /etc/cron.monthly &lt;==每个月 1 号执行一次###run-parts 我们可以通过which命令来查看其实是/usr/bin/run-parts命令，我们可以man一下这个命令，或者直接打开这个命令的脚本，会发现这个命令会将目录下的脚本全部执行一次。","categories":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/tags/linux/"},{"name":"command","slug":"command","permalink":"https://blog.xiemx.com/tags/command/"}],"author":"xiemx"},{"title":"Linux日志系统","slug":"2015-11-28-linux-log-system","date":"2015-11-28T02:11:40.000Z","updated":"2019-10-19T14:29:34.494Z","comments":false,"path":"/2015/11/28/2015-11-28-linux-log-system/","link":"","permalink":"https://blog.xiemx.com/2015/11/28/2015-11-28-linux-log-system/","excerpt":"","text":"在Linux系统中，有三个主要的日志子系统： 连接日志–由多个程序执行，把纪录写入到/var/log/wtmp和/var/run/utmp，login等程序更新wtmp和utmp文件，使系统管理员能够跟踪谁在何时登录到系统。 进程统计–由系统内核执行。当一个进程终止时，为每个进程往进程统计文件（pacct或acct）中写一个纪录。进程统计的目的是为系统中的基本服务提供命令使用统计。 错误日志–由syslogd（8）执行。各种系统守护进程、用户程序和内核通过syslog（3）向文件/var/log/messages报告值得注意的事件。另外有许多UNIX程序创建日志。像HTTP和FTP这样提供网络服务的服务器也保持详细的日志。 大部分Linux系统都使用syslog管理日志，系统日志默认会写在/var/log目录下，syslog会依据/etc/syslog.conf文件中的配置将不同级别、不同类别的日志分门别类的纪录到不同的日志文件中去。/etc/syslog.conf中依照如下格式纪录日志配置： 12日志对象.级别 日志文件存放位置authpriv.info /var/log/secure 1234567891011121314151617［root@localhost ～］#cat /etc/syslog.conf//将info或更高级别的消息送到/var/log/messages，除了mail以外。//其中*是通配符，代表任何设备；none表示不对任何级别的信息进行记录。*.info;mail.none;authpriv.none /var/log/messages//将authpirv设备的任何级别的信息记录到/var/log/secure文件中，这主要是一些和认、权限使用相关的信息。authpriv.* /var/log/secure//将mail设备中的任何级别的信息记录到/var/log/maillog文件中，这主要是和电子邮件相关的信息。mail.* /var/log/maillog//将cron设备中的任何级别的信息记录到/var/log/cron文件中，这主要是和系统中定期执行的任务相关的信息。cron.* /var/log/cron//将任何设备的emerg级别的信息发送给所有正在系统上的用户。*.emerg *//将uucp和news设备的crit级别的信息记录到/var/log/spooler文件中。uucp,news.crit /var/log/spooler//将和系统启动相关的信息记录到/var/log/boot.log文件中。local7.* /var/log/boot.log 日志对象kern——内核User——用户程序Damon——系统守护进程Mail——电子邮件系统Auth——与安全权限相关的命令Lpr——打印机News——新闻组信息Uucp——Uucp程序Cron——记录当前登录的每个用户信息wtmp——一个用户每次登录进入和退出时间的永久记录Authpriv——授权信息 日志的级别emerg——最高的紧急程度状态alert——紧急状态Cirt——重要信息warning——警告err——错误notice——通知info——一般性消息Debug——调试级信息None——不记录任何日志信息常用的日志文件 常见的日志文件cron: crontab例行事务的日志dmesg: 内核启动时的检测信息,输出同 dmesg 命令lastlog: 所有帐号最后一次登录的相关信息，输出同 lastlog 命令maillog: 邮件来往信息messages ： 系统错误信息secure ： 涉及到“输入口令”的操作，都会记录于此wtmp与faillog: 登录成功的用户信息(wtmp)和登录失败的用户信息(faillog)httpd, samba等 ： 不同的网络服务用自己的定制的日志文件utmp、wtmp和lastlog日志文件是多数重用UNIX日志子系统的关键–保持用户登录进入和退出的纪录。有关当前登录用户的信息记录在文件utmp中；登录进入和退出纪录在文件wtmp中；最后一次登录文件可以用lastlog命令察看。数据交换、关机和重起也记录在wtmp文件中。通常，每次有一个用户登录时，login程序在文件lastlog中察看用户的UID。如果找到了，则把用户上次登录、退出时间和主机名写到标准输出中，然后login程序在lastlog中纪录新的登录时间。在新的lastlog纪录写入后，utmp文件打开并插入用户的utmp纪录。该纪录一直用到用户登录退出时删除。utmp文件被各种命令文件使用，包括who、w、users和finger。 下一步，login程序打开文件wtmp附加用户的utmp纪录。当用户登录退出时，具有更新时间戳的同一utmp纪录附加到文件中。wtmp文件被程序last和ac使用。 日志轮替（logrotate）在系统用户众多的，或系统运行有访问量非常大的程序时，日志文件的增长会非常迅速，严重的会导致系统磁盘被写满数据无法被写入系统进程报错中止等。为了应对这个问题，Linux采取了日志轮替的方式来管理日志文件。logrotate程序会依据/etc/logrotate.conf和/etc/logrotate.d/中定义的轮替规则来裁剪、删除、备份日志文件。logrotate会在/etc/cron.daily目录下生成脚本，每天自动执行。 12345678910111213[root@localhost ~]# cat /etc/logrotate.confweekly #默认每周进行一次日志清理rotate 4 #保留的日志文件数量create #创建一个新的来存储#compress #是否需要压缩include /etc/logrotate.d #读取/etc/logrotate.d目录下的文件/var/log/wtmp &#123; #针对单个wtmp日志操作monthly #每个月一次minsize 1M #超过1M整理create 0664 root utmp #新加文件权限和用户组rotate 1 #保留一个文件&#125;","categories":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/tags/linux/"}],"author":"xiemx"},{"title":"Linux启动过程","slug":"2015-11-15-linux-boot-workflow","date":"2015-11-14T20:11:22.000Z","updated":"2019-10-19T12:25:24.121Z","comments":false,"path":"/2015/11/15/2015-11-15-linux-boot-workflow/","link":"","permalink":"https://blog.xiemx.com/2015/11/15/2015-11-15-linux-boot-workflow/","excerpt":"","text":"加载自检（post）、BIOS 当你打开计算机电源，计算机会对硬件设备加电，然后去检查cpu，内存，主板这些硬件设备。加点通过后加载BIOS信息，读取bios中存储的设备启动顺序信息，寻找启动设备一般都是硬盘、光盘、U盘等。 读取MBR 硬盘上第0磁道第一个扇区被称为MBR，也就是Master Boot Record，即主引导记录，大小是512字节，存放了446字节的boots loader、64字节的分区表信息和2字节的硬盘有效标记。 系统找到BIOS所指定的硬盘的MBR后，就会将其复制到0×7c00地址所在的物理内存中。其实被复制到物理内存的内容就是Boot Loader，常用的boot loader 有grub、lilo。 Boot Loader 系统读取内存中的grub配置信息（一般为menu.lst或grub.lst），并依照此配置信息来启动不同的操作系统。 加载内核 依据grub.conf配置文件中的配置文件，系统读取内核，挂载虚拟文件系统initrd。 启动init 内核被加载后运行第一个进程/sbin/init，读取配置文件/etc/init/rc.Scanf,根据此文件配置在找到/etc/rc.d/rc.sysinit开启系统初始化，在读取/etc/inittab文件，确定系统的启动级别。 运行inittab中规定级别的脚本程序 根据运行级别的不同，系统会运行rc0.d到rc6.d中的相应的脚本程序，/etc/rc.d/rcX.d/目录。 执行/etc/rc.d/rc.local /etc/rc.d/rcX.d/中的脚本按照配置启动完成后，会启动rc.local。 执行/bin/login 调用登录程序，启动登陆界面。 以上只是简单的系统启动过程，具体详细的系统层面的启动从第boot loader之后的执行都可以参看系统中的配置文件。可以读取相对应的脚本和配置代码，根据文件中的规定一步步的理解系统的启动，理解系统启动的详细步骤。","categories":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/tags/linux/"}],"author":"xiemx"},{"title":"Linux进程管理","slug":"2015-11-15-linux-process-manager","date":"2015-11-14T20:11:14.000Z","updated":"2019-10-19T12:10:20.320Z","comments":false,"path":"/2015/11/15/2015-11-15-linux-process-manager/","link":"","permalink":"https://blog.xiemx.com/2015/11/15/2015-11-15-linux-process-manager/","excerpt":"","text":"当我们运行程序时，Linux会为程序创建一个特殊的环境，该环境包含程序运行需要的所有资源，以保证程序能够独立运行，不受其他程序的干扰。 在系统中我们调用一个指令，Linux 就会创建一个新的进程。例如使用 ls 命令遍历目录中的文件时，就创建了一个进程。进程就是程序的实例。 系统通过一个五位数字跟踪程序的运行状态，这个数字称为 pid 或进程ID。每个进程都拥有唯一的 pid。两个 pid 一样的进程不能同时存在，Linux用 pid 来跟踪程序的运行状态。 进程查看ps1234567891011121314151617181920212223242526272829303132333435[root@localhost ~]# ps auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.0 19356 1540 ? Ss 02:09 0:01 /sbin/initroot 2 0.0 0.0 0 0 ? S 02:09 0:00 [kthreadd]root 3 0.0 0.0 0 0 ? S 02:09 0:00 [migration/0][root@localhost ~]# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 02:09 ? 00:00:01 /sbin/initroot 2 0 0 02:09 ? 00:00:00 [kthreadd]root 3 2 0 02:09 ? 00:00:00 [migration/0][root@localhost ~]# ps -leF S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD4 S 0 1 0 0 80 0 - 4839 poll_s ? 00:00:01 init1 S 0 2 0 0 80 0 - 0 kthrea ? 00:00:00 kthreadd1 S 0 3 2 0 -40 - - 0 migrat ? 00:00:00 migration/0USER 用户名 UID 用户ID（User ID） PID 进程ID（Process ID） PPID 父进程的进程ID（Parent Process id） SID 会话ID（Session id） %CPU 进程的cpu占用率 %MEM 进程的内存占用率 VSZ 进程所使用的虚存的大小（Virtual Size） RSS 进程使用的实际内存的大小 TTY 与进程关联的终端（tty） STAT 进程的状态：进程状态使用字符表示的（STAT的状态码） START 进程启动时间和日期 TIME 进程使用的总cpu时间 COMMAND 正在执行的命令行命令 NI 优先级(Nice) PRI 进程优先级编号(Priority)进程状态（S字段）： D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程 top123456789101112131415[root@localhost ~]# toptop - 02:45:58 up 36 min, 4 users, load average: 0.00, 0.00, 0.00Tasks: 175 total, 1 running, 174 sleeping, 0 stopped, 0 zombieCpu(s): 0.3%us, 0.3%sy, 0.0%ni, 99.3%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stMem: 1907580k total, 528968k used, 1378612k free, 26060k buffersSwap: 2097144k total, 0k used, 2097144k free, 173020k cachedPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND2603 root 20 0 15028 1372 1000 R 0.3 0.1 0:00.21 top1 root 20 0 19356 1540 1228 S 0.0 0.1 0:01.04 init2 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kthreadd第一行显示系统运行时间，用户数，系统负载1分钟 ，5分钟，15分钟的情况第二行显示经常总数，不同stat下的进程数量 第三行显示cpu使用情况 第四行显示物理内存使用情况 第五行显示swap交换分区使用情况 信号 1234567891011121314[root@localhost ~]# kill -l1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR111) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+338) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+843) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+1348) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-1253) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-758) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-263) SIGRTMAX-1 64) SIGRTMAX Linux中信号有64种，通过kill命令我们可以对进程发送这些信号来告知这些进程，做什么操作。常用的以下四种 1) SIGHUP这个信号用于通知它重新读取配置文件但不关闭进程，相当于reload。2) SIGINT程序终止(interrupt)信号, 在用户键入INTR字符(通常是Ctrl-C)时发出，用于通知前台进程组终止进程。9) SIGKILL用来立即结束程序的运行. 本信号不能被阻塞、处理和忽略。如果管理员发现某个进程终止不了，可尝试发送这个信号15) SIGTERM程序结束(terminate)信号, 与SIGKILL不同的是该信号可以被阻塞和处理。通常用来要求程序自己正常退出，shell命令kill缺省产生这个信号。如果进程终止不了，我们才会尝试SIGKILL。 例：kill -1 2205 ——告知PID为2205的进程重载下配置文件 kill -9 2205 ——直接杀死2205进程 kill -15 2205 ——通知2205进程关闭，进程会在收到信号后结束工作后自行关闭，有些进程会屏蔽15信号如bash，此时我们只能通过9信号强制结束。 进程优先级假设系统中有多个进程同时在排队是，cpu应该有限处理哪个进程哪？这就取决于进程的优先级，优先级高的优先处理。进程在创建是我们可以通过nice命令指定他的优先级，对于已经开始的进程我们可以通过renice来修改他的优先级。 优先级范围：-20~19 数字越小优先级越高，默认创建的进程优先级为0。 例： 12345678910111213141516nice -n 10 vim —— 创建一个优先级为10的vim进程[root@localhost ~]# ps -el | grep vimF S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD0 T 0 6891 2563 0 90 10 - 35930 signal pts/2 00:00:00 vimrenice -n -10 6891 ——调整6891进程的有限级为-10[root@localhost ~]# renice -n -10 68916891: old priority 10, new priority -10root用户可以自由调整经常的优先级，普通用户只能调低不能调高。 [testuser@localhost root]$ renice -n 5 69256925: old priority 0, new priority 5 [testuser@localhost root]$ renice -n 2 6925renice: 6925: setpriority: Permission denied 前台进程/后台进程由于一个终端在处理工作时只能处理一个，当我们想在处理第二个进程时会由于终端被占用而无法处理。这时我们可以将前台的这个进程放到后台去运行，把前台的终端空出让我们运行新的进程。 命令： command &amp;让进程在后台运行 jobs –l 查看后台运行的进程 fg %n 让后台运行的进程n到前台来 bg %n 让后台的进程n运行 以上的%n是通过jobs -l 查看到的工作编号不是pid。 例： 123456789101112131415161718192021vim &amp; ——在后台运行vim进程jobs -l ——查看后台所有进程fg %1 ——将后台1号进程移动到前台来运行bg %2 —— 让后台的2号进程开始运行ctrl+z ——暂停当前的进程放到后台，但不运行[root@localhost ~]# vim &amp;[2] 6987 [root@localhost ~]# vi /etc/resolv.conf[3]+ Stopped vi /etc/resolv.conf[root@localhost ~]# jobs -l[1] 6891 Stopped nice -n 10 vim[2]- 6987 Stopped (tty output) vim[3]+ 6989 Stopped vi /etc/resolv.conf[root@localhost ~]# bg %2[2]- vim &amp;[root@localhost ~]# fg %3","categories":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/categories/linux/"}],"tags":[{"name":"tool","slug":"tool","permalink":"https://blog.xiemx.com/tags/tool/"},{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/tags/linux/"}],"author":"xiemx"},{"title":"Linux包管理yum仓库配置","slug":"2015-11-09-linux-yum-repo","date":"2015-11-08T23:11:22.000Z","updated":"2019-10-19T15:00:03.412Z","comments":false,"path":"/2015/11/09/2015-11-09-linux-yum-repo/","link":"","permalink":"https://blog.xiemx.com/2015/11/09/2015-11-09-linux-yum-repo/","excerpt":"","text":"Rhel中使用传统的rpm包安装会存在依存关系问题，虽然rpm为我们列出了依存关系，但是并没有为我们自动解决这个问题，依旧需要我们手动的去安装依存包。这样使得软件的安装非常麻烦，由此Linux中出现了yum工具，yum通过预读取软件的依存关系然后生存缓存，以后我们通过yum去安装软件是yum会根据缓存的关系表自动去识别rpm包的依存关系，为我们自动处理依存关系，使得软件的安装的工作变得简单。yum安装软件需要一个软件仓，yum会读取/etc/yum.repos.d/xxxxxxx.repo的仓库配置文件去仓库中提取软件来安装。我们可以通过修改仓配置文件，来修改仓库位置，使用本地或者速度快的仓，加速软件安装。 配置文件内容： 123456[base]name=CentOS-$releasever - Base - mirrors.aliyun.combaseurl=http://mirrors.aliyun.com/centos/$releasever/os/$basearch/enabled=1gpgcheck=1gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-6 以上是阿里云的开源镜像站的yum源配置。 配置文件中：[ ]内的是仓库的名字name是仓库的描述也可以说是名字baseurl 仓库的位置enabled 是否启用这个仓库，1为起用，0为禁用gpgcheck 是否检查GPG签名（用来验证要安装的包是不是REDHAT官方的）gpgkey gpgkey的存放地址我们开启gpgcheck=1才使用这项功能 由以上的配置文件我们可以看出只要修改baseurl我们就可以修改仓的位置，因此我们也可以使用本地的仓库。只要配置baseurl指向相对应的仓即可。如下 12345[xiemx] name=xiemx baseurl=file:///yumdatebase enabled=1 gpgcheck=0 在运行下yum clear all清空下缓存 yum makecache制作新的缓存。以上的baseurl我们是将系统安装盘的内容复制到本机的/yumdatebase下，由于是光盘的源系统由此也不需要校验程序的安全性所以关闭gpgcheck。也可以使用其它协议来链接仓库如ftp://xxxxxxxxx。","categories":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/tags/linux/"},{"name":"yum","slug":"yum","permalink":"https://blog.xiemx.com/tags/yum/"}],"author":"xiemx"},{"title":"Linux目录结构","slug":"2015-11-07-linux-tree","date":"2015-11-07T04:11:03.000Z","updated":"2019-10-19T13:18:21.688Z","comments":false,"path":"/2015/11/07/2015-11-07-linux-tree/","link":"","permalink":"https://blog.xiemx.com/2015/11/07/2015-11-07-linux-tree/","excerpt":"","text":"1234567891011121314151617181920212223242526[root@localhost ~]# ls /bin dev home lost+found mnt proc sbin srv tmp varboot etc lib media opt root selinux sys usr/bin bin是Binary的缩写。这个目录存放着最经常使用的命令。/boot 这里存放的是启动Linux时使用的一些核心文件，包括一些连接文件以及镜像文件。/dev dev是Device(设备)的缩写。该目录下存放的是Linux的外部设备，在Linux中访问设备的方式和访问文件的方式是相同的。/etc 这个目录用来存放所有的系统管理所需要的配置文件和子目录。/home 用户的主目录，在Linux中，每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的。/lib 这个目录里存放着系统最基本的动态连接共享库，其作用类似于Windows里的DLL文件。几乎所有的应用程序都需要用到这些共享库。/lost+found ext文件系统创建，这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。/media Linux系统会自动识别一些设备，例如U盘、光驱等等，当识别后，Linux会把识别的设备挂载到这个目录下。/mnt 系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将光驱挂载在/mnt/上，然后进入该目录就可以查看光驱里的内容了。/opt 这是给主机额外安装软件所摆放的目录。比如你安装一个ORACLE数据库则就可以放到这个目录下。默认是空的。/proc 这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息。/root 该目录为系统管理员，也称作超级权限者的用户主目录。/sbin 系统管理员使用的系统管理程序。/selinux 这个目录是Redhat/CentOS所特有的目录，Selinux是一个安全机制，类似于windows的防火墙，但是这套机制比较复杂，这个目录就是存放selinux相关的文件的。/srv 该目录存放一些服务启动之后需要提取的数据。/sys 这是linux2.6内核的一个很大的变化。该目录下安装了2.6内核中新出现的一个文件系统 sysfs ，sysfs文件系统集成了下面3种文件系统的信息：针对进程信息的proc文件系统、针对设备的devfs文件系统以及针对伪终端的devpts文件系统。该文件系统是内核设备树的一个直观反映。当一个内核对象被创建的时候，对应的文件和目录也在内核对象子系统种被创建。/tmp 这个目录是用来存放一些临时文件的。/usr 这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似与windows下的program files目录。/usr/bin 系统用户使用的应用程序。/usr/sbin 超级用户使用的比较高级的管理程序和系统守护程序。/usr/src 内核源代码默认的放置目录。/var 经常被修改的目录放在这个目录下。包括各种日志文件。","categories":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/tags/linux/"}],"author":"xiemx"},{"title":"Linux标准输入输出重定向","slug":"2015-11-07-linux-stdin-stdout-stderr","date":"2015-11-06T21:11:17.000Z","updated":"2019-10-19T13:30:28.680Z","comments":false,"path":"/2015/11/07/2015-11-07-linux-stdin-stdout-stderr/","link":"","permalink":"https://blog.xiemx.com/2015/11/07/2015-11-07-linux-stdin-stdout-stderr/","excerpt":"","text":"一个命令执行前，先会准备好所有输入输出，默认分别绑定（stdin,stdout,stderr)，如果这个时候出现错误，命令将终止，不会执行。执行成功会将结果输出到屏幕上，执行错误时也会将错误信息输出到屏幕。这些默认的输出，输入都是linux系统内定的，我们在使用过程中，有时候并不希望执行结果输出到屏幕。我想输出到文件或其它设备。这个时候我们就需要进行输出重定向了。这些标准输入输出对应的文件如下： 1234[root@localhost ~]# ll /dev/stdin /dev/stdout /dev/stderrlrwxrwxrwx. 1 root root 15 Nov 8 01:03 /dev/stderr -&gt; /proc/self/fd/2 lrwxrwxrwx. 1 root root 15 Nov 8 01:03 /dev/stdin -&gt; /proc/self/fd/0 lrwxrwxrwx. 1 root root 15 Nov 8 01:03 /dev/stdout -&gt; /proc/self/fd/1 由这些管道文件可以看到linux shell下常用输入输出操作符是： 标准输入 (stdin) ：代码为 0 ，使用 &lt; 标准输出 (stdout)：代码为 1 ，使用 &gt; 或 &gt;&gt; 标准错误输出(stderr)：代码为 2 ，使用 2&gt; 或 2&gt;&gt; 当我们执行命令时我们不想要程序将结果输出到屏幕时我们就可以调用&gt; &gt;&gt;重定向操作符来重定向数据流的输出位置 123456789101112ls -l &gt; testfile1 ##查看当前目录下的文件属性，但不输出到屏幕，输出到testfile1的文件中。echo 123qwe321|passwd --stdin testuse###输出字符串123qwe321，并将字符串通过标准输入设备传递给passwd程序去修改testuser用户的密码。###--stdin代表标准输入设备，也可以用/dev/stdin来代替。(stdin前是双横杠)重定向时要注意## &gt;重定向输出到文件中时会清空文件中的内容## &gt;&gt;不会清空文件内容会在文件最后追加输出内容###由于&gt;会清空文件在写入输出内容，因此常用来清空文件echo \"\" &gt; /tmp/testfile1 ##清空testfile1文件。","categories":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/tags/linux/"}],"author":"xiemx"},{"title":"Linux特殊权限和acl访问控制列表","slug":"2015-11-05-linux-premissions-and-acl","date":"2015-11-05T01:11:28.000Z","updated":"2019-10-19T13:13:46.304Z","comments":false,"path":"/2015/11/05/2015-11-05-linux-premissions-and-acl/","link":"","permalink":"https://blog.xiemx.com/2015/11/05/2015-11-05-linux-premissions-and-acl/","excerpt":"","text":"特殊权限linux中除了常见的读（r）、写（w）、执行（x）权限以外，还有3个特殊的权限，分别是setuid、setgid和stick bit。 setuid只能设置文件，让普通用户拥有可以执行“只有root权限才能执行”的特殊权限，一般设置命令文件此属性。设置方法： 12chmod u+s /tmp/testfilechmod u-s /tmp/testfile 如： 1-rwsr-xr-x 1 root root 22984 2015-11-04 /usr/bin/passwd setgid可设置文件和目录，当目录设置此属性时在目录下创建的文件，默认所属组都会变为该目录的所属组。 设置方法: 12chmod g+s /tmp/test/chmod g-s /tmp/test/ 如: 12drwxr-sr-x 1 testuser testadmin 22984 2015-11-04 /tmp/test/-rw-r--r-- 1 root testadmin 22984 2015-11-04 /tmp/test/file1 ——文件所属组自动继承test的所属组 sticky bit在目录下创建的文件，只有文件的拥有者和此目录的拥有者有编辑、删除权限 设置方法： 12chmod +t /tmpchmod -t /tmp 以上权限也可以通过数字的方式来设置特殊权限 setuid=4，setgid=2，sticky bit=1 123chmod 4777 /test/file1 ——setuidchmod 2777 /test/file2 ——setgidchmod 1777 /test/ ——sticky bit attr权限（隐藏权限）用来设置特殊用户例如：root之类的权限。 12345678910111213141516设置方法：chattr 参数 文件+ ：在原有参数设定基础上，追加参数。- ：在原有参数设定基础上，移除参数。= ：更新为指定参数设定。A：文件或目录的 atime (access time)不可被修改(modified), 避免磁盘I/O读写占用资源。S：硬盘I/O同步选项，功能类似sync。a：即append，设定该参数后，只能向文件中添加数据，而不能删除，多用于服务器日志文件安全，只有root才能设定这个属性。c：即compresse，设定文件是否经压缩后再存储。读取时需要经过自动解压操作。d：即no dump，设定文件不能成为dump程序的备份目标。i：设定文件不能被删除、改名、设定链接关系，同时不能写入或新增内容。i参数对于文件 系统的安全设置有很大帮助。j：即journal，设定此参数使得当通过mount参数：data=ordered 或者 data=writeback 挂 载的文件系统，文件在写入时会先被记录(在journal中)。如果filesystem被设定参数为 data=journal，则该参数自动失效。s：保密性地删除文件或目录，即文件的block、inode空间被全部清空收回。u：与s相反，当设定为u时，数据内容其实还存在磁盘中，可以用于undeletion。各参数选项中常用到的是a和i。a选项强制只可添加不可删除，多用于日志系统的安全设定。而i是更为严格的安全设定，只有superuser (root) 或具有CAP_LINUX_IMMUTABLE处理能力（标识）的进程能够施加该选项。 例： 12345chattr +i /tmp/testfile1 ——增加i权限chattr +a /tmp/testfile1 ——增加a权限查看文件的attr权限可使用lsattr命令lsattr /tmp/testfile1 ——查看文件隐藏权限 ACL(访问控制列表)Linux中-rwxrwxrwx.这种权限方式只能规范出三种类型的权限，在系统的使用中并不能完全的满足管理者对于用户权限的指定，因此Linux推出了ACL访问控制列表这种权限管理方法，此方法可以针对不同的用户和组来分配不同的权限配置不局限于原始的UGO权限类型。 ACL权限的种类 R——读权限 W——写权限 X——执行权限 acl访问控制列表对于权限的规范和UGO类型的权限是一样的，也是通过RWX来组合控制用户权限。但设置方法不同具体如下 设置方法： 1234setfacl 参数 文件-m 设置acl规则-x 删除用户的acl规则-b 清空acl规则 例： 123setfacl -m u:test:rwx /tmp/testfile1 ——赋予用户test对于文件testfile1有RWX权限setfacl -x u:test /tmp/testfile1 ——删除用户test对于文件testfile1的acl条目setfacl -b /tmp/testfile1 ——清空testfile1文件的acl条目 查看文件acl是否开启可以通过ls -l 查看文件权限字段 最后一位是否为+号。如：-rwxrwxrwx+则此文件开启了acl，也可通过命令getfacl /tmp/testfile1来查看文件具体acl条目信息。","categories":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/tags/linux/"},{"name":"acl","slug":"acl","permalink":"https://blog.xiemx.com/tags/acl/"}],"author":"xiemx"},{"title":"Linux链接文件（ln）","slug":"2015-11-05-linux-ln","date":"2015-11-05T01:11:15.000Z","updated":"2019-10-19T11:59:17.079Z","comments":false,"path":"/2015/11/05/2015-11-05-linux-ln/","link":"","permalink":"https://blog.xiemx.com/2015/11/05/2015-11-05-linux-ln/","excerpt":"","text":"Linux系统中,内核为每一个文件分配一个inode ，文件属性保存在inode里，在访问文件时，文件系统通过查找到文件的inode中关于文件的存放block，进而去相对应的block中读取文件内容。而链接是一种在共享文件和访问它的用户的若干目录项之间建立联系的一种方法，在系统中有相同的一份文件A和B两个用户都需要这个文件，传统模式下我们会直接复制文件分给2个用户，这样虽然能实现目的。但是两份文件占用了我们双倍的block和inode资源。如果这个文件非常大的话将造成资源的极大浪费。此时Linux引入链接的概念，我们可以通过链接的方式去将文件共享给两个用户。 两种链接： 硬链接(Hard Link) 和软链接(Soft Link)，软链接又称为符号链接（Symbolic link） 硬链接硬链接文件可以理解为一个指针，指向文件的inode，系统不重新分配inode。硬链接文件中记录的是源文件的inode信息，所以硬链接文件的属性和源文件的属性相同只是在硬链接连接数上+1，硬链接文件。 创建方法： 1ln 源文件 目标文件 例子： 1234ln /tmp/testdir/testdir2/testfile1 /tmp/testfile1ls -i /tmp/testdir/testdir2/testfile1 /tmp/testfile11491138 -rw-r–r– 1 root root 48 11-14 14:10 /tmp/testdir/testdir2/testfile11491138 -rw-r–r– 1 root root 48 11-14 14:29 /tmp/testfile1 注意事项： 1.硬链接文件无法跨文件系统，因为不同的文件系统inode信息记录的信息不同。 2.只能对文件设置硬链接 3.删除硬链接文件或源文件，只会使硬链接计数器-1，不会删除文件内容。直到硬链接计数器为1时，在删除文件，此时则会清除文件。 4.修改一个文件，其他文件同时被修改 软连接软连接通过存储源文件的路径来定位源文件。所以软连接文件的大小为路径的长度。软链接文件属性同源文件属性不同。 创建方法: 1ln -s 源文件 目标文件 例子： 123ln -s /tmp/testfile /testfilesoftls -i /tmp/testfile /testfilesoft1491138 -rw-r–r– 1 root root 48 11-14 14:17 /tmp/testfile 1491140 lrwxrwxrwx 1 root root 13 11-14 14:24 /testfilesoft -&gt; /tmp/testfile 查看以上文件的输出结果，源文件和链接文件的属性不同。链接文件的大小为13字节，这13字节等于软连接的路径“/tmp/testfile”的长度。 注意事项： 1.软连接文件记录的是文件路径因此可以跨文件系统创建 2.可针对目录设置软连接 3.删除软连接文件不影响其它链接文件的访问，删除源文件则所有软连接文件都无法访问。 4.修改任意文件，所有文件都会变动 Linux系统中软硬链接的方式可使用的环境不同，其中软连接使用的最多","categories":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/tags/linux/"},{"name":"command","slug":"command","permalink":"https://blog.xiemx.com/tags/command/"}],"author":"xiemx"},{"title":"Linux用户和组","slug":"2015-11-05-linux-user-and-group","date":"2015-11-04T23:11:13.000Z","updated":"2019-10-19T13:02:55.732Z","comments":false,"path":"/2015/11/05/2015-11-05-linux-user-and-group/","link":"","permalink":"https://blog.xiemx.com/2015/11/05/2015-11-05-linux-user-and-group/","excerpt":"","text":"用户和组的配置文件在linux中，用户帐号，用户密码，用户组信息和用户组密码均是存放在不同的配置文件中的。 /etc/passwd存放用户配置信息 /etc/shadow存放用户密码和密码策略 /etc/group存放组配置信息 /etc/gshadow存放组配置信息 /etc/passwd文件中每行存储一个用户信息以：分割分7段具体含义如下 用户帐号:密码占位符:用户ID:用户组ID:描述:用户主目录:用户所使用的shell 123456[mingxu.xie@cn-aux-cc ~]$ cat /etc/passwdroot:x:0:0:root:/root:/bin/bashbin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologinlp:x:4:7:lp:/var/spool/lpd:/sbin/nologin /etc/shadow文件对应passwd文件分9段，具体信息如下 用户名:密码:最近修改时间:最短有效时间:最长有效时间:过期提醒日期:过期后宽限时间:账户失效时间:保留字段1234567[mingxu.xie@cn-aux-cc ~]$ sudo cat /etc/shadowroot:*LOCK*:14600::::::bin:*:16323:0:99999:7:::daemon:*:16323:0:99999:7:::adm:*:16323:0:99999:7:::lp:*:16323:0:99999:7:::test:$6$getqHfvX$xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx:18185:0:99999:7::: /etc/group文件保存了所有组的信息。文件中每一行表示一个组由3个冒号分隔成4段，具体信息如下 组名:密码占位符:组ID:组成员123456789101112[mingxu.xie@cn-aux-cc ~]$ sudo cat /etc/grouproot:x:0:bin:x:1:bin,daemondaemon:x:2:bin,daemonsys:x:3:bin,admadm:x:4:adm,daemontty:x:5:disk:x:6:lp:x:7:daemonmem:x:8:kmem:x:9:wheel:x:10:ec2-user,test /etc/gshadow文件对应group文件，分4段，具体信息如下 组名:口令:组管理者:组内用户列表123456[mingxu.xie@cn-aux-cc ~]$ sudo cat /etc/gshadowroot:::bin:::bin,daemondaemon:::bin,daemonsys:::bin,admadm:::adm,daemon 修改以上文件可以完成对用户和组属性和配置的改变，/sbin/nologin的shell可以使用户不可登陆等等。也可以通过命令来修改用户属性，但实质也是修改此配置文件中的参数。 用户和组的操作1234567891011121314151617181920212223242526272829303132333435363738394041424344454647481、添加新用户 useradd 选项 用户名 其中各选项含义如下： -c comment 指定一段注释性描述。 -d 目录 指定用户主目录，如果此目录不存在，则同时使用-m选项，能创建主目录。 -g 指定用户所属的用户组。 -G 指定用户所属的附加组。 -s 指定用户的登录Shell。 -u 指定用户的用户号，如果同时有-o选项，则能重复使用其他用户的标识号。useradd -G bin -g root -s /sbin/noloing -u 101 testuser2、添加组 用法：groupadd 选项 用户名 其中各选项含义如下： -g 指定用户组IDgroupadd -g 40000 testgrp3、修改用户属性和组属性 groupmod ——修改组属性 usermod ——修改用户属性 参数、语法和创建时使用的相同。4、删除用户和组 userdel testuser ——默认删除testuser用户的配置但不删除家目录和邮件需手工删除 userdel -r testuser ——删除用户同时删除家目录文件和邮件 groupdel testgrp ——删除组5、修改用户密码策略[mingxu.xie@cn-aux-cc ~]$ chage --helpUsage: chage [options] [LOGIN]Options: -d, --lastday LAST_DAY set date of last password change to LAST_DAY -E, --expiredate EXPIRE_DATE set account expiration date to EXPIRE_DATE -h, --help display this help message and exit -I, --inactive INACTIVE set password inactive after expiration to INACTIVE -l, --list show account aging information -m, --mindays MIN_DAYS set minimum number of days before password change to MIN_DAYS -M, --maxdays MAX_DAYS set maximim number of days before password change to MAX_DAYS -W, --warndays WARN_DAYS set expiration warning days to WARN_DAYS系统添加用户时如果没有规定用户的详细属性，则系统默认参照/etc/login.defs 和/etc/default/useradd来配置用户的属性！修改此处文档可以设置默认用户添加时的属性。","categories":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/tags/linux/"},{"name":"commands","slug":"commands","permalink":"https://blog.xiemx.com/tags/commands/"}],"author":"xiemx"},{"title":"Linux查找","slug":"2015-10-16-linux-find-commands","date":"2015-10-16T02:10:31.000Z","updated":"2019-10-19T13:24:27.253Z","comments":false,"path":"/2015/10/16/2015-10-16-linux-find-commands/","link":"","permalink":"https://blog.xiemx.com/2015/10/16/2015-10-16-linux-find-commands/","excerpt":"","text":"find 功能强大，用法自行google locate locate命令其实是”find -name”的另一种写法，但是要比后者快得多，原因在于它不搜索具体目录，而是搜索一个数据库（/var/lib/mlocate/mlocate.db），这个数据库中含 有本地所有文件信息。Linux系统自动创建这个数据库，并且每天自动更新一次，所以使用locate命令查不到最新变动过的文件。为了避免这种情况，可 以在使用locate之前，先使用updatedb命令，手动更新数据库。 例： locate /etc/sh* ——搜索etc目录下所有以sh开头的文件。 locate -i ~/m*——搜索用户主目录下，所有以m开头的文件，并且忽略大小写。 whereis whereis命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息。 which which命令的作用是，在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。","categories":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.xiemx.com/tags/linux/"},{"name":"command","slug":"command","permalink":"https://blog.xiemx.com/tags/command/"}],"author":"xiemx"},{"title":"PHP 调用date()函数时区报错","slug":"2015-10-11-php-data-timezone","date":"2015-10-10T20:10:30.000Z","updated":"2019-10-19T09:51:38.551Z","comments":false,"path":"/2015/10/11/2015-10-11-php-data-timezone/","link":"","permalink":"https://blog.xiemx.com/2015/10/11/2015-10-11-php-data-timezone/","excerpt":"","text":"现象：1234“PHP Warning: date() [function.date]: It is not safe to rely on the system’s timezone settings.You are *required* to use the date.timezone setting or the date_default_timezone_set() function.In case you used any of those methods and you are still getting this warning, you most likelymisspelled the timezone identifier. We selected ‘UTC’ for ’8.0/no DST’ instead in” php 有些版本默认的时区为格林威治标准时间，我们需要调整时区为+0800 “Asia/shanghai” 解决以下是三种方法(任选一种都行)： 1231. 在页头使用date_default_timezone_set()设置 date_default_timezone_set('PRC');2. 在页头使用 ini_set('date.timezone','Asia/Shanghai');3. 修改php.ini。打开php.ini查找date.timezone 去掉前面的分号修改成为：date.timezone = PRC","categories":[{"name":"php","slug":"php","permalink":"https://blog.xiemx.com/categories/php/"}],"tags":[{"name":"debug","slug":"debug","permalink":"https://blog.xiemx.com/tags/debug/"},{"name":"php","slug":"php","permalink":"https://blog.xiemx.com/tags/php/"}],"author":"xiemx"},{"title":"window系统文件或目录无法删除解决方法","slug":"2015-10-08-window-force-delete-file","date":"2015-10-07T20:10:01.000Z","updated":"2019-09-27T06:52:36.185Z","comments":false,"path":"/2015/10/08/2015-10-08-window-force-delete-file/","link":"","permalink":"https://blog.xiemx.com/2015/10/08/2015-10-08-window-force-delete-file/","excerpt":"","text":"新建.BAT批处理文件输入如下命令，然后将要删除的文件拖放到批处理文件图标上即可删除。 12DEL /F /A /QRD /S /Q 可利用软件来解锁占用文件的进程推荐unlock","categories":[{"name":"window","slug":"window","permalink":"https://blog.xiemx.com/categories/window/"}],"tags":[{"name":"window","slug":"window","permalink":"https://blog.xiemx.com/tags/window/"}],"author":"xiemx"},{"title":"session_start()函数报错","slug":"2015-09-30-session_start-error","date":"2015-09-29T17:09:15.000Z","updated":"2019-10-19T08:15:13.023Z","comments":false,"path":"/2015/09/30/2015-09-30-session_start-error/","link":"","permalink":"https://blog.xiemx.com/2015/09/30/2015-09-30-session_start-error/","excerpt":"","text":"现象Warning: session_start() [function.session-start]: Cannot send session cache limiter - headers already sent 解决(PHP 4, PHP 5) session_start — 启动新会话或者重用现有会话 在调用此函数是不能向浏览器输出内容，如出现上面报错可以将函数放到 &lt;？php最上方顶句如下 1234&lt;?phpsession_start();include xxxxxxxx;?&gt;","categories":[{"name":"php","slug":"php","permalink":"https://blog.xiemx.com/categories/php/"},{"name":"debug","slug":"php/debug","permalink":"https://blog.xiemx.com/categories/php/debug/"}],"tags":[{"name":"debug","slug":"debug","permalink":"https://blog.xiemx.com/tags/debug/"},{"name":"php","slug":"php","permalink":"https://blog.xiemx.com/tags/php/"}],"author":"xiemx"},{"title":"PHPCMS V9 “密码重试次数太多，请过-xxx分钟后重新登录！”的解决办法","slug":"2015-09-29-phpcms-v9-password-retry-too-more","date":"2015-09-28T18:09:35.000Z","updated":"2019-10-19T09:47:11.968Z","comments":false,"path":"/2015/09/29/2015-09-29-phpcms-v9-password-retry-too-more/","link":"","permalink":"https://blog.xiemx.com/2015/09/29/2015-09-29-phpcms-v9-password-retry-too-more/","excerpt":"","text":"找到文件 /phpcms/modules/admin/index.php 将如下代码注释掉： 1234if($rtime['times'] &gt;= $maxloginfailedtimes) &#123;$minute = 60-floor((SYS_TIME-$rtime['logintime'])/60);showmessage(L('wait_1_hour',array('minute'=&gt;$minute)));&#125; 注意哦，一共是4行。","categories":[{"name":"php","slug":"php","permalink":"https://blog.xiemx.com/categories/php/"}],"tags":[{"name":"debug","slug":"debug","permalink":"https://blog.xiemx.com/tags/debug/"},{"name":"php","slug":"php","permalink":"https://blog.xiemx.com/tags/php/"},{"name":"CMS","slug":"CMS","permalink":"https://blog.xiemx.com/tags/CMS/"}],"author":"xiemx"},{"title":"phpmyadmin上传文件大小修改限制","slug":"2015-08-24-phpmyadmin-upload-limit-size","date":"2015-08-23T18:08:12.000Z","updated":"2019-10-19T09:44:05.643Z","comments":false,"path":"/2015/08/24/2015-08-24-phpmyadmin-upload-limit-size/","link":"","permalink":"https://blog.xiemx.com/2015/08/24/2015-08-24-phpmyadmin-upload-limit-size/","excerpt":"","text":"修改phpmyadmin上传文件大小限制主要分修改php.ini配置文件和phpmyadmin配置文件两个步骤。 第一步：修改php.ini配置文件中文件上传大小配置**此步骤与一般的php.ini 配置文件上传功能方法一致，需要修改php.ini配置文件中upload_max_filesize和post_max_size两个选项值 第二步：修改php执行时间及内存限制实现phpmyadmin上传大文件功能如果想要phpmyadmin上传大文件，还需修改php.ini配置文件中的max_execution_time（php页面执行最大时间）、 max_input_time（php页面接受数据最大时间）、memory_limit（php页面占用的最大内存）三个配置选项，这是因为 phpmyadmin上传大文件时，php页面的执行时间、内存占用也势必变得更长更大，其需要php运行环境的配合，光修改上传文件大小限制是不够的。 第三步：修改phpmyadmin配置文件在完成php.ini的相关配置后，还需要修改phpmyadmin配置。 1、修改phpmyadmin config配置文件中的$cfg［&#39;ExecTimeLimit&#39;］配置选项，默认值是300，需要修改为0，即没有时间限制。 2、修改phpmyadmin安装根目录下的import页面中的$memory_limit 说明：首选读取php.ini配置文件中的内存配置选项memory_limit，如果为空则默认内存大小限制为2M，如果没有限制则内存大小限制为10M，你可以结合你php.ini配置文件中的相关信息修改这段代码。 至此，经过修改php.ini配置文件中的文件上传配置选项以及phpmyadmin配置文件后，即可解决phpmyadmin上传文件大小限制问题，从而实现phpmyadmin上传大文件功能。","categories":[{"name":"mysql","slug":"mysql","permalink":"https://blog.xiemx.com/categories/mysql/"}],"tags":[{"name":"database","slug":"database","permalink":"https://blog.xiemx.com/tags/database/"},{"name":"mysql","slug":"mysql","permalink":"https://blog.xiemx.com/tags/mysql/"},{"name":"php","slug":"php","permalink":"https://blog.xiemx.com/tags/php/"},{"name":"phpmyadmin","slug":"phpmyadmin","permalink":"https://blog.xiemx.com/tags/phpmyadmin/"}],"author":"xiemx"},{"title":"MySQL数据库mysqlcheck的使用方法详解","slug":"2015-08-24-mysqlcheck","date":"2015-08-23T17:08:44.000Z","updated":"2019-10-19T10:29:45.582Z","comments":false,"path":"/2015/08/24/2015-08-24-mysqlcheck/","link":"","permalink":"https://blog.xiemx.com/2015/08/24/2015-08-24-mysqlcheck/","excerpt":"","text":"mysqlcheck，是mysql自带的可以检查和修复MyISAM表，并且它还可以优化和分析表，mysqlcheck的功能类似myisamchk，但其工作不同。主要差别是当mysqld服务器在运行时必须使用mysqlcheck，而myisamchk应用于服务器没有运行时。myisamchk修复失败是不可逆的。 1234567891.如果需要检查并修复所有的数据库的数据表mysqlcheck -A -o -r -pEnter password:database1 OKdatabase2 OK2.如果需要修复指定的数据库用mysqlcheck -A -o -r database1 -pdatabase1 OK","categories":[{"name":"mysql","slug":"mysql","permalink":"https://blog.xiemx.com/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://blog.xiemx.com/tags/mysql/"},{"name":"mysqlcheck","slug":"mysqlcheck","permalink":"https://blog.xiemx.com/tags/mysqlcheck/"},{"name":"tool","slug":"tool","permalink":"https://blog.xiemx.com/tags/tool/"}],"author":"xiemx"},{"title":"URLRewriter设置Config和IIS配置做伪静态","slug":"2015-08-21-iis-set-urlrewrite","date":"2015-08-21T02:08:51.000Z","updated":"2019-10-17T07:34:07.555Z","comments":true,"path":"/2015/08/21/2015-08-21-iis-set-urlrewrite/","link":"","permalink":"https://blog.xiemx.com/2015/08/21/2015-08-21-iis-set-urlrewrite/","excerpt":"","text":"一、首先检查config文件里面是否包含这两个节点 1234567891011121314&lt;configSections&gt; &lt;section name=\"RewriterConfig\" requirePermission=\"false\" type=\"URLRewriter.Config.RewriterConfigSerializerSectionHandler, URLRewriter\"/&gt;&lt;/configSections&gt;&lt;system.web&gt; &lt;httpModules&gt; &lt;add type=\"URLRewriter.ModuleRewriter, URLRewriter\" name=\"ModuleRewriter\"/&gt; &lt;/httpModules&gt;&lt;/system.web&gt; 二、配置IIS。 1.打开IIS，选择网站。右键，点击属性 2.选择主目录，点击配置按钮 3.在弹出的应用程序配置中，点击添加 4.在弹出的添加/编辑应用程序扩展名映射中，填写可执行文件 ​ 点击浏览，选择aspnet_isapi.dll这个DLL，它一般在：C:\\WINDOWS\\Microsoft.NET\\Framework\\v2.0.50727文件夹中(也就是你的系统盘中） 5.在扩展名中填写.html 6.在动作里选择限制为，他的框就可以填写了！请填写：GET,POST 7.不要勾选把确认文件是否存在，然后点击确定","categories":[{"name":"iis","slug":"iis","permalink":"https://blog.xiemx.com/categories/iis/"},{"name":"windows","slug":"iis/windows","permalink":"https://blog.xiemx.com/categories/iis/windows/"}],"tags":[{"name":"windows","slug":"windows","permalink":"https://blog.xiemx.com/tags/windows/"},{"name":"iis","slug":"iis","permalink":"https://blog.xiemx.com/tags/iis/"}],"author":"xiemx"},{"title":"MSSQL数据库 阻止保存要求重新创建表的更改","slug":"2015-08-21-mssql-deny-save","date":"2015-08-21T02:08:42.000Z","updated":"2019-10-19T11:09:04.998Z","comments":true,"path":"/2015/08/21/2015-08-21-mssql-deny-save/","link":"","permalink":"https://blog.xiemx.com/2015/08/21/2015-08-21-mssql-deny-save/","excerpt":"","text":"MSSQL2008以上 工具菜单—-选项—-Designers(设计器)—-阻止保存要求重新创建表的更改 取消勾选即可 MSSql2005下 添加excute权限","categories":[{"name":"mssql","slug":"mssql","permalink":"https://blog.xiemx.com/categories/mssql/"}],"tags":[{"name":"mssql","slug":"mssql","permalink":"https://blog.xiemx.com/tags/mssql/"},{"name":"database","slug":"database","permalink":"https://blog.xiemx.com/tags/database/"},{"name":"debug","slug":"debug","permalink":"https://blog.xiemx.com/tags/debug/"}],"author":"xiemx"},{"title":"常见MIME类型设置方法","slug":"2015-08-21-mime-type","date":"2015-08-20T17:08:58.000Z","updated":"2019-09-27T06:50:46.124Z","comments":false,"path":"/2015/08/21/2015-08-21-mime-type/","link":"","permalink":"https://blog.xiemx.com/2015/08/21/2015-08-21-mime-type/","excerpt":"","text":"常见MIME类型和设置方法当前列举了常用的MIME类型，查询详细：MIME Type MIME类型 扩展名 文档类型 MIME 类型 .aac AAC audio audio/aac .abw AbiWord document application/x-abiword .arc Archive document (multiple files embedded) application/x-freearc .avi AVI: Audio Video Interleave video/x-msvideo .azw Amazon Kindle eBook format application/vnd.amazon.ebook .bin Any kind of binary data application/octet-stream .bmp Windows OS/2 Bitmap Graphics image/bmp .bz BZip archive application/x-bzip .bz2 BZip2 archive application/x-bzip2 .csh C-Shell script application/x-csh .css Cascading Style Sheets (CSS) text/css .csv Comma-separated values (CSV) text/csv .doc Microsoft Word application/msword .epub Electronic publication (EPUB) application/epub+zip .gif Graphics Interchange Format (GIF) image/gif .html HyperText Markup Language (HTML) text/html .ico Icon format image/vnd.microsoft.icon .ics iCalendar format text/calendar .jar Java Archive (JAR) application/java-archive .jpeg JPEG images image/jpeg .js JavaScript text/javascript .json JSON format application/json .mjs JavaScript module text/javascript .mp3 MP3 audio audio/mpeg .mpeg MPEG Video video/mpeg .oga OGG audio audio/ogg .ogv OGG video video/ogg .ogx OGG application/ogg .otf OpenType font font/otf .png Portable Network Graphics image/png .pdf Adobe Portable Document Format (PDF) application/pdf .rar RAR archive application/x-rar-compressed .rtf Rich Text Format (RTF) application/rtf .sh Bourne shell script application/x-sh .svg Scalable Vector Graphics (SVG) image/svg+xml .tar Tape Archive (TAR) application/x-tar .tiff Tagged Image File Format (TIFF) image/tiff .ttf TrueType Font font/ttf .txt Text, (generally ASCII or ISO 8859-n) text/plain .vsd Microsoft Visio application/vnd.visio .wav Waveform Audio Format audio/wav .woff Web Open Font Format (WOFF) font/woff .woff2 Web Open Font Format (WOFF) font/woff2 .xhtml XHTML application/xhtml+xml .xls Microsoft Excel application/vnd.ms-excel .xml XML application/xml .zip ZIP archive application/zip .3gp 3GPP audio/video container video/3gpp .3g2 3GPP2 audio/video container video/3gpp2 .7z 7-zip archive application/x-7z-compressed 设置方法12345# IIS默认网站属性--&gt;http 头--&gt;MIME映射--&gt;文件类型--&gt;新类型# nginxconf/mime.types","categories":[{"name":"http","slug":"http","permalink":"https://blog.xiemx.com/categories/http/"},{"name":"mime-type","slug":"http/mime-type","permalink":"https://blog.xiemx.com/categories/http/mime-type/"}],"tags":[{"name":"http","slug":"http","permalink":"https://blog.xiemx.com/tags/http/"},{"name":"mime-type","slug":"mime-type","permalink":"https://blog.xiemx.com/tags/mime-type/"}],"author":"xiemx"}]}