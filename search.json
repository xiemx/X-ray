[{"title":"alicloud k8s 日志接入 sls","url":"//2020/12/17/aliyun-ack-sls/","content":"## 采集原理\n通过 DaemonSet 模式在每个 node 上运行 logtail client，客户端启动后加入指定 machine-group，通过 docker.sock 与 ecs 上的 docker daemon 通讯，通过检索 container 环境变量来查找符合采集规则的 container，以及定位内部文件和容器 stdout 日志，在通过监听内核 inotify 事件来进行日志文件变化监听。 \n## Pod 日志接入模式（环境变量注入）\n通过在pod中注入环境变量aliyun_logs_{key} 来接入aliyun SLS日志系统，如未指定project 则使用在DaemonSet 模式默认部署的 logtail 默认配置。\n\n## logtail-ds configMap\n采集插件安装不再赘述\n\n```yaml\napiVersion: v1\ndata:\n  cpu-core-limit: \"2\"\n  log-ali-uid: \"163139xxxxxxxx497\"\n  log-config-path: /etc/ilogtail/conf/cn-hangzhou/ilogtail_config.json\n  log-endpoint: cn-hangzhou-intranet.log.aliyuncs.com\n  log-machine-group: k8s-group-cbe7f0d404866409exxxxxxcefeda\n  log-project: k8s-log-c79a7ce35db39488xxxxxx9432e53e6\n  max-bytes-per-sec: \"209715200\"\n  mem-limit: \"512\"\n  send-requests-concurrency: \"20\"\nkind: ConfigMap\nmetadata:\n  name: alibaba-log-configuration\n  namespace: kube-system\n```\n\n## 环境变量\n| 支持的环境变量                 |                                                                                                                                                                                   |                                                      |\n| ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------- |\n| aliyun_logs_{key}              | 必选项。{key}只能包含小写字母、数字和-。若不存在aliyun_logs_{key}_logstore，则默认创建并采集到名为{key}的logstore。当值为stdout表示采集容器的标准输出；其他值为容器内的日志路径。 | 默认采集方式为极简模式, {key}需保持在K8s集群内唯一。 |\n| aliyun_logs_{key}_tags         | 可选。值为{tag-key}={tag-value}类型，用于对日志进行标识。                                                                                                                         | -                                                    |\n| aliyun_logs_{key}_project      | 可选。值为指定的日志服务Project。当不存在该环境变量时为您安装时所选的Project。                                                                                                    | Project需与您的Logtail工作所在Region一致。           |\n| aliyun_logs_{key}_logstore     | 可选。值为指定的日志服务Logstore。当不存在该环境变量时Logstore和{key}一致。                                                                                                       | -                                                    |\n| aliyun_logs_{key}_shard        | 可选。值为创建Logstore时的shard数，有效值为1~10。当不存在该环境变量时值为2。                                                                                                      | -                                                    |\n| aliyun_logs_{key}_ttl          | 可选。值为指定的日志保存时间，有效值为1~3650。当取值为3650时，指定日志的保存时间为永久保存。当不存在该环境变量时，默认指定日志的保存时间为90天。                                  | -                                                    |\n| aliyun_logs_{key}_machinegroup | 可选。值为应用的机器组。当不存在该环境变量时与安装Logtail的默认机器组一致。                                                                                                       | -                                                    |\n\n\n----\n\n## Example\n### Step1\n  创建测试容器，并注入环境变量\n```yaml\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app.kubernetes.io/instance: xiemx-training\n        app.kubernetes.io/name: xiemx-training\n    spec:\n      containers:\n      - env:\n        - name: aliyun_logs_xiemx-training-stdout-log\n          value: stdout\n        image: nginx:1.16.0\n```\n### Step 2\n此时阿里云SLS日志服务中即会创建logStore，默认 logtail 为极简模式，日志未进行解析。\n\n<div align=center><img src=\"/images/ali-sls-logstore.png\"></div>\n\n```json\n{\n    \"inputs\": [\n        {\n            \"detail\": {\n                \"IncludeEnv\": {\n                    \"aliyun_logs_xiemx-training-stdout-log\": \"stdout\"\n                },\n                \"Stderr\": true,\n                \"Stdout\": true\n            },\n            \"type\": \"service_docker_stdout\"\n        }\n    ]\n}\n```\n### Step 3\n进行日志拆分极细，aliyun支持多种解析插件，当前使用自定义的正则来演示，logtail 配置如下。\n插件可参考：https://help.aliyun.com/document_detail/64957.html?spm=a2c4g.11186623.6.640.60a31de3orfOsD\n```json\n{\n    \"inputs\": [\n        {\n            \"detail\": {\n                \"IncludeEnv\": {\n                    \"aliyun_logs_xiemx-training-stdout-log\": \"stdout\"\n                },\n                \"Stderr\": true,\n                \"Stdout\": true\n            },\n            \"type\": \"service_docker_stdout\"\n        }\n    ],\n    \"processors\": [\n        {\n            \"detail\": {\n                \"KeepSource\": true,\n                \"NoMatchError\": true,\n                \"Keys\": [\n                    \"remote_addr\",\n                    \"time\",\n                    \"http_method\",\n                    \"request_uri\",\n                    \"status_code\",\n                    \"body_bytes_sent\",\n                    \"http_referer\",\n                    \"user_agent\",\n                    \"http_x_forwarded_for\"\n                ],\n                \"SourceKey\": \"content\",\n                \"Regex\": \"(\\\\S+) \\\\S+ \\\\S+ \\\\[(\\\\S+) \\\\S+\\\\] \\\"([A-Z]+) (.*)\\\" (\\\\d+) (\\\\S+) \\\"(\\\\S+)\\\" \\\"(.*)\\\" \\\"(.*)\\\"\",\n                \"NoKeyError\": true\n            },\n            \"type\": \"processor_regex\"\n        }\n    ]\n}\n```\n需要注意：当使用 regex 进行分组时，请务必不要使用 `(()())` 类似嵌套表达式会导致日志解析结果和 Key 对应关系错乱，日志数据混乱。如下例子：\n```\n((\\d+\\.){3}(\\d+))  --替换-> (\\S+)\n```\n### Step 4\n此时如果正则解析无问题的情况下即可查看刀拆分后的日志，如没有查看刀拆分的日志，可点击诊断按钮查看客户端错误进行debug。\n<div align=center><img src=\"/images/ali-sls-search.png\"></div>","tags":["k8s","aliyun"],"categories":["k8s"]},{"title":"Ingress Controller V2 TargetGroupBinding使用方法","url":"//2020/12/03/aws-targetgroupbinding/","content":"Ingress Controller V2 TargetGroupBinding使用方法 场景\n实现EC2到EKS的平滑过渡，想在暴露EKS Ingress的时候使用原来给EC2使用的ALB，因此可以使用Ingress Controller V2版本的TargetGroupBinding新功能。\nDemo步骤\n- 安装ALBIngressControllerv2版本, 或将v1版本迁移至v2 \n    - https://kubernetes-sigs.github.io/aws-load-balancer-controller/guide/controller/installation/\n    - https://kubernetes-sigs.github.io/aws-load-balancer-controller/guide/upgrade/migrate_v1_v2/\n- 创建一个新的targetgroup，并记录下arn，后续要把该arn写到TargetGroupBinding的CR当中\n- 在alb上配置到目标组的路由策略\n   - ![tgbinding-rule](/images/aws-tgbinding.jpg)\n- 赋予nodegroup上的iam role能够注册到目标组的权限\n- 创建ns/deployment/services\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: wormhole\n    service: wormhole\nspec:\n  progressDeadlineSeconds: 600\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: wormhole\n      service: wormhole\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 25%\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: wormhole\n        service: wormhole\n    spec:\n      containers:\n      - envFrom:\n        - configMapRef:\n            name: wormhole\n        image: nginx\n        imagePullPolicy: IfNotPresent\n        name: wormhole\n        ports:\n        - containerPort: 80\n          name: http-web\n          protocol: TCP\n        - containerPort: 443\n          name: https-web\n          protocol: TCP\n      nodeSelector:\n        node.kubernetes.io/service-type: product\n        node.kubernetes.io/workload-type: stateless\n      restartPolicy: Always\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: wormhole-svc\nspec:\n  externalTrafficPolicy: Local\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: http-web\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: https-web\n  selector:\n    app: wormhole\n  type: NodePort\n```\n\n- service暴露完成之后，不再需要创建ingress了，直接和targetgroup进行绑定，（kubectl logs aws-load-balancer-controller-xxx -f -n kube-system可查看是否发生错误)，注册到目标组的方式通过targetType来指定，支持ip与instance两种方式，绑定后，实例或ip会自动注册到目标组中\n    - ```yaml\n        apiVersion: elbv2.k8s.aws/v1alpha1\n        kind: TargetGroupBinding\n        metadata: \n          name: wormhole-tg-bind\n        spec:\n          targetType: instance\n        serviceRef:\n          name: wormhole-svc\n          port: 80\n        targetGroupArn: arn:aws:elasticloadbalancer:xxxxxx:xxxxxxxx:targetgroup/tg-bind\n        ```\n    - ![aws-tgbinding-ec2](/images/aws-tgbinding-ec2.jpg)\n---\n**注意**\n- 和一般的ingress不同，路由策略要在alb侦听器上自行编辑\n- 实例端口也不会自动在安全组中开放，需要在node使用的安全组上自行打开 \n- node需要有足够的权限注册到目标组中\n ","tags":["k8s"],"categories":["k8s"]},{"title":"k8s services iptables规则探究","url":"//2020/12/03/k8s-services-iptables/","content":"### 结论\n本文只讨论iptables的services实现，ipvs实现有所不同，这里先说结论如下：\n- service 使用iptables 来进行pod的dnat，services本身只是一条虚拟的iptables规则，不存在实体。\n- service 使用iptales 扩展模块来实现pod轮询策略故而无法实现最小链接数等负载算法\n  - 多pod下的RR实现使用iptables statistic模块的random算法加权实现\n  - sessionAffinity 使用iptables recent模块实现保持，功能主要依赖于nf_conntrack来实现连接标记\n\n---\n#### 单pod时service iptables 规则实现\n```shell\n# nginx pod ip地址：\n$ kubectl describe pod nginx-fb8d45fxt-dcc1t | grep \"IP\"\n\nIP:             10.200.1.21\n\n# Service服务，通过172.30.13.253:80则实际访问到10.200.1.21:80\n$ kubectl describe svc nginx\n...\nType:              ClusterIP\nIP:                172.30.13.25\nPort:              <unset>  80/TCP\nTargetPort:        80/TCP\nEndpoints:         10.200.1.21:80\nSession Affinity:  None\n...\n\n$ iptables-save (选取部分关键规则)\n\n...\n\n-A PREROUTING -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\n-A OUTPUT -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\n\n...\n\n-A KUBE-SEP-UWNFTKZFYWNNNTK7 -s 10.200.1.21/32 -m comment --comment \"demo/nginx:\" \\\n   -j KUBE-MARK-MASQ\n-A KUBE-SEP-UWNFTKZFYWNNNTK7 -p tcp -m comment --comment \"demo/nginx:\" \\\n   -m tcp -j DNAT --to-destination 10.200.1.21:80\n-A KUBE-SERVICES -d 172.30.13.25/32 -p tcp -m comment \\\n   --comment \"demo/nginx: cluster IP\" -m tcp --dport 80 -j KUBE-SVC-1T7IBBNZDJ76\n-A KUBE-SVC-1T7IBBNZDJ76 -m comment --comment \"demo/nginx:\" \\\n   -j KUBE-SEP-UWNFTKZFYWNNNTK7\n\n```\n#### 分析：\n当访问 172.30.13.25/32 的80 端口时，iptables 规则会被第三条规则命中KUBE-SERVICES，在转发到 KUBE-SVC-1T7IBBNZDJ76 链 ，KUBE-SVC-1T7IBBNZDJ76链对流量进行comment标记之后 转发到 KUBE-SEP-UWNFTKZFYWNNNTK7\n该链上实现了第一条访问外网的和一条对内的DNAT，此时是向内访问规则会命中第二条向内部进行DNAT 转发到pod地址上，进行路由寻址。\n\n---\n### 负载算法实现\n\n```shell\n$ kubectl scale deploy/nginx --replicas=3\n\n\n$ iptables-save (选取部分规则)\n-A KUBE-SEP-BI123VOIAZZ5S7 -s 10.129.1.12/32 -m comment --comment \"demo/nginx:\" \\\n   -j KUBE-MARK-MASQ\n-A KUBE-SEP-BI123VOIAZZ5S7 -p tcp -m comment --comment \"demo/nginx:\" \\\n   -m tcp -j DNAT --to-destination 10.129.1.12:80\n\n-A KUBE-SEP-CDQIKASRG66RK -s 10.129.1.13/32 -m comment --comment \"demo/nginx:\" \\\n   -j KUBE-MARK-MASQ\n-A KUBE-SEP-CDQIKASRG66RK -p tcp -m comment --comment \"demo/nginx:\" \\\n   -m tcp -j DNAT --to-destination 10.129.1.13:80\n\n-A KUBE-SEP-W5HTO42ZVNHJQWBG -s 10.129.1.14/32 -m comment --comment \"demo/nginx:\" \\\n   -j KUBE-MARK-MASQ\n-A KUBE-SEP-W5HTO42ZVNHJQWBG -p tcp -m comment --comment \"demo/nginx:\" \\\n   -m tcp -j DNAT --to-destination 10.129.1.14:80\n\n-A KUBE-SERVICES -d 172.30.13.25/32 -p tcp -m comment \\\n   --comment \"demo/nginx: cluster IP\" -m tcp --dport 80 -j KUBE-SVC-1T7IBBNZDJ76\n\n-A KUBE-SVC-1T7IBBNZDJ76 -m comment --comment \"demo/nginx:\" \\\n   -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-BI123VOIAZZ5S7\n-A KUBE-SVC-1T7IBBNZDJ76 -m comment --comment \"demo/nginx:\" \\\n   -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-CDQIKASRG66RK\n-A KUBE-SVC-1T7IBBNZDJ76 -m comment --comment \"demo/nginx:\" \\\n   -j KUBE-SEP-W5HTO42ZVNHJQWBG\n\n```\n### 分析：\niptable 在实现多pod 负载算法的时候使用了 statistic模块的random 加权算法 `-m statistic --mode random --probability 0.33332999982 `来实现。具体模块用法可以参考：http://ipset.netfilter.org/iptables-extensions.man.html\n\n\n---\n\n### 会话保持\n``` shell\n$ kubectl edit svc nginx\n...\n  sessionAffinity: ClientIP\n  sessionAffinityConfig:\n    clientIP:\n      timeoutSeconds: 3600\n...\n\n$ iptables-save (选取部分规则) \n-A KUBE-SEP-BI123VOIAZZ5S7 -s 10.129.1.12/32 -m comment --comment \"demo/nginx:\" \\\n   -j KUBE-MARK-MASQ\n-A KUBE-SEP-BI123VOIAZZ5S7 -p tcp -m comment --comment \"demo/nginx:\" \\\n   -m recent --set --name KUBE-SEP-BI123VOIAZZ5S7 --mask 255.255.255.255 \\\n   --rsource -m tcp -j DNAT --to-destination 10.129.1.12:80\n\n...\n\n-A KUBE-SERVICES -d 172.30.13.25/32 -p tcp -m comment \\\n   --comment \"demo/nginx: cluster IP\" -m tcp --dport 80 -j KUBE-SVC-1T7IBBNZDJ76\n\n-A KUBE-SVC-1T7IBBNZDJ76 -m comment --comment \"demo/nginx:\" \\\n   -m recent --rcheck --seconds 3600 --reap --name KUBE-SEP-BI123VOIAZZ5S7 \\\n   --mask 255.255.255.255 --rsource -j KUBE-SEP-BI123VOIAZZ5S7\n\n...\n\n-A KUBE-SVC-1T7IBBNZDJ76 -m comment --comment \"demo/nginx:\" \\\n   -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-BI123VOIAZZ5S7\n-A KUBE-SVC-1T7IBBNZDJ76 -m comment --comment \"demo/nginx:\" \\\n   -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-CDQIKASRG66RK\n-A KUBE-SVC-1T7IBBNZDJ76 -m comment --comment \"demo/nginx:\" \\\n   -j KUBE-SEP-W5HTO42ZVNHJQWBG\n```\n#### 分析\niptables 主要使用recent 模块来标记追踪连接状态 `-m recent --rcheck --seconds 3600`来完成会话保持。具体recent模块用法可以参考：http://ipset.netfilter.org/iptables-extensions.man.html","tags":["k8s","iptables"],"categories":["k8s","iptables"]},{"title":"k8s cluster-autoscaler","url":"//2020/12/02/k8s-cluster-autoscaler/","content":"为了在集群中能够动态的根据pod的资源使用量来进行node的动态扩容，K8S cluster-autoscaler 提供了这样的功能，目前接入了大部分主流的厂商，由于我们使用的是aws eks，这里只进行aws 编排。\n - 项目地址：https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler\n\n\n```yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    k8s-addon: cluster-autoscaler.addons.k8s.io\n    k8s-app: cluster-autoscaler\n  name: cluster-autoscaler\n  namespace: kube-system\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: cluster-autoscaler\n  labels:\n    k8s-addon: cluster-autoscaler.addons.k8s.io\n    k8s-app: cluster-autoscaler\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"events\", \"endpoints\"]\n    verbs: [\"create\", \"patch\"]\n  - apiGroups: [\"\"]\n    resources: [\"pods/eviction\"]\n    verbs: [\"create\"]\n  - apiGroups: [\"\"]\n    resources: [\"pods/status\"]\n    verbs: [\"update\"]\n  - apiGroups: [\"\"]\n    resources: [\"endpoints\"]\n    resourceNames: [\"cluster-autoscaler\"]\n    verbs: [\"get\", \"update\"]\n  - apiGroups: [\"\"]\n    resources: [\"nodes\"]\n    verbs: [\"watch\", \"list\", \"get\", \"update\"]\n  - apiGroups: [\"\"]\n    resources:\n      - \"pods\"\n      - \"services\"\n      - \"replicationcontrollers\"\n      - \"persistentvolumeclaims\"\n      - \"persistentvolumes\"\n    verbs: [\"watch\", \"list\", \"get\"]\n  - apiGroups: [\"extensions\"]\n    resources: [\"replicasets\", \"daemonsets\"]\n    verbs: [\"watch\", \"list\", \"get\"]\n  - apiGroups: [\"policy\"]\n    resources: [\"poddisruptionbudgets\"]\n    verbs: [\"watch\", \"list\"]\n  - apiGroups: [\"apps\"]\n    resources: [\"statefulsets\", \"replicasets\", \"daemonsets\"]\n    verbs: [\"watch\", \"list\", \"get\"]\n  - apiGroups: [\"storage.k8s.io\"]\n    resources: [\"storageclasses\", \"csinodes\"]\n    verbs: [\"watch\", \"list\", \"get\"]\n  - apiGroups: [\"batch\", \"extensions\"]\n    resources: [\"jobs\"]\n    verbs: [\"get\", \"list\", \"watch\", \"patch\"]\n  - apiGroups: [\"coordination.k8s.io\"]\n    resources: [\"leases\"]\n    verbs: [\"create\"]\n  - apiGroups: [\"coordination.k8s.io\"]\n    resourceNames: [\"cluster-autoscaler\"]\n    resources: [\"leases\"]\n    verbs: [\"get\", \"update\"]\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: cluster-autoscaler\n  namespace: kube-system\n  labels:\n    k8s-addon: cluster-autoscaler.addons.k8s.io\n    k8s-app: cluster-autoscaler\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"configmaps\"]\n    verbs: [\"create\",\"list\",\"watch\"]\n  - apiGroups: [\"\"]\n    resources: [\"configmaps\"]\n    resourceNames: [\"cluster-autoscaler-status\", \"cluster-autoscaler-priority-expander\"]\n    verbs: [\"delete\", \"get\", \"update\", \"watch\"]\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: cluster-autoscaler\n  labels:\n    k8s-addon: cluster-autoscaler.addons.k8s.io\n    k8s-app: cluster-autoscaler\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-autoscaler\nsubjects:\n  - kind: ServiceAccount\n    name: cluster-autoscaler\n    namespace: kube-system\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: cluster-autoscaler\n  namespace: kube-system\n  labels:\n    k8s-addon: cluster-autoscaler.addons.k8s.io\n    k8s-app: cluster-autoscaler\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: cluster-autoscaler\nsubjects:\n  - kind: ServiceAccount\n    name: cluster-autoscaler\n    namespace: kube-system\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    cluster-autoscaler.kubernetes.io/safe-to-evict: \"false\"\n  name: cluster-autoscaler\n  namespace: kube-system\n  labels:\n    app: cluster-autoscaler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cluster-autoscaler\n  template:\n    metadata:\n      labels:\n        app: cluster-autoscaler\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '8085'\n    spec:\n      nodeSelector:\n        node.kubernetes.io/service-type: auxiliary\n        node.kubernetes.io/workload-type: stateless\n      serviceAccountName: cluster-autoscaler\n      containers:\n        - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.17.3\n          name: cluster-autoscaler\n          resources:\n            limits:\n              cpu: 100m\n              memory: 300Mi\n            requests:\n              cpu: 100m\n              memory: 300Mi\n          command:\n            - ./cluster-autoscaler\n            - --v=4\n            - --stderrthreshold=info\n            - --cloud-provider=aws\n            - --skip-nodes-with-local-storage=true\n            - --expander=least-waste\n            # 我们这里使用自动发现模式，将底层的机器配置，spot/on-daemon机器比例等等逻辑放到aws autoscaling group中去，通过配置的tag寻找对应的asg组，建议使用此方法也可参考官网使用其它模式。\n            - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,kubernetes.io/cluster/aux-eks \n            # - --balance-similar-node-groups\n            - --skip-nodes-with-system-pods=false\n            # - --aws-use-static-instance-list=true\n          volumeMounts:\n            - name: ssl-certs\n              mountPath: /etc/ssl/certs/ca-certificates.crt\n              readOnly: true\n          imagePullPolicy: \"Always\"\n      volumes:\n        - name: ssl-certs\n          hostPath:\n            path: \"/etc/ssl/certs/ca-bundle.crt\"\n\n```\n\n### 现象\n```shell\n➜  cluster-autoscaler git:(master) k get pod -A | grep auto\nkube-system              cluster-autoscaler-6f5dcc568c-xsbgc                          1/1     Running     1          66d\n\n➜  k logs -f -n kube-system cluster-autoscaler-6f5dcc568c-xsbgc\n\nI1203 07:59:42.152656       1 static_autoscaler.go:194] Starting main loop\nI1203 07:59:42.153624       1 clusterstate.go:252] Scale up in group aux-eks-product-stateless-20200814145032009600000010 finished successfully in 2m10.67671515s\n...\nI1203 08:00:02.185930       1 static_autoscaler.go:194] Starting main loop\nI1203 08:00:02.337720       1 auto_scaling_groups.go:351] Regenerating instance to ASG map for ASGs: [aux-eks-auxiliary-stateful-2020081414503199620000000e aux-eks-auxiliary-stateless-2020081414503200200000000f aux-eks-product-stateful-2020081414503199420000000d aux-eks-product-stateless-20200814145032009600000010]\n...\n```\n\n### 注意\n- cluster-autoscaler 通过计算node 上pod的request/limit来分析资源是否存在不足，因此建议对pod进行明确的资源限制\n- node 扩容时尽量选择同配置的机型，或CPU/Memory相同的机型。如果需要使用不同机型可以针对资源进行编组，扩容时按需选择不同的组进行扩容。","tags":["k8s","cluster-autoscaler"],"categories":["k8s"]},{"title":"k8s descheduler","url":"//2020/12/01/k8s-descheduler/","content":"### 原理\n kube-scheduler通过各种算法计算出最佳节点去运行Pod,当出现新的 Pod 进行调度时，调度程序会根据其当时对 Kubernetes 集群的资源做出调度决定。但Kubernetes集群发生变化，比如一个节点为了维护，执行了驱逐操作，这个节点的 Pod 会被驱逐到其他节点，但是当我们维护完成Pod 不会自动回到该节点上来，由此 Kubernetes 集群出现了不均衡的状态，可以使用descheduler进行在均衡。\n - 项目地址：https://github.com/kubernetes-sigs/descheduler\n\n### install descheduler\n```yaml\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: descheduler-policy-configmap\n  namespace: kube-system\ndata:\n  policy.yaml: |\n    apiVersion: \"descheduler/v1alpha1\"\n    kind: \"DeschedulerPolicy\"\n    strategies:\n      \"RemoveDuplicates\":\n        enabled: true\n        params:\n          removeDuplicates:\n            excludeOwnerKinds:\n            - \"DaemonSet\"\n            - \"StatefulSet\"\n            namespaces:\n              include:\n              - uat\n              - uat2\n              - uat3\n              - aux\n              - preprod\n              - production\n              - staging\n              - qa\n      \"RemovePodsViolatingInterPodAntiAffinity\":\n        enabled: true\n      \"RemovePodsHavingTooManyRestarts\":\n        enabled: true\n        params:\n          podsHavingTooManyRestarts:\n            podRestartThreshold: 100\n            includingInitContainers: true\n      \"LowNodeUtilization\":\n        enabled: true\n        params:\n          nodeResourceUtilizationThresholds:\n            thresholds:\n              \"cpu\" : 20\n              \"memory\": 20\n              \"pods\": 20\n            targetThresholds:\n              \"cpu\" : 70\n              \"memory\": 70\n              \"pods\": 40\n      \"PodLifeTime\":\n        enabled: false\n        params:\n          podLifeTime:\n            maxPodLifeTimeSeconds: 86400\n            podStatusPhases:\n            - \"Pending\"\n---\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: descheduler-cronjob\n  namespace: kube-system\nspec:\n  schedule: \"*/2 * * * *\"\n  concurrencyPolicy: \"Forbid\"\n  successfulJobsHistoryLimit: 1\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          name: descheduler-pod\n        spec:\n          priorityClassName: system-cluster-critical\n          containers:\n            - name: descheduler\n              image: k8s.gcr.io/descheduler/descheduler:v0.19.0\n              volumeMounts:\n                - mountPath: /policy-dir\n                  name: policy-volume\n              command:\n                - \"/bin/descheduler\"\n              args:\n                - \"--policy-config-file\"\n                - \"/policy-dir/policy.yaml\"\n                - \"--v\"\n                - \"3\"\n          restartPolicy: \"Never\"\n          serviceAccountName: descheduler-sa\n          volumes:\n            - name: policy-volume\n              configMap:\n                name: descheduler-policy-configmap\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: descheduler-cluster-role\nrules:\n- apiGroups: [\"\"]\n  resources: [\"events\"]\n  verbs: [\"create\", \"update\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n- apiGroups: [\"\"]\n  resources: [\"namespaces\"]\n  verbs: [\"get\", \"list\"]\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"watch\", \"list\", \"delete\"]\n- apiGroups: [\"\"]\n  resources: [\"pods/eviction\"]\n  verbs: [\"create\"]\n- apiGroups: [\"scheduling.k8s.io\"]\n  resources: [\"priorityclasses\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: descheduler-sa\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: descheduler-cluster-role-binding\n  namespace: kube-system\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: descheduler-cluster-role\nsubjects:\n  - name: descheduler-sa\n    kind: ServiceAccount\n    namespace: kube-system\n```\n### 结果\n```shell\n➜  descheduler git:(master) k get cronjob  -A\nNAMESPACE     NAME                  SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE\nkube-system   descheduler-cronjob   */2 * * * *   True      0        3d2h            8d\n\n\n➜  descheduler git:(master) k get pod -A | grep desc\nkube-system   descheduler-cronjob-1606710120-np5xc               0/1     Completed   0          3d2h\n\n➜  descheduler git:(master) k logs -f descheduler-cronjob-1606710120-np5xc -n kube-system\nI1130 04:22:07.533753       1 node.go:45] node lister returned empty list, now fetch directly\nI1130 04:22:07.541476       1 toomanyrestarts.go:73] Processing node: cn-hongkong.10.0.3.20\nI1130 04:22:07.565399       1 toomanyrestarts.go:73] Processing node: cn-hongkong.10.0.3.21\nI1130 04:22:07.577738       1 duplicates.go:73] Processing node: \"cn-hongkong.10.0.3.20\"\nI1130 04:22:07.593279       1 duplicates.go:73] Processing node: \"cn-hongkong.10.0.3.21\"\nI1130 04:22:07.632923       1 lownodeutilization.go:203] Node \"cn-hongkong.10.0.3.20\" is appropriately utilized with usage: api.ResourceThresholds{\"cpu\":32.5, \"memory\":29.3980074559352, \"pods\":17.1875}\nI1130 04:22:07.632997       1 lownodeutilization.go:203] Node \"cn-hongkong.10.0.3.21\" is appropriately utilized with usage: api.ResourceThresholds{\"cpu\":30, \"memory\":23.303233323623655, \"pods\":15.625}\nI1130 04:22:07.633016       1 lownodeutilization.go:101] Criteria for a node under utilization: CPU: 20, Mem: 20, Pods: 20\nI1130 04:22:07.633028       1 lownodeutilization.go:105] No node is underutilized, nothing to do here, you might tune your thresholds further\nI1130 04:22:07.633042       1 pod_antiaffinity.go:72] Processing node: \"cn-hongkong.10.0.3.20\"\nI1130 04:22:07.647199       1 pod_antiaffinity.go:72] Processing node: \"cn-hongkong.10.0.3.21\"\n```","tags":["k8s","descheduler"],"categories":["k8s"]},{"title":"nginx-ingress下使用ldap 实现ingress auth认证","url":"//2020/11/21/nginx-ingress-ldap/","content":"### 原理\n使用nginx subrequest在请求直线通过 ingress annotation注入一条规则去调用auth接口完成ldap认证。参考文档：\n  - http://nginx.org/en/docs/http/ngx_http_auth_request_module.html\n  - https://www.nginx.com/blog/nginx-plus-authenticate-users\n  - https://docs.foxpass.com/docs/ldap-overview-debugging\n  - https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#external-authentication\n\n### 创建ldap认证服务\n\b```yaml\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: nginx-ldap-auth-config\ndata:\n  foxpass.conf: |\n    # define ldap server\n    ldap_server foxpass {\n        url \"ldaps://ldap.foxpass.com:636/dc=xiemx,dc=com?uid?sub?(objectClass=*)\";\n        binddn \"{dn信息}\"; # cn=test,dc=xiemx,dc=com\n        binddn_passwd \"xxxxxx\";\n        group_attribute groups;\n        group_attribute_is_dn on;\n        require valid_user;\n    }\n    \n    server {\n      listen 5555;\n\n      location / {\n        auth_ldap \"foxpass\";\n        auth_ldap_servers foxpass;\n\n        try_files index.html,index.htm @auth;\n      }\n\n      location @auth {\n        return 200 \"ldap auth\";\n      }\n    }\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: nginx-ldap-auth\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: Role\nmetadata:\n  name: nginx-ldap-auth\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - configmaps\n  resourceNames:\n  - \"nginx-ladp-auth-config\"\n  verbs:\n  - get\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: RoleBinding\nmetadata:\n  name: nginx-ldap-auth\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: nginx-ldap-auth\nsubjects:\n- kind: ServiceAccount\n  name: nginx-ldap-auth\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: nginx-ldap-auth\nspec:\n  type: ClusterIP\n  ports:\n  - name: nginx-ldap-auth\n    port: 5555\n    protocol: TCP\n    targetPort: 5555\n  selector:\n    app: nginx-ldap-auth\n---\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: nginx-ldap-auth\n  labels:\n    app: nginx-ldap-auth\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: \"nginx-ldap-auth\"\n  template:\n    metadata:\n      labels:\n        app: nginx-ldap-auth\n    spec:\n      serviceAccountName: nginx-ldap-auth\n      containers:\n      - image: weseek/nginx-auth-ldap:1.15.11-alpine\n        name: nginx-ldap-auth\n        ports:\n        - name: http\n          containerPort: 5555\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: config\n          mountPath: /etc/nginx/conf.d\n      volumes:\n      - name: config\n        configMap:\n          name: nginx-ldap-auth-config\n---\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n    kubernetes.io/ingress.class: nginx\n  labels:\n    app: nginx-ldap-auth\n  name: nginx-ldap-auth\nspec:\n  rules:\n  - host: foxpass.i.xiemx.com\n    http:\n      paths:\n      - backend:\n          serviceName: nginx-ldap-auth\n          servicePort: 5555\n        path: /\n\n```\n\n### ingress 开启auth认证\n\n```yaml\n\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n      nginx.ingress.kubernetes.io/auth-url: https://foxpass.i.xiemx.com\n  name: kibana\nspec:\n  rules:\n  - host: kibana.i.xiemx.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: kibana\n          servicePort: 5601\n\n### 测试\n➜  kibana git:(master) curl kibana.i.xiemx.com\n<html>\n<head><title>401 Authorization Required</title></head>\n<body>\n<center><h1>401 Authorization Required</h1></center>\n<hr><center>nginx/1.15.9</center>\n</body>\n</html>\n\n➜  kibana git:(master) curl kibana.i.xiemx.com --user mingxu.xie\nEnter host password for user 'mingxu.xie':\n```","tags":["k8s","ingress","nginx","ldap"],"categories":["k8s"]},{"title":"kong 测试代码","url":"//2020/11/03/kong-test-code/","content":"```yaml\n### docker-compose.yaml\n➜  kong git:(master) ✗ cat docker-compose.yaml\nversion: '2'\nservices:\n  kong:\n    image: kong:latest\n    environment:\n      - KONG_ADMIN_LISTEN=0.0.0.0:8001, 0.0.0.0:8444 ssl\n      - KONG_DATABASE=off\n      - KONG_DECLARATIVE_CONFIG=/usr/local/kong/declarative/kong.yml\n      - KONG_PROXY_ACCESS_LOG=/dev/stdout json\n      - KONG_ADMIN_ACCESS_LOG=/dev/stdout\n      - KONG_PROXY_ERROR_LOG=/dev/stderr\n      - KONG_ADMIN_ERROR_LOG=/dev/stderr\n    volumes:\n      - ./kong.yaml:/usr/local/kong/declarative/kong.yml\n      - ./nginx-template:/etc/kong/nginx-template.conf\n    ports:\n      - 8000:8000\n      - 8001:8001\n      - 8443:8443\n      - 8444:8444\n      - 8881:8881\n    command: [\"kong\", \"docker-start\", \"--nginx-conf\", \"/etc/kong/nginx-template.conf\"]\n\n###nginx template\n➜  kong git:(master) ✗ cat nginx-template\nworker_processes ${{NGINX_WORKER_PROCESSES}};\ndaemon ${{NGINX_DAEMON}};\n\npid pids/nginx.pid;\n\n\n\nerror_log logs/error.log ${{LOG_LEVEL}};\n\nevents {\n    use epoll;\n    multi_accept on;\n}\n\nhttp {\n    log_format json '{ \"timestamp\": \"$time_local\", '\n                        '\"server_name\": \"$host\",'\n                        '\"remote_addr\": \"$remote_addr\", '\n                        '\"ip\": \"$http_ip\", '\n                        '\"remote_user\": \"$remote_user\", '\n                        '\"body_bytes_sent\": \"$body_bytes_sent\", '\n                        '\"request_time\": \"$request_time\", '\n                        '\"status\": \"$status\", '\n                        '\"request\": \"$request\", '\n                        '\"request_method\": \"$request_method\", '\n                        '\"http_referrer\": \"$http_referer\", '\n                        '\"body_bytes_sent\":\"$body_bytes_sent\", '\n                        '\"http_x_forwarded_for\": \"$http_x_forwarded_for\", '\n                        '\"http_user_agent\": \"$http_user_agent\" }';\n\n    include \"nginx-kong.conf\";\n\n    server {\n        listen 8881;\n        location / {\n            return 200 \"1\";\n        }\n    }\n}\n\n\n### kong.yaml\n➜  kong git:(master) ✗ cat kong.yaml\n_format_version: \"1.1\"\n\nservices:\n- name: xiemx.com-service\n  retries: 3\n  connect_timeout: 60000\n  read_timeout: 60000\n  write_timeout: 60000\n  # url: http://xiemx.com\n  host: xiemx.com-upstream\n  # port: 80\n  protocol: https\n\n- name: TCPService\n  url: http://ifconfig.io\n\n- name: mockbin.org-service\n  url: http://mockbin.org\n\nroutes:\n- name: HTTPRoute\n  service: xiemx.com-service\n  # preserve_host: true\n  hosts: [\"xiemx.com\"]\n  methods: [\"GET\", \"POST\", \"HEAD\"]\n  # strip_path: true\n  headers: { \"version\": [\"v1\",\"v2\"]}\n  paths:\n    - /v1/\\d+\n    - /v2/\\d+/status/\\d+\n    - /v3\n    - /v4/any/\n    - /\n  regex_priority: 6\n\n- name: mockbin.org-route\n  service: mockbin.org-service\n  hosts: [\"mockbin.org\"]\n\n- name: TCPRoute\n  service: TCPService\n  protocols:\n    - tls\n    - tcp\n  sources:\n    - { ip: 1.1.1.1/32, port: 1234 }\n    - ip: 127.0.0.0/8\n  destinations:\n    - { ip: 2.2.2.2/32, port: 1234 }\n    - ip: 10.0.0.0/8\n\n\n\nplugins:\n- name: key-auth\n  service: mockbin.org-service\n  consumer: xiemx1\n\nconsumers:\n- username: xiemx1\n  keyauth-credentials:\n  - key: 1\n\n- username: xiemx2\n  keyauth-credentials:\n  - key: 2\n\nupstreams:\n# - name: mockbin.org-upstream\n#   targets:\n#     - target: mockbin.org\n\n- name: xiemx.com-upstream\n  targets:\n    - target: www.xiemx.com:443\n  algorithm: \"round-robin\"\n  # host_header: \"baidu.com\"  ### proxy_set_header host \"xiemx.com\"\n  hash_on: \"none\"\n  hash_fallback: \"none\"\n  hash_on_cookie_path: \"/\"\n  slots: 10001\n  healthchecks:\n    active:\n        https_verify_certificate: true\n        unhealthy:\n            http_statuses: [429, 404, 500, 501, 502, 503, 504, 505]\n            tcp_failures: 0\n            timeouts: 0\n            http_failures: 0\n            interval: 0\n        http_path: \"/\"\n        timeout: 1\n        healthy:\n            http_statuses: [200, 302]\n            interval: 0\n            successes: 0\n        # https_sni: \"example.com\"\n        concurrency: 10\n        type: \"http\"\n    passive:\n        unhealthy:\n            http_failures: 0\n            http_statuses: [429, 500, 503]\n            tcp_failures: 0\n            timeouts: 0\n        type: http\n        healthy:\n            successes: 0\n            http_statuses: [200, 201, 202, 203, 204, 205, 206, 207, 208, 226, 300, 301, 302, 303, 304, 305, 306, 307, 308]\n  tags:\n    - http\n    - xiemx\n```","tags":["docker","kong","nginx"],"categories":["kong"]},{"title":"kong ingress","url":"//2020/11/03/aws-alb-ingress/","content":"## Kong Ingress \n\n项目地址：https://github.com/Kong/kubernetes-ingress-controller\n\n![kong-architecture](/images/kong-architecture.jpg)\n\n结论：\n- kongIngress和kongPlugin 可以看成是对原始ingress进行增强的补丁（个人理解）\n- kong ingress 可以原则上无缝接管nginx ingress\n- kong ingress 可以实现规则灵活挂接，可允许在`SVC`、`INGRESS` 上接入规则，可以基于ingress实现host 级别 global规则，也可基于path分别对于不同的svc进行管理\n---\n\n#### install controller\n<details>\n<summary>ingress-controller.yaml</summary>\n\n```yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: kong\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: kongclusterplugins.configuration.konghq.com\nspec:\n  additionalPrinterColumns:\n  - JSONPath: .plugin\n    description: Name of the plugin\n    name: Plugin-Type\n    type: string\n  - JSONPath: .metadata.creationTimestamp\n    description: Age\n    name: Age\n    type: date\n  - JSONPath: .disabled\n    description: Indicates if the plugin is disabled\n    name: Disabled\n    priority: 1\n    type: boolean\n  - JSONPath: .config\n    description: Configuration of the plugin\n    name: Config\n    priority: 1\n    type: string\n  group: configuration.konghq.com\n  names:\n    kind: KongClusterPlugin\n    plural: kongclusterplugins\n    shortNames:\n    - kcp\n  scope: Cluster\n  validation:\n    openAPIV3Schema:\n      properties:\n        config:\n          type: object\n        configFrom:\n          properties:\n            secretKeyRef:\n              properties:\n                key:\n                  type: string\n                name:\n                  type: string\n                namespace:\n                  type: string\n              required:\n              - name\n              - namespace\n              - key\n              type: object\n          type: object\n        disabled:\n          type: boolean\n        plugin:\n          type: string\n        protocols:\n          items:\n            enum:\n            - http\n            - https\n            - grpc\n            - grpcs\n            - tcp\n            - tls\n            type: string\n          type: array\n        run_on:\n          enum:\n          - first\n          - second\n          - all\n          type: string\n      required:\n      - plugin\n  version: v1\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: kongconsumers.configuration.konghq.com\nspec:\n  additionalPrinterColumns:\n  - JSONPath: .username\n    description: Username of a Kong Consumer\n    name: Username\n    type: string\n  - JSONPath: .metadata.creationTimestamp\n    description: Age\n    name: Age\n    type: date\n  group: configuration.konghq.com\n  names:\n    kind: KongConsumer\n    plural: kongconsumers\n    shortNames:\n    - kc\n  scope: Namespaced\n  validation:\n    openAPIV3Schema:\n      properties:\n        credentials:\n          items:\n            type: string\n          type: array\n        custom_id:\n          type: string\n        username:\n          type: string\n  version: v1\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: kongcredentials.configuration.konghq.com\nspec:\n  additionalPrinterColumns:\n  - JSONPath: .type\n    description: Type of credential\n    name: Credential-type\n    type: string\n  - JSONPath: .metadata.creationTimestamp\n    description: Age\n    name: Age\n    type: date\n  - JSONPath: .consumerRef\n    description: Owner of the credential\n    name: Consumer-Ref\n    type: string\n  group: configuration.konghq.com\n  names:\n    kind: KongCredential\n    plural: kongcredentials\n  scope: Namespaced\n  validation:\n    openAPIV3Schema:\n      properties:\n        consumerRef:\n          type: string\n        type:\n          type: string\n      required:\n      - consumerRef\n      - type\n  version: v1\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: kongingresses.configuration.konghq.com\nspec:\n  group: configuration.konghq.com\n  names:\n    kind: KongIngress\n    plural: kongingresses\n    shortNames:\n    - ki\n  scope: Namespaced\n  validation:\n    openAPIV3Schema:\n      properties:\n        proxy:\n          properties:\n            connect_timeout:\n              minimum: 0\n              type: integer\n            path:\n              pattern: ^/.*$\n              type: string\n            protocol:\n              enum:\n              - http\n              - https\n              - grpc\n              - grpcs\n              - tcp\n              - tls\n              type: string\n            read_timeout:\n              minimum: 0\n              type: integer\n            retries:\n              minimum: 0\n              type: integer\n            write_timeout:\n              minimum: 0\n              type: integer\n          type: object\n        route:\n          properties:\n            headers:\n              additionalProperties:\n                items:\n                  type: string\n                type: array\n              type: object\n            https_redirect_status_code:\n              type: integer\n            methods:\n              items:\n                type: string\n              type: array\n            path_handling:\n              enum:\n              - v0\n              - v1\n              type: string\n            preserve_host:\n              type: boolean\n            protocols:\n              items:\n                enum:\n                - http\n                - https\n                - grpc\n                - grpcs\n                - tcp\n                - tls\n                type: string\n              type: array\n            regex_priority:\n              type: integer\n            strip_path:\n              type: boolean\n        upstream:\n          properties:\n            algorithm:\n              enum:\n              - round-robin\n              - consistent-hashing\n              - least-connections\n              type: string\n            hash_fallback:\n              type: string\n            hash_fallback_header:\n              type: string\n            hash_on:\n              type: string\n            hash_on_cookie:\n              type: string\n            hash_on_cookie_path:\n              type: string\n            hash_on_header:\n              type: string\n            healthchecks:\n              properties:\n                active:\n                  properties:\n                    concurrency:\n                      minimum: 1\n                      type: integer\n                    healthy:\n                      properties:\n                        http_statuses:\n                          items:\n                            type: integer\n                          type: array\n                        interval:\n                          minimum: 0\n                          type: integer\n                        successes:\n                          minimum: 0\n                          type: integer\n                      type: object\n                    http_path:\n                      pattern: ^/.*$\n                      type: string\n                    timeout:\n                      minimum: 0\n                      type: integer\n                    unhealthy:\n                      properties:\n                        http_failures:\n                          minimum: 0\n                          type: integer\n                        http_statuses:\n                          items:\n                            type: integer\n                          type: array\n                        interval:\n                          minimum: 0\n                          type: integer\n                        tcp_failures:\n                          minimum: 0\n                          type: integer\n                        timeout:\n                          minimum: 0\n                          type: integer\n                      type: object\n                  type: object\n                passive:\n                  properties:\n                    healthy:\n                      properties:\n                        http_statuses:\n                          items:\n                            type: integer\n                          type: array\n                        interval:\n                          minimum: 0\n                          type: integer\n                        successes:\n                          minimum: 0\n                          type: integer\n                      type: object\n                    unhealthy:\n                      properties:\n                        http_failures:\n                          minimum: 0\n                          type: integer\n                        http_statuses:\n                          items:\n                            type: integer\n                          type: array\n                        interval:\n                          minimum: 0\n                          type: integer\n                        tcp_failures:\n                          minimum: 0\n                          type: integer\n                        timeout:\n                          minimum: 0\n                          type: integer\n                      type: object\n                  type: object\n                threshold:\n                  type: integer\n              type: object\n            host_header:\n              type: string\n            slots:\n              minimum: 10\n              type: integer\n          type: object\n  version: v1\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: kongplugins.configuration.konghq.com\nspec:\n  additionalPrinterColumns:\n  - JSONPath: .plugin\n    description: Name of the plugin\n    name: Plugin-Type\n    type: string\n  - JSONPath: .metadata.creationTimestamp\n    description: Age\n    name: Age\n    type: date\n  - JSONPath: .disabled\n    description: Indicates if the plugin is disabled\n    name: Disabled\n    priority: 1\n    type: boolean\n  - JSONPath: .config\n    description: Configuration of the plugin\n    name: Config\n    priority: 1\n    type: string\n  group: configuration.konghq.com\n  names:\n    kind: KongPlugin\n    plural: kongplugins\n    shortNames:\n    - kp\n  scope: Namespaced\n  validation:\n    openAPIV3Schema:\n      properties:\n        config:\n          type: object\n        configFrom:\n          properties:\n            secretKeyRef:\n              properties:\n                key:\n                  type: string\n                name:\n                  type: string\n              required:\n              - name\n              - key\n              type: object\n          type: object\n        disabled:\n          type: boolean\n        plugin:\n          type: string\n        protocols:\n          items:\n            enum:\n            - http\n            - https\n            - grpc\n            - grpcs\n            - tcp\n            - tls\n            type: string\n          type: array\n        run_on:\n          enum:\n          - first\n          - second\n          - all\n          type: string\n      required:\n      - plugin\n  version: v1\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: tcpingresses.configuration.konghq.com\nspec:\n  additionalPrinterColumns:\n  - JSONPath: .status.loadBalancer.ingress[*].ip\n    description: Address of the load balancer\n    name: Address\n    type: string\n  - JSONPath: .metadata.creationTimestamp\n    description: Age\n    name: Age\n    type: date\n  group: configuration.konghq.com\n  names:\n    kind: TCPIngress\n    plural: tcpingresses\n  scope: Namespaced\n  subresources:\n    status: {}\n  validation:\n    openAPIV3Schema:\n      properties:\n        apiVersion:\n          type: string\n        kind:\n          type: string\n        metadata:\n          type: object\n        spec:\n          properties:\n            rules:\n              items:\n                properties:\n                  backend:\n                    properties:\n                      serviceName:\n                        type: string\n                      servicePort:\n                        format: int32\n                        type: integer\n                    type: object\n                  host:\n                    type: string\n                  port:\n                    format: int32\n                    type: integer\n                type: object\n              type: array\n            tls:\n              items:\n                properties:\n                  hosts:\n                    items:\n                      type: string\n                    type: array\n                  secretName:\n                    type: string\n                type: object\n              type: array\n          type: object\n        status:\n          type: object\n  version: v1beta1\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: kong-serviceaccount\n  namespace: kong\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRole\nmetadata:\n  name: kong-ingress-clusterrole\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - endpoints\n  - nodes\n  - pods\n  - secrets\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes\n  verbs:\n  - get\n- apiGroups:\n  - \"\"\n  resources:\n  - services\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - networking.k8s.io\n  - extensions\n  - networking.internal.knative.dev\n  resources:\n  - ingresses\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - \"\"\n  resources:\n  - events\n  verbs:\n  - create\n  - patch\n- apiGroups:\n  - networking.k8s.io\n  - extensions\n  - networking.internal.knative.dev\n  resources:\n  - ingresses/status\n  verbs:\n  - update\n- apiGroups:\n  - configuration.konghq.com\n  resources:\n  - tcpingresses/status\n  verbs:\n  - update\n- apiGroups:\n  - configuration.konghq.com\n  resources:\n  - kongplugins\n  - kongclusterplugins\n  - kongcredentials\n  - kongconsumers\n  - kongingresses\n  - tcpingresses\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - \"\"\n  resources:\n  - configmaps\n  verbs:\n  - create\n  - get\n  - update\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: kong-ingress-clusterrole-nisa-binding\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: kong-ingress-clusterrole\nsubjects:\n- kind: ServiceAccount\n  name: kong-serviceaccount\n  namespace: kong\n---\napiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    #    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp\n    #    service.beta.kubernetes.io/aws-load-balancer-type: nlb\n  name: kong-proxy\n  namespace: kong\nspec:\n  ports:\n  - name: proxy\n    port: 80\n    protocol: TCP\n    targetPort: 8000\n  - name: proxy-ssl\n    port: 443\n    protocol: TCP\n    targetPort: 8443\n  selector:\n    app: ingress-kong\n  type: LoadBalancer\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kong-validation-webhook\n  namespace: kong\nspec:\n  ports:\n  - name: webhook\n    port: 443\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    app: ingress-kong\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: ingress-kong\n  name: ingress-kong\n  namespace: kong\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ingress-kong\n  template:\n    metadata:\n      annotations:\n        kuma.io/gateway: enabled\n        prometheus.io/port: \"8100\"\n        prometheus.io/scrape: \"true\"\n        traffic.sidecar.istio.io/includeInboundPorts: \"\"\n      labels:\n        app: ingress-kong\n    spec:\n      containers:\n      - env:\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8000, 0.0.0.0:8443 ssl http2\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 ssl\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100\n        - name: KONG_DATABASE\n          value: \"off\"\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: \"1\"\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        image: kong:2.0\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - kong quit\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status\n            port: 8100\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: proxy\n        ports:\n        - containerPort: 8000\n          name: proxy\n          protocol: TCP\n        - containerPort: 8443\n          name: proxy-ssl\n          protocol: TCP\n        - containerPort: 8100\n          name: metrics\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status\n            port: 8100\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        securityContext:\n          runAsUser: 1000\n      - env:\n        - name: CONTROLLER_KONG_ADMIN_URL\n          value: https://127.0.0.1:8444\n        - name: CONTROLLER_KONG_ADMIN_TLS_SKIP_VERIFY\n          value: \"true\"\n        - name: CONTROLLER_PUBLISH_SERVICE\n          value: kong/kong-proxy\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        image: kong-docker-kubernetes-ingress-controller.bintray.io/kong-ingress-controller:0.9.1\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: ingress-controller\n        ports:\n        - containerPort: 8080\n          name: webhook\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      serviceAccountName: kong-serviceaccount\n```\n</details>\n\n#### 部署测试服务\n<details>\n<summary>echo.yaml</summary>\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: echo\n  name: echo-v1\nspec:\n  ports:\n  - port: 8080\n    name: high\n    protocol: TCP\n    targetPort: 8080\n  - port: 80\n    name: low\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    app: echo\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: echo\n  name: echo-v2\nspec:\n  ports:\n  - port: 8080\n    name: high\n    protocol: TCP\n    targetPort: 8080\n  - port: 80\n    name: low\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    app: echo\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: echo\n  name: echo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: echo\n  strategy: {}\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: echo\n    spec:\n      containers:\n      - image: gcr.io/kubernetes-e2e-test-images/echoserver:2.2\n        name: echo\n        ports:\n        - containerPort: 8080\n        env:\n          - name: NODE_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: spec.nodeName\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: POD_IP\n            valueFrom:\n              fieldRef:\n                fieldPath: status.podIP\n        resources: {}\n---\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: demo\nspec:\n  rules:\n  - http:\n      paths:\n      - path: /v1\n        backend:\n          serviceName: echo-v1\n          servicePort: 80\n      - path: /v2\n        backend:\n          serviceName: echo-v2\n          servicePort: 80\n```\n</details>\n\n<details>\n<summary>KongPlugins.yaml</summary>\n\n```yaml\n---\napiVersion: configuration.konghq.com/v1\nkind: KongPlugin\nmetadata:\n  name: auth\n  namespace: kong\nplugin: key-auth\n---\napiVersion: configuration.konghq.com/v1\nkind: KongPlugin\nmetadata:\n  labels:\n    global: 'true'\n  name: global-rate-limit\n  namespace: kong\nconfig:\n  limit_by: consumer\n  minute: 5\n  policy: local\nplugin: rate-limiting\n---\napiVersion: configuration.konghq.com/v1\nkind: KongPlugin\nmetadata:\n  name: rate-limit\n  namespace: kong\nconfig:\n  limit_by: consumer\n  minute: 10\n  policy: local\nplugin: rate-limiting\n---\napiVersion: configuration.konghq.com/v1\nplugin: correlation-id\nconfig:\n  header_name: X-Request-ID\nkind: KongPlugin\nmetadata:\n  name: request-id\n  namespace: kong\n---\napiVersion: configuration.konghq.com/v1\nkind: KongPlugin\nmetadata:\n  name: response-header\n  namespace: kong\nconfig:\n  add:\n    headers:\n    - 'x-cust-resp-header: xiemx'\nplugin: response-transformer\n```\n\n</details>\n<details>\n<summary>KongIngress.yaml</summary>\n\n```yaml\napiVersion: configuration.konghq.com/v1\nkind: KongIngress\nmetadata:\n  name: custom-ingress\nroute:\n  methods:\n  - GET\n    #strip_path: true\n---\napiVersion: configuration.konghq.com/v1\nkind: KongIngress\nmetadata:\n  name: custom-svc\nupstream:\n  hash_on: ip\nproxy:\n  path: /foo/\n```\n</details>\n\n\n#### 测试\n本测试只使用少数几个plugin测试kong的ingress、svc规则生效，具体plugin可以另行测试，已附上对应具体yaml\n```shell\n➜  kong git:(master) ✗ kga\nNAME                               READY   STATUS    RESTARTS   AGE\npod/echo-85fb7989cc-pmjq7          1/1     Running   0          2d14h\npod/ingress-kong-d7b5d68f4-8v9tk   2/2     Running   0          2d14h\n\nNAME                              TYPE           CLUSTER-IP       EXTERNAL-IP                                                                    PORT(S)                      AGE\nservice/echo-v1                   ClusterIP      172.20.76.165    <none>                                                                         8080/TCP,80/TCP              2d21h\nservice/echo-v2                   ClusterIP      172.20.198.164   <none>                                                                         8080/TCP,80/TCP              2d21h\nservice/kong-proxy                LoadBalancer   172.20.113.185   a7a642206e35c414d8c7be4330144b8f-1688272697.ap-northeast-1.elb.amazonaws.com   80:31957/TCP,443:30218/TCP   2d21h\nservice/kong-validation-webhook   ClusterIP      172.20.51.128    <none>                                                                         443/TCP                      2d21h\n\nNAME                           READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/echo           1/1     1            1           2d21h\ndeployment.apps/ingress-kong   1/1     1            1           2d21h\n\nNAME                                     DESIRED   CURRENT   READY   AGE\nreplicaset.apps/echo-85fb7989cc          1         1         1       2d21h\nreplicaset.apps/ingress-kong-d7b5d68f4   1         1         1       2d21h\n\n➜  kong git:(master) ✗ k get kongingress\nNAME             AGE\ncustom-ingress   2d21h\ncustom-svc       2d21h\n\n➜  kong git:(master) ✗ k get kongPlugin\nNAME              PLUGIN-TYPE            AGE\nrate-limit        rate-limiting          2d21h\nrequest-id        correlation-id         2d21h\nresponse-header   response-transformer   2d21h\n```\n\n- 测试ingress 规则注入\n```shell\n### 无注入访问/v1 和 /v2\n➜  kong git:(master) ✗ curl a7a642206e35c414d8c7be4330144b8f-1688272697.ap-northeast-1.elb.amazonaws.com/v2 -X POST -I\nHTTP/1.1 200 OK\nContent-Type: text/plain; charset=UTF-8\nTransfer-Encoding: chunked\nConnection: keep-alive\nDate: Mon, 07 Dec 2020 03:53:09 GMT\nServer: echoserver\nX-Kong-Upstream-Latency: 0\nX-Kong-Proxy-Latency: 0\nVia: kong/2.0.5\n\n➜  kong git:(master) ✗ curl a7a642206e35c414d8c7be4330144b8f-1688272697.ap-northeast-1.elb.amazonaws.com/v1 -X POST -I\nHTTP/1.1 200 OK\nContent-Type: text/plain; charset=UTF-8\nTransfer-Encoding: chunked\nConnection: keep-alive\nDate: Mon, 07 Dec 2020 03:53:16 GMT\nServer: echoserver\nX-Kong-Upstream-Latency: 0\nX-Kong-Proxy-Latency: 0\nVia: kong/2.0.5\n\n### 注入ingress，限制请求方法为GET\n➜  kong git:(master) ✗ cat kongIngress/ingress.yaml\napiVersion: configuration.konghq.com/v1\nkind: KongIngress\nmetadata:\n  name: custom-ingress\nroute:\n  methods:\n  - GET\n\n➜  kong git:(master) ✗ k describe ingress demo\nName:             demo\nNamespace:        kong\nAddress:          a7a642206e35c414d8c7be4330144b8f-1688272697.ap-northeast-1.elb.amazonaws.com\nDefault backend:  default-http-backend:80 (<none>)\nRules:\n  Host  Path  Backends\n  ----  ----  --------\n  *\n        /v1   echo-v1:80 (10.200.1.88:8080)\n        /v2   echo-v2:80 (10.200.1.88:8080)\nAnnotations:\n  konghq.com/override:                               custom-ingress\n\n\n➜  kong git:(master) ✗ curl a7a642206e35c414d8c7be4330144b8f-1688272697.ap-northeast-1.elb.amazonaws.com/v1 -X POST -I\nHTTP/1.1 404 Not Found\nDate: Mon, 07 Dec 2020 03:56:31 GMT\nContent-Type: application/json; charset=utf-8\nConnection: keep-alive\nContent-Length: 48\nX-Kong-Response-Latency: 1\nServer: kong/2.0.5\n\n➜  kong git:(master) ✗ curl a7a642206e35c414d8c7be4330144b8f-1688272697.ap-northeast-1.elb.amazonaws.com/v2 -X POST -I\nHTTP/1.1 404 Not Found\nDate: Mon, 07 Dec 2020 03:56:37 GMT\nContent-Type: application/json; charset=utf-8\nConnection: keep-alive\nContent-Length: 48\nX-Kong-Response-Latency: 0\nServer: kong/2.0.5\n\n➜  kong git:(master) ✗ curl a7a642206e35c414d8c7be4330144b8f-1688272697.ap-northeast-1.elb.amazonaws.com/v2 -X GET -I\nHTTP/1.1 200 OK\nContent-Type: text/plain; charset=UTF-8\nTransfer-Encoding: chunked\nConnection: keep-alive\nDate: Mon, 07 Dec 2020 03:56:43 GMT\nServer: echoserver\nX-Kong-Upstream-Latency: 1\nX-Kong-Proxy-Latency: 0\nVia: kong/2.0.5\n\n➜  kong git:(master) ✗ curl a7a642206e35c414d8c7be4330144b8f-1688272697.ap-northeast-1.elb.amazonaws.com/v1 -X GET -I\nHTTP/1.1 200 OK\nContent-Type: text/plain; charset=UTF-8\nTransfer-Encoding: chunked\nConnection: keep-alive\nDate: Mon, 07 Dec 2020 03:56:48 GMT\nServer: echoserver\nX-Kong-Upstream-Latency: 0\nX-Kong-Proxy-Latency: 0\nVia: kong/2.0.5\n```\n- 测试svc 规则注入\n```shell\n➜  kong git:(master) ✗ cat kongIngress/ingress.yaml\napiVersion: configuration.konghq.com/v1\nkind: KongIngress\nmetadata:\n  name: custom-svc\nproxy:\n  path: /foo/\n\n➜  kong git:(master) ✗ k describe svc echo-v1 echo-v2\nName:              echo-v1\nNamespace:         kong\nLabels:            app=echo\nAnnotations:       konghq.com/override: custom-svc\n                   kubectl.kubernetes.io/last-applied-configuration:\n                     {\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{\"configuration.konghq.com\":\"custom-svc\"},\"labels\":{\"app\":\"echo\"},\"name\":\"ec...\nSelector:          app=echo\nType:              ClusterIP\nIP:                172.20.76.165\nPort:              high  8080/TCP\nTargetPort:        8080/TCP\nEndpoints:         10.200.1.88:8080\nPort:              low  80/TCP\nTargetPort:        8080/TCP\nEndpoints:         10.200.1.88:8080\nSession Affinity:  None\nEvents:            <none>\n\n\nName:              echo-v2\nNamespace:         kong\nLabels:            app=echo\nAnnotations:       kubectl.kubernetes.io/last-applied-configuration:\n                     {\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"app\":\"echo\"},\"name\":\"echo-v2\",\"namespace\":\"kong\"},\"spec\":{\"por...\nSelector:          app=echo\nType:              ClusterIP\nIP:                172.20.198.164\nPort:              high  8080/TCP\nTargetPort:        8080/TCP\nEndpoints:         10.200.1.88:8080\nPort:              low  80/TCP\nTargetPort:        8080/TCP\nEndpoints:         10.200.1.88:8080\nSession Affinity:  None\nEvents:            <none>\n\n\n\n### 测试\n➜  kong git:(master) ✗ curl a7a642206e35c414d8c7be4330144b8f-1688272697.ap-northeast-1.elb.amazonaws.com/v1 -X GET\n\n\nHostname: echo-85fb7989cc-pmjq7\n\nPod Information:\n\tnode name:\tip-10-200-1-155.ap-northeast-1.compute.internal\n\tpod name:\techo-85fb7989cc-pmjq7\n\tpod namespace:\tkong\n\tpod IP:\t10.200.1.88\n\nServer values:\n\tserver_version=nginx: 1.12.2 - lua: 10010\n\nRequest Information:\n\tclient_address=10.200.1.189\n\tmethod=GET\n\treal path=/foo/v1\n\tquery=\n\trequest_version=1.1\n\trequest_scheme=http\n\trequest_uri=http://a7a642206e35c414d8c7be4330144b8f-1688272697.ap-northeast-1.elb.amazonaws.com:8080/foo/v1\n\nRequest Headers:\n\taccept=*/*\n\tconnection=keep-alive\n\thost=a7a642206e35c414d8c7be4330144b8f-1688272697.ap-northeast-1.elb.amazonaws.com\n\tuser-agent=curl/7.54.0\n\tx-forwarded-for=10.200.2.91\n\tx-forwarded-host=a7a642206e35c414d8c7be4330144b8f-1688272697.ap-northeast-1.elb.amazonaws.com\n\tx-forwarded-port=8000\n\tx-forwarded-proto=http\n\tx-real-ip=10.200.2.91\n\nRequest Body:\n\t-no body in request-\n\n➜  kong git:(master) ✗ curl a7a642206e35c414d8c7be4330144b8f-1688272697.ap-northeast-1.elb.amazonaws.com/v2 -X GET\n\n\nHostname: echo-85fb7989cc-pmjq7\n\nPod Information:\n\tnode name:\tip-10-200-1-155.ap-northeast-1.compute.internal\n\tpod name:\techo-85fb7989cc-pmjq7\n\tpod namespace:\tkong\n\tpod IP:\t10.200.1.88\n\nServer values:\n\tserver_version=nginx: 1.12.2 - lua: 10010\n\nRequest Information:\n\tclient_address=10.200.1.189\n\tmethod=GET\n\treal path=/v2\n\tquery=\n\trequest_version=1.1\n\trequest_scheme=http\n\trequest_uri=http://a7a642206e35c414d8c7be4330144b8f-1688272697.ap-northeast-1.elb.amazonaws.com:8080/v2\n\nRequest Headers:\n\taccept=*/*\n\tconnection=keep-alive\n\thost=a7a642206e35c414d8c7be4330144b8f-1688272697.ap-northeast-1.elb.amazonaws.com\n\tuser-agent=curl/7.54.0\n\tx-forwarded-for=10.200.1.234\n\tx-forwarded-host=a7a642206e35c414d8c7be4330144b8f-1688272697.ap-northeast-1.elb.amazonaws.com\n\tx-forwarded-port=8000\n\tx-forwarded-proto=http\n\tx-real-ip=10.200.1.234\n\nRequest Body:\n\t-no body in request-\n\n```","tags":["k8s","ingress","kong"],"categories":["kong","ingress","k8s"]},{"title":"kong 学习随记","url":"//2020/11/02/kong-notebook/","content":"#### 模式\n* db\n* dbless\n---\n#### install\n\n```shell\ndocker run -d --name kong \\\n     -v \"kong.yaml:/usr/local/kong/declarative\" \\\n     -e \"KONG_DATABASE=off\" \\\n     -e \"KONG_DECLARATIVE_CONFIG=/usr/local/kong/declarative/kong.yml\" \\\n     -e \"KONG_PROXY_ACCESS_LOG=/dev/stdout\" \\\n     -e \"KONG_ADMIN_ACCESS_LOG=/dev/stdout\" \\\n     -e \"KONG_PROXY_ERROR_LOG=/dev/stderr\" \\\n     -e \"KONG_ADMIN_ERROR_LOG=/dev/stderr\" \\\n     -e \"KONG_ADMIN_LISTEN=0.0.0.0:8001, 0.0.0.0:8444 ssl\" \\\n     -p 8000:8000 \\\n     -p 8443:8443 \\\n     -p 8001:8001 \\\n     -p 8444:8444 \\\n     kong:latest\n```\n\n* plugins --> module\n* comsumer --> 消费者目前只看到用作auth认证用\n* services --> upstream\n* routes --> server\n\n---\n#### debug 模式\n```shell\ncurl -H \"X-Kong-Debug: 1\" http://localhost:8000\n```\n\n#### 注入环境变量\n\n从配置文件中加载属性时，Kong还将寻找同名的环境变量。声明前缀`KONG_`并大写可以覆盖默认配置。\nexample:\n```\n$ export KONG_DATABASE=off\n使用dbless 模式启动kong\n```\n#### 注入nginx变量\n\n前缀为的条目`nginx_http_`将注入到整体http 块指令中。\n\n前缀为的条目`nginx_proxy_`将注入到server处理Kong代理端口的block指令中。\n\n前缀为的条目`nginx_admin_`将注入到server处理Kong的Admin API端口的block指令中。\n\nexample：\n```\n$ export KONG_NGINX_HTTP_OUTPUT_BUFFERS=\"4 64k\"\n\n将会在nginx的http 代码段中加入一条 \noutput_buffers=\"4 64k\"\n```\n---\n\n#### HOST传递\n`preserve_host: true` 将会传递request原始的host，相当于`proxy_set_header host $host`\n\nexpamle:\n```\nservices:\n- name: test\n  preserve_host: true\n  url: http://mockbin.org\n\n```\n---\n#### request header区分路由\n\n注意：headers键是逻辑键AND，其值是逻辑键OR。\n\n```\nroutes:\n- name: test\n  service: test\n  hosts: [\"x.t.xiemx.com\"]\n  headers: { \"version\": [\"v1\",\"v2\"]}\n\n\n➜  Documents curl localhost:8000 -H version:v1 -H host:x.t.xiemx.com -I\nHTTP/1.1 200 OK\nContent-Type: text/html; charset=utf-8\nContent-Length: 10695\nConnection: keep-alive\nServer: Cowboy\nEtag: W/\"29c7-XG+PICJmz/J+UYWt5gkKqqAUXjc\"\nVary: Accept-Encoding\nDate: Fri, 03 Jan 2020 03:19:08 GMT\nVia: kong/1.4.2\nX-Kong-Upstream-Status: 200\nX-Kong-Upstream-Latency: 517\nX-Kong-Proxy-Latency: 34\nKong-Cloud-Request-ID: eeb277b4d0df3eb242e92dc0c669f8bc\n\n➜  Documents curl localhost:8000 -H version:v2 -H host:x.t.xiemx.com -I\nHTTP/1.1 200 OK\nContent-Type: text/html; charset=utf-8\nContent-Length: 10695\nConnection: keep-alive\nServer: Cowboy\nEtag: W/\"29c7-XG+PICJmz/J+UYWt5gkKqqAUXjc\"\nVary: Accept-Encoding\nDate: Fri, 03 Jan 2020 03:19:17 GMT\nVia: kong/1.4.2\nX-Kong-Upstream-Status: 200\nX-Kong-Upstream-Latency: 566\nX-Kong-Proxy-Latency: 2\nKong-Cloud-Request-ID: f3eaf8e27f3d3f9510f1c0c4619df035\n\n➜  Documents curl localhost:8000 -H version:v3 -H host:x.t.xiemx.com -I\nHTTP/1.1 404 Not Found\nDate: Fri, 03 Jan 2020 03:19:23 GMT\nContent-Type: application/json; charset=utf-8\nConnection: keep-alive\nContent-Length: 48\nX-Kong-Response-Latency: 0\nServer: kong/1.4.2\n\n```\n\n---\n\n#### path优先级\n\n* 用路径前缀代理时，最长的路径将首先被评估\n* regex_priority 优先级\n\n#### src/dest ip白名单\nsources/destinations路由属性仅适用于TCP和TLS路线。它允许通过传入连接IP、网段或端口源的列表来匹配路由\n```\n  service: TCPService\n  protocols: \n    - tls\n    - tcp\n  sources: \n    - { ip: 1.1.1.1/32, port: 1234 }\n    - ip: 127.0.0.0/8\n  destinations:\n    - { ip: 2.2.2.2/32, port: 1234 }\n    - ip: 10.0.0.0/8\n```\n\n\n#### strip_path 转发时删除路径\n```\n{\n    \"paths\": [\"/service\"],\n    \"strip_path\": true,\n}\n\nrequest_url -> /service/1/\nproxy_url -> /1/\n```\n---\n### Kong proxy时默认处理的HEADER\n* Host: <your_upstream_host>\n* Connection: keep-alive，以允许重用上游连接。\n* X-Real-IP: <remote_addr>，其中`$remote_addr`变量的名称与ngx_http_core_module提供的名称相同。`$remote_addr`可能被ngx_http_realip_module覆盖。\n* X-Forwarded-For: <ip>, 访问路径。\n* X-Forwarded-Proto: <protocol>，客户端使用的协议。使用ngx_http_core_module `$scheme`提供的变量的值。\n* X-Forwarded-Host: <host>，客户端发送的主机名。\n* X-Forwarded-Port: <port>，服务器的端口。使用ngx_http_core_module `$server_port`变量的值。\n\n---\n### Kong Response headers\nnginx 接收到upstream 的响应只会执行header-filter阶段将下面的header封装进header，后续会在执行body—filter阶段\n\n* Via: kong/x.x.x，x.x.x使用的Kong版本在哪里\n* X-Kong-Proxy-Latency: <latency>，Kong从客户端接收请求到将请求发送到上游服务之间的时间（以毫秒为单位）。\n* X-Kong-Upstream-Latency: <latency>，Kong等待上游服务响应的第一个字节的时间（以毫秒为单位）。\n\n\n---\n\n### 负载均衡\n- #### DNS负载均衡(ttl=0)\n  * SRV record\n  * A record\n  * CNAME record\n\n  **优先级：**\n  - LAST(最后一次成功的类型) -> SRV -> A -> CNAME\n  - 可以在Kong的配置文件中修改,默认`dns_order:LAST,SRV,A,CNAME`\n\n  **问题：**\n  - DNS可能不会返回所有的记录，导致下游服务器负载不均衡\n\n- #### Ring-balancer\n  * upstream\n    * service中host=\"upstream_name\"使用的名字，使用方法同nginx类似\n    * upstream 需要预先分配一个slots，target会根据weight分配到slots上，默认upstream的slots=10000，范围（1 - 65536）。\n    * 插槽数越多，随机分布越好，但是更改的开销就越大（添加/删除目标）\n    * 当插槽数量更改时，将需要重建\n    * which can be overwritten before proxying, using upstream’s property `host_header`\n\n  * target\n    * 实际upstream中使用的主机地址，等同于nginx upstream中的server指令，支持定义`weight`\n    * 当target使用域名时，会将域名进行解析，然后将每条记录都设置到slots上，切遵循DNS TTL过期时间，去重复查询\n    * 如果存在SRV记录，会优先使用SRV 记录的weight/port\n\n- #### 负载均衡算法\n  * HASH（如下hash key）\n    * none 不使用哈希，而采用weighted-round-robin（默认值）。\n    * cookie\n    * ip\n    * header\n    * consumer\n\n  * WRR(weighted-round-robin)\n  \n  **PS**: upstream 中的target 如未指定port则使用requst 请求中的port 不会使用，service中定义的port\n\n---\n### 健康检测\n\n  * active\n    主动发出http/https request轮询target\n\n  * passive\n    分析backend response的状态码，不需要指定URL等参数\n\n  PS: interval=0时禁止，healthcheck可以同时开启被动检测，已实现断路功能，并开启主动检测中的unhealthcheck实现target故障自愈后自动加入\n\n---\n### kong cluster \n  * 集群之间的节点流量不会自动负载均衡，需要在node前面有一个负载均衡器\n  * 由于性能的原因，node不会在proxy流量时去链接PG，而是将数据cache在内存中，多node集群中需要保证节点的数据同步更新\n\n    * db_update_frequency: 5 #node节点去db中同步数据的间隔，数据刷新间隔\n    * db_update_propagation: 0 #如果后端是分布式数据库的话，则需要配置这个值。此值应该等于分布式数据库内部传播数据最终达到数据一致的时间。如果设置这个值，数据库更新时间将会为`db_update_frequency + db_update_propagation`，并且设置的节点也会等待一个propagation时间再去update cache\n    * db_cache_ttl: 0 #防止invalid event事件node没有接收到而错过更新，可以设置一个主动过期时间\n","tags":["kong"],"categories":["kong"]},{"title":"aws alb ingress controller","url":"//2020/10/03/kong-ingress/","content":"## AWS-alb-ingress-controller\n项目：https://github.com/kubernetes-sigs/aws-load-balancer-controller\n\n![aws-alb-ingress-controller](/images/aws-alb-ingress-controller.jpg)\n\n  - **步骤1**: controller 监听api-services的event事件,当发现 Ingress 资源满足要求，则将开始创建 AWS 资源。\n  - **步骤2**: 为 Ingress 资源创建 ALB\n  - **步骤3**: 为 Ingress 资源中指定的每个后端创建目标组\n  - **步骤4**: 为 Ingress 资源注释中指定的每个端口创建侦听器\n  - **步骤5**: 为 Ingress 资源中指定的每个路径创建规则\n#### **注意**\n- ingress-controller 需要有权限访问aws创建资源，具体权限可参考：`iam-policy.json`。本例子中直接对eks worknode role进行授权。理论上也可以进行OIDC接入aws iam针对于pod级别进行权限管理（未测试）。\n- 确保ingress annotation的资源存在，否则创建会失败，具体可看pod日志。\n- `service` 的type为 `nodeport`。\n\n---\n### Install controller\n\n\n<details>\n<summary>rbac+sa+ns.yaml</summary>\n\n```yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: alb-ingress-controller\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    app: alb-ingress-controller\n  name: alb-ingress-controller\nrules:\n  - apiGroups:\n      - \"\"\n      - extensions\n    resources:\n      - configmaps\n      - endpoints\n      - events\n      - ingresses\n      - ingresses/status\n      - services\n    verbs:\n      - create\n      - get\n      - list\n      - update\n      - watch\n      - patch\n  - apiGroups:\n      - \"\"\n      - extensions\n    resources:\n      - nodes\n      - pods\n      - secrets\n      - services\n      - namespaces\n    verbs:\n      - get\n      - list\n      - watch\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  labels:\n    app: alb-ingress-controller\n  name: alb-ingress-controller\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: alb-ingress-controller\nsubjects:\n  - kind: ServiceAccount\n    name: alb-ingress\n    namespace: alb-ingress-controller\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    app: alb-ingress-controller\n  name: alb-ingress\n  namespace: alb-ingress-controller\n```\n</details>\n\n\n<details>\n<summary>controller.yaml</summary>\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: alb-ingress-controller\n  name: alb-ingress-controller\n  # Namespace the ALB Ingress Controller should run in. Does not impact which\n  # namespaces it's able to resolve ingress resource for. For limiting ingress\n  # namespace scope, see --watch-namespace.\n  namespace: alb-ingress-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: alb-ingress-controller\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: alb-ingress-controller\n    spec:\n      containers:\n        - args:\n            # Limit the namespace where this ALB Ingress Controller deployment will\n            # resolve ingress resources. If left commented, all namespaces are used.\n            # - --watch-namespace=uat\n\n            # Setting the ingress-class flag below ensures that only ingress resources with the\n            # annotation kubernetes.io/ingress.class: \"alb\" are respected by the controller. You may\n            # choose any class you'd like for this controller to respect.\n            - --ingress-class=alb\n\n            # Name of your cluster. Used when naming resources created\n            # by the ALB Ingress Controller, providing distinction between\n            # clusters.\n            - --cluster-name=aux-eks\n\n            # AWS VPC ID this ingress controller will use to create AWS resources.\n            # If unspecified, it will be discovered from ec2metadata.\n            # - --aws-vpc-id=vpc-xxxxxx\n\n            # AWS region this ingress controller will operate in.\n            # If unspecified, it will be discovered from ec2metadata.\n            # List of regions: http://docs.aws.amazon.com/general/latest/gr/rande.html#vpc_region\n            # - --aws-region=us-west-1\n\n            # Enables logging on all outbound requests sent to the AWS API.\n            # If logging is desired, set to true.\n            # - ---aws-api-debug\n            # Maximum number of times to retry the aws calls.\n            # defaults to 10.\n            # - --aws-max-retries=10\n          env:\n            # AWS key id for authenticating with the AWS API.\n            # This is only here for examples. It's recommended you instead use\n            # a project like kube2iam for granting access.\n            #- name: AWS_ACCESS_KEY_ID\n            #  value: KEYVALUE\n\n            # AWS key secret for authenticating with the AWS API.\n            # This is only here for examples. It's recommended you instead use\n            # a project like kube2iam for granting access.\n            #- name: AWS_SECRET_ACCESS_KEY\n            #  value: SECRETVALUE\n          # Repository location of the ALB Ingress Controller.\n          image: 894847497797.dkr.ecr.us-west-2.amazonaws.com/aws-alb-ingress-controller:v1.0.0\n          imagePullPolicy: Always\n          name: server\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n      dnsPolicy: ClusterFirst\n      restartPolicy: Always\n      securityContext: {}\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: alb-ingress\n      serviceAccount: alb-ingress\n```\n</details>\n<details>\n<summary>iam-policy.json</summary>\n\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"acm:DescribeCertificate\",\n        \"acm:ListCertificates\",\n        \"acm:GetCertificate\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:AuthorizeSecurityGroupIngress\",\n        \"ec2:CreateSecurityGroup\",\n        \"ec2:CreateTags\",\n        \"ec2:DeleteTags\",\n        \"ec2:DeleteSecurityGroup\",\n        \"ec2:DescribeInstances\",\n        \"ec2:DescribeInstanceStatus\",\n        \"ec2:DescribeSecurityGroups\",\n        \"ec2:DescribeSubnets\",\n        \"ec2:DescribeTags\",\n        \"ec2:DescribeVpcs\",\n        \"ec2:ModifyInstanceAttribute\",\n        \"ec2:ModifyNetworkInterfaceAttribute\",\n        \"ec2:RevokeSecurityGroupIngress\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"elasticloadbalancing:AddTags\",\n        \"elasticloadbalancing:CreateListener\",\n        \"elasticloadbalancing:CreateLoadBalancer\",\n        \"elasticloadbalancing:CreateRule\",\n        \"elasticloadbalancing:CreateTargetGroup\",\n        \"elasticloadbalancing:DeleteListener\",\n        \"elasticloadbalancing:DeleteLoadBalancer\",\n        \"elasticloadbalancing:DeleteRule\",\n        \"elasticloadbalancing:DeleteTargetGroup\",\n        \"elasticloadbalancing:DeregisterTargets\",\n        \"elasticloadbalancing:DescribeListeners\",\n        \"elasticloadbalancing:DescribeLoadBalancers\",\n        \"elasticloadbalancing:DescribeLoadBalancerAttributes\",\n        \"elasticloadbalancing:DescribeRules\",\n        \"elasticloadbalancing:DescribeSSLPolicies\",\n        \"elasticloadbalancing:DescribeTags\",\n        \"elasticloadbalancing:DescribeTargetGroups\",\n        \"elasticloadbalancing:DescribeTargetGroupAttributes\",\n        \"elasticloadbalancing:DescribeTargetHealth\",\n        \"elasticloadbalancing:ModifyListener\",\n        \"elasticloadbalancing:ModifyLoadBalancerAttributes\",\n        \"elasticloadbalancing:ModifyRule\",\n        \"elasticloadbalancing:ModifyTargetGroup\",\n        \"elasticloadbalancing:ModifyTargetGroupAttributes\",\n        \"elasticloadbalancing:RegisterTargets\",\n        \"elasticloadbalancing:RemoveTags\",\n        \"elasticloadbalancing:SetIpAddressType\",\n        \"elasticloadbalancing:SetSecurityGroups\",\n        \"elasticloadbalancing:SetSubnets\",\n        \"elasticloadbalancing:SetWebACL\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"iam:GetServerCertificate\",\n        \"iam:ListServerCertificates\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"waf-regional:GetWebACLForResource\",\n        \"waf-regional:GetWebACL\",\n        \"waf-regional:AssociateWebACL\",\n        \"waf-regional:DisassociateWebACL\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"tag:GetResources\",\n        \"tag:TagResources\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"waf:GetWebACL\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n```\n</details>\n\n<details>\n<summary>ingress.yaml</summary>\n\n```yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n    alb.ingress.kubernetes.io/actions.ssl-redirect: '{\"Type\": \"redirect\", \"RedirectConfig\":\n      { \"Protocol\": \"HTTPS\", \"Port\": \"443\", \"StatusCode\": \"HTTP_301\"}}'\n    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:ap-northeast-1:xxxxxxxxx:certificate/281bee29-717c-4b50-bf8e-41a39748c548\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80, \"HTTPS\": 443}]'\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/subnets: subnet-32ce5544, subnet-41cf6c19\n    alb.ingress.kubernetes.io/security-groups: sg-018347dc7a3ade5a2\n    kubernetes.io/ingress.class: alb\n  name: xiemx-web-ingress\nspec:\n  rules:\n  - host: www.xiemx.com\n    http:\n      paths:\n      - backend:\n          serviceName: echo-v1\n          servicePort: 80\n  - host: '*.xiemx.com'\n    http:\n      paths:\n      - backend:\n          serviceName: ssl-redirect\n          servicePort: use-annotation\n        path: /*\n      - backend:\n          serviceName: echo-v1\n          servicePort: 80\n        path: /*\n```\n</details>\n\n\n---\n#### 测试：\n- 创建`alb-ingress-controller`\n```shell\n➜  aws-alb-ingress-controller git:(master) ✗ k get all\nNAME                                         READY   STATUS    RESTARTS   AGE\npod/alb-ingress-controller-9596b67b9-d7p69   1/1     Running   0          80d\n\nNAME                                     READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/alb-ingress-controller   1/1     1            1           361d\n\nNAME                                               DESIRED   CURRENT   READY   AGE\nreplicaset.apps/alb-ingress-controller-9596b67b9   1         1         1       361d\n\n\n\n➜  aws-alb-ingress-controller git:(master) ✗ k logs -f pod/alb-ingress-controller-9596b67b9-d7p69\n-------------------------------------------------------------------------------\nAWS ALB Ingress controller\n  Release:    v1.0.0\n  Build:      git-c25bc6c5\n  Repository: https://github.com/kubernetes-sigs/aws-alb-ingress-controller\n-------------------------------------------------------------------------------\n\nW0917 09:50:21.934737       1 client_config.go:552] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.\nI0917 09:50:21.990251       1 :0] kubebuilder/controller \"level\"=0 \"msg\"=\"Starting EventSource\"  \"Controller\"=\"alb-ingress-controller\" \"Source\"={\"Type\":{\"metadata\":{\"creationTimestamp\":null},\"spec\":{},\"status\":{\"loadBalancer\":{}}}}\nI0917 09:50:21.990434       1 :0] kubebuilder/controller \"level\"=0 \"msg\"=\"Starting EventSource\"  \"Controller\"=\"alb-ingress-controller\" \"Source\"={\"Type\":{\"metadata\":{\"creationTimestamp\":null},\"spec\":{},\"status\":{\"loadBalancer\":{}}}}\nI0917 09:50:21.990555       1 :0] kubebuilder/controller \"level\"=0 \"msg\"=\"Starting EventSource\"  \"Controller\"=\"alb-ingress-controller\" \"Source\"={\"Type\":{\"metadata\":{\"creationTimestamp\":null}}}\nI0917 09:50:21.990841       1 :0] kubebuilder/controller \"level\"=0 \"msg\"=\"Starting EventSource\"  \"Controller\"=\"alb-ingress-controller\" \"Source\"={\"Type\":{\"metadata\":{\"creationTimestamp\":null},\"spec\":{},\"status\":{\"daemonEndpoints\":{\"kubeletEndpoint\":{\"Port\":0}},\"nodeInfo\":{\"machineID\":\"\",\"systemUUID\":\"\",\"bootID\":\"\",\"kernelVersion\":\"\",\"osImage\":\"\",\"containerRuntimeVersion\":\"\",\"kubeletVersion\":\"\",\"kubeProxyVersion\":\"\",\"operatingSystem\":\"\",\"architecture\":\"\"}}}}\nI0917 09:50:21.993054       1 leaderelection.go:185] attempting to acquire leader lease  alb-ingress-controller/ingress-controller-leader-alb...\nI0917 09:50:38.515944       1 leaderelection.go:194] successfully acquired lease alb-ingress-controller/ingress-controller-leader-alb\nI0917 09:50:38.716164       1 :0] kubebuilder/controller \"level\"=0 \"msg\"=\"Starting Controller\"  \"Controller\"=\"alb-ingress-controller\"\nI0917 09:50:38.816322       1 :0] kubebuilder/controller \"level\"=0 \"msg\"=\"Starting workers\"  \"Controller\"=\"alb-ingress-controller\" \"WorkerCount\"=1\n```\n\n- 创建`ingress`测试\n```shell\n➜  aws-alb-ingress-controller git:(master) ✗ k get ingress\nNAME                HOSTS                       ADDRESS                                                                      PORTS   AGE\nxiemx-web-ingress   www.xiemx.com,*.xiemx.com   71a14391-albingresscontrol-2ac6-648325502.ap-northeast-1.elb.amazonaws.com   80      14m\n➜  aws-alb-ingress-controller git:(master) ✗ curl 71a14391-albingresscontrol-2ac6-648325502.ap-northeast-1.elb.amazonaws.com -H host:www.xiemx.com -I\nHTTP/1.1 200 OK\nDate: Mon, 07 Dec 2020 06:39:29 GMT\nContent-Type: text/plain\nConnection: keep-alive\nServer: echoserver\n\n➜  aws-alb-ingress-controller git:(master) ✗ curl 71a14391-albingresscontrol-2ac6-648325502.ap-northeast-1.elb.amazonaws.com -H host:www.xiemx.com -i\nHTTP/1.1 200 OK\nDate: Mon, 07 Dec 2020 06:39:37 GMT\nContent-Type: text/plain\nTransfer-Encoding: chunked\nConnection: keep-alive\nServer: echoserver\n\n\n\nHostname: echo-85fb7989cc-d556c\n\nPod Information:\n\tnode name:\tip-10-200-2-113.ap-northeast-1.compute.internal\n\tpod name:\techo-85fb7989cc-d556c\n\tpod namespace:\talb-ingress-controller\n\tpod IP:\t10.200.2.197\n\nServer values:\n\tserver_version=nginx: 1.12.2 - lua: 10010\n\nRequest Information:\n\tclient_address=10.200.2.21\n\tmethod=GET\n\treal path=/\n\tquery=\n\trequest_version=1.1\n\trequest_scheme=http\n\trequest_uri=http://www.xiemx.com:8080/\n\nRequest Headers:\n\taccept=*/*\n\thost=www.xiemx.com\n\tuser-agent=curl/7.54.0\n\tx-amzn-trace-id=Root=1-5fcdce29-473e8ac375ce203f4961bec3\n\tx-forwarded-for=101.231.43.114\n\tx-forwarded-port=80\n\tx-forwarded-proto=http\n\nRequest Body:\n\t-no body in request-\n\n➜  aws-alb-ingress-controller git:(master) ✗ curl 71a14391-albingresscontrol-2ac6-648325502.ap-northeast-1.elb.amazonaws.com -H host:blog.xiemx.com -I\nHTTP/1.1 301 Moved Permanently\nServer: awselb/2.0\nDate: Mon, 07 Dec 2020 06:39:44 GMT\nContent-Type: text/html\nContent-Length: 134\nConnection: keep-alive\nLocation: https://blog.xiemx.com:443/\n```\n\n- ### aws alb 规则\n![aws-alb-ingress-ex1](/images/aws-alb-ingress-ex1.jpg)\n![aws-alb-ingress-ex2](/images/aws-alb-ingress-ex2.jpg)\n![aws-alb-ingress-ex3](/images/aws-alb-ingress-ex3.jpg)\n![aws-alb-ingress-ex4](/images/aws-alb-ingress-ex4.jpg)","tags":["k8s","aws","ingress"],"categories":["ingress","k8s"]},{"title":"nginx/openresty 执行阶段","url":"//2020/02/28/nginx-and-openresty-phases/","content":"#### nginx\n---\nnginx 官方文档：http://nginx.org/en/docs/dev/development_guide.html#http_phases\n\n```\nNGX_HTTP_POST_READ_PHASE:\n接收完请求头之后的第一个阶段，ngx_http_realip_module 注册在这个阶段\n\nNGX_HTTP_SERVER_REWRITE_PHASE:\nserver级别的uri重写阶段， ngx_http_rewrite_module 注册在这个阶段\n\nNGX_HTTP_FIND_CONFIG_PHASE:\n寻找location配置阶段，该阶段使用重写之后的uri来查找对应的location\n\nNGX_HTTP_REWRITE_PHASE:\nlocation级别的uri重写阶段\n\nNGX_HTTP_POST_REWRITE_PHASE:\nlocation级别重写的后一阶段，用来检查上阶段是否有uri重写，并根据结果跳转到合适的阶段\n\nNGX_HTTP_PREACCESS_PHASE:\n一般也用于访问控制，比如限制访问频率，并发等\nngx_http_limit_conn_module/ngx_http_limit_req_module这两个模块注册在此阶段\n\nNGX_HTTP_ACCESS_PHASE:\n访问权限控制阶段，比如基于ip黑白名单的权限控制，基于用户名密码的权限控制等， ngx_http_access_module/ngx_http_auth_basic_module 注册在此阶段\n\nNGX_HTTP_POST_ACCESS_PHASE:\n问权限控制的后一阶段，该阶段根据权限控制阶段的执行结果进行相应处理 `satisfy` 指令在此阶段生效。\n\nNGX_HTTP_TRY_FILES_PHASE:\ntry_files指令的处理阶段，如果没有配置try_files指令，则该阶段被跳过\n\nNGX_HTTP_CONTENT_PHASE:\n内容生成阶段，该阶段产生响应，并发送到客户端\n\nNGX_HTTP_LOG_PHASE:\n日志记录阶段，该阶段记录访问日志\n```\n\n#### openresty\n---\n参考文档：https://moonbingbing.gitbooks.io/openresty-best-practices/content/ngx_lua/phase.html\n![phase](/images/openresty_phases.png)\n```\nset_by_lua*: 流程分支处理判断变量初始化\nrewrite_by_lua*: 转发、重定向、缓存等功能(例如特定请求代理到外网)\naccess_by_lua*: IP 准入、接口权限等情况集中处理(例如配合 iptable 完成简单防火墙)\ncontent_by_lua*: 内容生成\nheader_filter_by_lua*: 响应头部过滤处理(例如添加头部信息)\nbody_filter_by_lua*: 响应体过滤处理(例如完成应答内容统一成大写)\nlog_by_lua*: 会话完成后本地异步完成日志记录(日志可以记录在本地，还可以同步到其他机器)\n```","tags":["nginx","openresty"],"categories":["nginx"]},{"title":"K8S initContainer","url":"//2020/02/25/k8s-initContainer/","content":"\nk8s 可以通过init container来在容器启动之前执行一下初始化之类的操作，initContainers中的容器将按照顺序执行，并在上一个退出后执行下一个，所有容器安全运行结束后启动spec中的容器。\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: xiemx\n  labels:\n    app: xiemx\nspec:\n  containers:\n  - name: xiemx\n    image: busybox\n    command: ['sh', '-c', 'echo running && sleep 60']\n  initContainers:\n  - name: init-1\n    image: busybox\n    command: ['sh', '-c', 'echo init container 1']\n  - name: init-2\n    image: busybox\n    command: ['sh', '-c', 'echo init container 2']\n```","tags":["k8s","initContainer"],"categories":["k8s"]},{"title":"K8S PDB","url":"//2020/02/24/k8s-pdb/","content":"\npdb 控制pod自愿中断时，最大可用和不可用的pod数量，可能会在node drain时阻断维护进程。\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: xiemx\n  name: xiemx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: xiemx\n  template:\n    metadata:\n      labels:\n        app: xiemx\n    spec:\n      containers:\n      - image: busybox\n        name: busybox\n        command: [\"sleep\", \"60\"]\n        \n---\napiVersion: policy/v1beta1\nkind: PodDisruptionBudget\nmetadata:\n  name: xiemx-PDB\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: xiemx\n```\n\n```shell\n➜  pdb git:(master) ✗ k get pdb\nNAME        MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE\nxiemx-PDB   N/A             1                 0                     7s\n```\n","tags":["k8s","pdb"],"categories":["k8s"]},{"title":"K8S LimitRange and ResourceQuota","url":"//2020/02/22/k8s-limit-range/","content":"#### ResourceQuota\n\nresourceQuota 可以限制一个ns下可以创建的资源数量和资源的limit\n\n```yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: compute-resources\nspec:\n  hard:\n    pods: \"4\"\n    requests.cpu: \"1\"\n    requests.memory: 1Gi\n    limits.cpu: \"2\"\n    limits.memory: 2Gi\n```\n---\n#### LimitRange\n\nk8s 使用limit range开控制一个命名空间下的不同type(pod, container)类型资源限制，参考下面\n\n```yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: test-limit-range\n\n---\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: limit-mem-cpu-per-container\nspec:\n  limits:\n  - max:\n      cpu: \"800m\"\n      memory: \"1Gi\"\n    min:\n      cpu: \"100m\"\n      memory: \"100Mi\"\n    default:\n      cpu: \"700m\"\n      memory: \"900Mi\"\n    defaultRequest:\n      cpu: \"110m\"\n      memory: \"200Mi\"\n    type: Container\n\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test\nspec:\n  containers:\n    - name: t01\n      image: busybox\n      command: [ \"sleep\", \"60\"]\n\n    - name: t02\n      image: busybox\n      command: [ \"sleep\", \"60\"]\n      resources:\n        limits:\n          cpu: \"200m\"\n          memory: \"300Mi\"\n\n    - name: t03\n      image: busybox\n      command: [ \"sleep\", \"60\"]\n      resources:\n        requests:\n          cpu: \"300m\"\n          memory: \"400Mi\"\n\n    - name: t04\n      image: busybox\n      command: [ \"sleep\", \"60\"]\n      resources:\n        limits:\n          cpu: \"444m\"\n          memory: \"444Mi\"\n        requests:\n          cpu: \"444m\"\n          memory: \"444Mi\"\n```\n```shell\n➜  limitRange git:(master) ✗ k get pod\nNAME   READY   STATUS    RESTARTS   AGE\ntest   4/4     Running   0          20s\n➜  limitRange git:(master) ✗ k describe pod test\nName:         test\nNamespace:    test-limit-range\nPriority:     0\nNode:         ip-10-200-1-57.ap-northeast-1.compute.internal/10.200.1.57\nStart Time:   Mon, 24 Feb 2020 10:17:19 +0800\nLabels:       <none>\nAnnotations:  kubectl.kubernetes.io/last-applied-configuration:\n                {\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{},\"name\":\"test\",\"namespace\":\"test-limit-range\"},\"spec\":{\"containers\":[{\"command...\n              kubernetes.io/limit-ranger:\n                LimitRanger plugin set: cpu, memory request for container t01; cpu, memory limit for container t01; cpu, memory limit for container t03\n              kubernetes.io/psp: eks.privileged\nStatus:       Running\nIP:           10.200.1.207\nIPs:          <none>\nContainers:\n  t01:\n    Container ID:  docker://b3d8927e0654e7be5f9d826ae14244c9c191d9a9bdb505a9a0b552f8502730e9\n    Image:         busybox\n    Image ID:      docker-pullable://busybox@sha256:6915be4043561d64e0ab0f8f098dc2ac48e077fe23f488ac24b665166898115a\n    Port:          <none>\n    Host Port:     <none>\n    Command:\n      sleep\n      60\n    State:          Running\n      Started:      Mon, 24 Feb 2020 10:17:23 +0800\n    Ready:          True\n    Restart Count:  0\n    Limits:\n      cpu:     700m\n      memory:  900Mi\n    Requests:\n      cpu:        110m\n      memory:     200Mi\n    Environment:  <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-vjjds (ro)\n  t02:\n    Container ID:  docker://ca64d1db5597e14e4009c34febd4fd97e0a2858605dbcee5d2f4fa6e0d98342b\n    Image:         busybox\n    Image ID:      docker-pullable://busybox@sha256:6915be4043561d64e0ab0f8f098dc2ac48e077fe23f488ac24b665166898115a\n    Port:          <none>\n    Host Port:     <none>\n    Command:\n      sleep\n      60\n    State:          Running\n      Started:      Mon, 24 Feb 2020 10:17:27 +0800\n    Ready:          True\n    Restart Count:  0\n    Limits:\n      cpu:     200m\n      memory:  300Mi\n    Requests:\n      cpu:        200m\n      memory:     300Mi\n    Environment:  <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-vjjds (ro)\n  t03:\n    Container ID:  docker://21891b99b6ebd54eadc4ceed63c451e0eb58392dec19679793a141fbadf22491\n    Image:         busybox\n    Image ID:      docker-pullable://busybox@sha256:6915be4043561d64e0ab0f8f098dc2ac48e077fe23f488ac24b665166898115a\n    Port:          <none>\n    Host Port:     <none>\n    Command:\n      sleep\n      60\n    State:          Running\n      Started:      Mon, 24 Feb 2020 10:17:30 +0800\n    Ready:          True\n    Restart Count:  0\n    Limits:\n      cpu:     700m\n      memory:  900Mi\n    Requests:\n      cpu:        300m\n      memory:     400Mi\n    Environment:  <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-vjjds (ro)\n  t04:\n    Container ID:  docker://21e59b648db7aec5d345fc1d6b9998de3c924070c18fcc4e4627a45703401b9c\n    Image:         busybox\n    Image ID:      docker-pullable://busybox@sha256:6915be4043561d64e0ab0f8f098dc2ac48e077fe23f488ac24b665166898115a\n    Port:          <none>\n    Host Port:     <none>\n    Command:\n      sleep\n      60\n    State:          Running\n      Started:      Mon, 24 Feb 2020 10:17:33 +0800\n    Ready:          True\n    Restart Count:  0\n    Limits:\n      cpu:     444m\n      memory:  444Mi\n    Requests:\n      cpu:        444m\n      memory:     444Mi\n    Environment:  <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-vjjds (ro)\nConditions:\n  Type              Status\n  Initialized       True\n  Ready             True\n  ContainersReady   True\n  PodScheduled      True\nVolumes:\n  default-token-vjjds:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-vjjds\n    Optional:    false\nQoS Class:       Burstable\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                     Message\n  ----    ------     ----  ----                                                     -------\n  Normal  Scheduled  29s   default-scheduler                                        Successfully assigned test-limit-range/test to ip-10-200-1-57.ap-northeast-1.compute.internal\n  Normal  Pulling    28s   kubelet, ip-10-200-1-57.ap-northeast-1.compute.internal  pulling image \"busybox\"\n  Normal  Created    25s   kubelet, ip-10-200-1-57.ap-northeast-1.compute.internal  Created container\n  Normal  Started    25s   kubelet, ip-10-200-1-57.ap-northeast-1.compute.internal  Started container\n  Normal  Pulled     25s   kubelet, ip-10-200-1-57.ap-northeast-1.compute.internal  Successfully pulled image \"busybox\"\n  Normal  Pulling    24s   kubelet, ip-10-200-1-57.ap-northeast-1.compute.internal  pulling image \"busybox\"\n  Normal  Pulling    21s   kubelet, ip-10-200-1-57.ap-northeast-1.compute.internal  pulling image \"busybox\"\n  Normal  Pulled     21s   kubelet, ip-10-200-1-57.ap-northeast-1.compute.internal  Successfully pulled image \"busybox\"\n  Normal  Created    21s   kubelet, ip-10-200-1-57.ap-northeast-1.compute.internal  Created container\n  Normal  Started    21s   kubelet, ip-10-200-1-57.ap-northeast-1.compute.internal  Started container\n  Normal  Pulled     19s   kubelet, ip-10-200-1-57.ap-northeast-1.compute.internal  Successfully pulled image \"busybox\"\n  Normal  Created    19s   kubelet, ip-10-200-1-57.ap-northeast-1.compute.internal  Created container\n  Normal  Started    18s   kubelet, ip-10-200-1-57.ap-northeast-1.compute.internal  Started container\n  Normal  Pulling    18s   kubelet, ip-10-200-1-57.ap-northeast-1.compute.internal  pulling image \"busybox\"\n  Normal  Pulled     16s   kubelet, ip-10-200-1-57.ap-northeast-1.compute.internal  Successfully pulled image \"busybox\"\n  Normal  Created    16s   kubelet, ip-10-200-1-57.ap-northeast-1.compute.internal  Created container\n  Normal  Started    15s   kubelet, ip-10-200-1-57.ap-northeast-1.compute.internal  Started container\n```","tags":["k8s","limitRange","ResourceQuota"],"categories":["k8s"]},{"title":"K8S HPA","url":"//2020/01/25/k8s-hpa/","content":"k8s hpa当前有3个版本分别支持, v1和v2/beta1版本只能使用CPU使用情况来进行扩容，v2beta2版本可以使用自定义指标来定义\n使用v2beta2需要使用的metrics-server + prometheus, 使用另外的版本只需要metrics-server\n\nmetrics-server：https://github.com/kubernetes-sigs/metrics-server\nprometheus：https://github.com/coreos/prometheus-operator\n\n```shell\n➜  metrics-server git:(master) ✗ k api-versions | grep autoscaling\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\n```\n\n#### explame\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: test-hpa\n  name: test-hpa\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: test-hpa\n  template:\n    metadata:\n      labels:\n        app: test-hpa\n    spec:\n      containers:\n      - args:\n        - -cpus\n        - \"2\"\n        image: vish/stress\n        name: test\n        resources:\n          requests:\n            cpu: 0.01\n            memory: 25Mi\n          limits:\n            cpu: 0.05\n            memory: 60Mi\n---\napiVersion: autoscaling/v2beta1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: test-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1 \n    kind: Deployment \n    name: test-hpa\n  minReplicas: 1 \n  maxReplicas: 10 \n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      targetAverageUtilization: 10\n```\n```shell\n➜  metrics-server git:(master) ✗ k get hpa\nNAME       REFERENCE             TARGETS    MINPODS   MAXPODS   REPLICAS   AGE\ntest-hpa   Deployment/test-hpa   485%/10%   1         10        10         22h\n➜  metrics-server git:(master) ✗ kgp\nNAME                      READY   STATUS    RESTARTS   AGE\ntest-hpa-6b9dd99d-22db4   1/1     Running   0          29m\ntest-hpa-6b9dd99d-2dnt5   1/1     Running   0          30m\ntest-hpa-6b9dd99d-68vnv   1/1     Running   0          29m\ntest-hpa-6b9dd99d-8l59h   1/1     Running   0          29m\ntest-hpa-6b9dd99d-hqbwf   1/1     Running   0          30m\ntest-hpa-6b9dd99d-jx7s8   1/1     Running   0          30m\ntest-hpa-6b9dd99d-qsw4n   1/1     Running   0          22h\ntest-hpa-6b9dd99d-rvxw4   1/1     Running   0          29m\ntest-hpa-6b9dd99d-sq4vz   1/1     Running   0          29m\ntest-hpa-6b9dd99d-wppjm   1/1     Running   0          29m\n```","tags":["k8s","HPA"],"categories":["k8s"]},{"title":"K8S 预选优选","url":"//2020/01/25/k8s-predicate-and-priority/","content":"\n![predicate.jpg](/images/predicate.jpg)\n\n#### predicate\n\n-----\n\n- NoDiskConflict：pod所需的卷是否和节点已存在的卷冲突\n- NoVolumeZoneConflict：检查给定的zone限制前提下，检查如果在此主机上部署Pod是否存在卷冲突。云厂商可用区限制。\n- PodFitsResources：检查节点是否有足够资源\n- PodFitsHostPorts：检查Pod容器所需的Port是否已被占用\n- HostName：检查节点是否满足PodSpec的NodeName字段中指定节点主机名，不满足节点的全部会被过滤掉。\n- MatchNodeSelector：检查节点标签（label）是否匹配Pod的nodeSelector属性要求\n- MaxEBSVolumeCount：确保已挂载的EBS存储卷不超过设置的最大值（默认值：39。Amazon推荐最大卷数量为40）\n- MaxGCEPDVolumeCount：确保已挂载的GCE存储卷不超过预设的最大值（默认值：16）\n- MaxAzureDiskVolumeCount : 确保已挂载的Azure存储卷不超过设置的最大值（默认值：16）\n- CheckNodeMemoryPressure : 判断节点是否已经进入到内存压力状态，如果是则只允许调度内存为0标记的Pod。检查Pod能否调度到内存有压力的节点上。如有节点存在内存压力， Guaranteed类型的Pod（例如，requests与limit均指定且值相等） 不能调度到节点上\n- CheckNodeDiskPressure : 判断节点是否已经进入到磁盘压力状态，如果是，则不调度新的Pod。\n- PodToleratesNodeTaints : 根据 taints 和 toleration 的关系判断Pod是否可以调度到节点上Pod是否满足节点容忍的一些条件。\n- MatchInterPodAffinity : 节点亲和性筛选。\n- GeneralPredicates：包含一些基本的筛选规则，主要考虑 Kubernetes 资源是否充足，比如 CPU 和 内存 是否足够，端口是否冲突、selector 是否匹配等：\n  - PodFitsResources：检查主机上的资源是否满足Pod的需求。资源的计算是根据主机上运行Pod请求的资源作为参考的，而不是以实际运行的资源数量\n  - PodFitsHost：如果Pod指定了spec.NodeName，看节点的名字是否何它匹配，只有匹配的节点才能运行Pod\n  - PodFitsHostPorts：检查Pod申请的主机端口是否已经被其他Pod占用，如果是，则不能调度\n  - PodSelectorMatches：检查主机的标签是否满足Pod的 selector。包括NodeAffinity和nodeSelector中定义的标签。\n\n#### priority\n\n-----\n\n- LeastRequestedPriority：节点的优先级就由节点空闲资源与节点总容量的比值，即由（总容量-节点上Pod的容量总和-新Pod的容量）/总容量）来决定。CPU和内存具有相同权重，资源空闲比越高的节点得分越高。需要注意的是，这个优先级函数起到了按照资源消耗来跨节点分配Pod的作用。详细的计算规则如下：\n\n```math\ncpu((capacity – sum(requested)) * 10 / capacity) + memory((capacity – sum(requested)) * 10 / capacity) / 2\n```\n\n- LeastRequestedPriority举例说明：例如CPU的可用资源为100，运行容器申请的资源为15，则cpu分值为8.5分，内存可用资源为100，运行容器申请资源为20，则内存分支为8分。则此评价规则在此节点的分数为(8.5 +8) / 2 = 8.25分。\n- BalancedResourceAllocation：CPU和内存使用率越接近的节点权重越高，该策略不能单独使用，必须和LeastRequestedPriority组合使用，尽量选择在部署Pod后各项资源更均衡的机器。如果请求的资源（CPU或者内存）大于节点的capacity，那么该节点永远不会被调度到。\n- BalancedResourceAllocation举例说明：该调度策略是出于平衡度的考虑，避免出现CPU，内存消耗不均匀的事情。例如某节点的CPU剩余资源还比较充裕，假如为100，申请10，则cpuFraction为0.1，而内存剩余资源不多，假如为20，申请10，则memoryFraction为0.5，这样由于CPU和内存使用不均衡，此节点的得分为10-abs ( 0.1 - 0.5 ) * 10 = 6 分。假如CPU和内存资源比较均衡，例如两者都为0.5，那么代入公式，则得分为10分。\n- InterPodAffinityPriority：通过迭代 weightedPodAffinityTerm 的元素计算和，并且如果对该节点满足相应的PodAffinityTerm，则将 “weight” 加到和中，具有最高和的节点是最优选的。 `\n- SelectorSpreadPriority：为了更好的容灾，对同属于一个service、replication controller或者replica的多个Pod副本，尽量调度到多个不同的节点上。如果指定了区域，调度器则会尽量把Pod分散在不同区域的不同节点上。当一个Pod的被调度时，会先查找Pod对于的service或者replication controller，然后查找service或replication controller中已存在的Pod，运行Pod越少的节点的得分越高。\n\n- SelectorSpreadPriority举例说明：这里主要针对多实例的情况下使用。例如，某一个服务，可能存在5个实例，例如当前节点已经分配了2个实例了，则本节点的得分为10*（（5-2）/ 5）=6分，而没有分配实例的节点，则得分为10 * （（5-0） / 5）=10分。没有分配实例的节点得分越高。\n\n- NodePreferAvoidPodsPriority（权重1W）：如果 节点的 Anotation 没有设置 key-value:scheduler. alpha.kubernetes.io/ preferAvoidPods = \"...\"，则节点对该 policy 的得分就是10分，加上权重10000，那么该node对该policy的得分至少10W分。如果Node的Anotation设置了，scheduler.alpha.kubernetes.io/preferAvoidPods = \"...\" ，如果该 pod 对应的 Controller 是 ReplicationController 或 ReplicaSet，则该 node 对该 policy 的得分就是0分。\n- TaintTolerationPriority : 使用 Pod 中 tolerationList 与 节点 Taint 进行匹配，配对成功的项越多，则得分越低。\n\n\n\n-----\n\n搬运自：http://dockone.io/article/2885\n\n","tags":["k8s"],"categories":["k8s"]},{"title":"character-varying vs character vs text","url":"//2020/01/22/character-varying/","content":"\n#### 字符串类型\n|名字|描述|\n|-----|-----|\n|character varying(n), varchar(n)|变长，有长度限制|\n|character(n), char(n)|定长，不足补空白|\n|text|变长，无长度限制|\n\n#### 类型区别\n* SQL定义了两种基本的字符类型：`character varying(n)` 和`character(n)`，这里的n是一个正整数。两种类型都可以存储最多n个字符的字符串。 试图存储更长的字符串到这些类型的字段里会产生一个错误， 除非超出长度的字符都是空白，这种情况下该字符串将被截断为最大长度。\n  * `varchar(n)`和`char(n)`分别是`character varying(n)`和`character(n)`的别名。\n\n  * 如果要存储的字符串比声明的长度短，类型为`character`的数值将会用空白填满； 而类型为`character varying`的数值将只是存储短些的字符串。\n\n  * 如果我们明确地把一个数值转换成`character varying(n)`或`character(n)`，那么超长的数值将被截断成n 个字符，且不会抛出错误。这也是SQL标准的要求。\n  \n  *  没有声明长度的`character`等于`character(1)`, 如果不带长度使用`character varying`， 那么该类型接受任何长度的字符串。后者是PostgreSQL的扩展。\n\n  * character类型的数值物理上都用空白填充到指定的长度n， 并且以这种方式存储和显示。不过，在比较两个character 类型的值时，尾随的空白不需要理会。 在空白比较重要的排序规则中，这个行为会导致意想不到的结果， 比如`SELECT 'a '::CHAR(2) collate \"C\" < 'a\\n'::CHAR(2)`返回`true`。 在将`character`值转换成其它字符串类型的时候， 它后面的空白会被删除。请注意， 在`character varying`和`text`数值里， 结尾的空白是有语意的。 并且当使用模式匹配时，如LIKE，使用正则表达式。\n\n#### 官方提示\n* 三种类型**没有性能差别**，除了当使用填充空白类型时的增加存储空间和当存储长度约束的列时一些检查存入时长度的额外的CPU周期。 某些其它的数据库系统里，character(n) 有一定的性能优势，但在PostgreSQL里没有。 事实上，character(n)通常是这三个中最慢的， 因为额外存储成本。在大多数情况下，应该使用text 或character varying。\n\n* 不管怎样，允许存储的最长字符串大概是`1GB` 。允许在数据类型声明中出现的n 的最大值比这还小。修改这个行为没有什么意义，因为在多字节编码下字符和字节的数目可能差别很大。 如果你想存储没有特定上限的长字符串，那么使用text 或没有长度声明的character varying，而不要选择一个任意长度限制。\n\n#### 使用\n```sql\nCREATE TABLE test1 (a character(4));\nINSERT INTO test1 VALUES ('ok');\nSELECT a, char_length(a) FROM test1; -- (1)\n  a   | char_length\n------+-------------\n ok   |           2\n\nCREATE TABLE test2 (b varchar(5));\nINSERT INTO test2 VALUES ('ok');\nINSERT INTO test2 VALUES ('good      ');\nINSERT INTO test2 VALUES ('too long');\nERROR:  value too long for type character varying(5)\nINSERT INTO test2 VALUES ('too long'::varchar(5)); -- 明确截断\nSELECT b, char_length(b) FROM test2;\n   b   | char_length\n-------+-------------\n ok    |           2\n good  |           5\n too l |           5\n```\n-----\npostgresql字符串类型比较，参考：\nhttps://www.postgresql.org/docs/9.1/datatype-character.html\nhttp://www.postgres.cn/docs/9.4/datatype-character.html\n","tags":["database","postgresql"],"categories":["database"]},{"title":"K8S QOS","url":"//2020/01/22/k8s-qos/","content":"\n对于一个 pod 来说，服务质量体现在两个具体的指标：CPU 和内存。当节点上内存资源紧张时，kubernetes 会根据预先设置的不同 QoS 类别进行相应处理。\n * guaranteed （有保证的）\n * burstable （不稳定的）\n * Best-Effort （尽力而为）\n\n #### Guaranteed\n\n  * Pod中的所有容器都且仅设置了 CPU 和内存的 limits\n  * pod中的所有容器都设置了 CPU 和内存的 requests 和 limits ，且单个容器内的requests==limits（requests不等于0）\n\n ```yaml\n #### set limit\n containers:\n  name: xiemx1\n    resources:\n      limits:\n        cpu: \"10m\"\n        memory: \"1Gi\"\n  name: xiemx2\n    resources:\n      limits:\n        cpu: \"100m\"\n        memory: \"100Mi\"\n        \n #### request=limit\n containers:\n  name: xiemx1\n    resources:\n      limits:\n        cpu: \"10m\"\n        memory: \"1Gi\"\n      requests:\n        cpu: \"10m\"\n        memory: \"1Gi\"\n\n  name: xiemx2\n    resources:\n      limits:\n        cpu: \"100m\"\n        memory: \"100Mi\"\n      requests:\n        cpu: \"100m\"\n        memory: \"100Mi\"\n ```\n\n #### Burstable\n * pod中只要有一个容器的requests和limits的设置不相同\n * pod中只要有一个容器的cpu or memory 没有设置limits\n\n```yaml\ncontainers:\n  name: xiemx1\n    resources:\n      limits:\n        memory: \"1Gi\"\n\n  name: xiemx2\n    resources:\n      limits:\n        cpu: \"100m\"\n```\n\n#### Best-Effort\n* Pod中所有容器的resources均未设置requests与limits\n\n```yaml \ncontainers:\n  name: xiemx1\n    resources:\n  name: xiemx2\n    resources:\n```\n\n#### 不同策略的QOS回收策略实现\nKubernetes 通过cgroup给pod设置QoS级别，当资源不足时先kill优先级低的pod，通过OOM_ADJ参数计算分数值来实现，OOM分数值范围为0-1000。计算出来的OOM分数越高，表明该pod优先级就越低，当出现资源竞争时会越早被kill掉\n* Guaranteed级别的 Pod，OOM_ADJ参数设置成了-998\n* Best-Effort级别的 Pod，OOM_ADJ参数设置成了1000\n* 对于Burstable级别的 Pod，OOM_ADJ参数取值从2到999\n* kuberntes 保留资源，比如kubelet，docker，OOM_ADJ参数设置成了-999，表示不会被OOM kill掉\n\n#### QoS pods被kill掉场景与顺序\n* Best-Effort pods：系统用完了全部内存时，该类型 pods 会最先被kill掉。\n* Burstable pods：系统用完了全部内存，且没有 Best-Effort 类型的容器可以被 kill 时，该类型的 pods 会被 kill 掉。\n* Guaranteed pods：系统用完了全部内存，且没有 Burstable 与 Best-Effort 类型的容器可以被 kill 时，该类型的 pods 会被 kill 掉。\n\n参考：https://www.qikqiak.com/post/kubernetes-qos-usage/","tags":["k8s","QOS"],"categories":["k8s"]},{"title":"《nginx 变量漫谈》随记","url":"//2020/01/22/nginx-tick/","content":"#### agentzh nginx变量漫谈随手记\n文章地址:https://openresty.org/download/agentzh-nginx-tutorials-zhcn.html  \n\n1. Nginx 变量名前面有一个 $ 符号\n```\nset $a \"b\";\nset $b \"$a, $a\";\n```\n\n2. 有没有办法把特殊的 `$`字符给转义掉呢？\n- 答案是否定的, 不过幸运的是，我们可以绕过这个限制，比如通过不支持“变量插值”的模块配置指令专门构造出取值为 $ 的 Nginx 变量，然后再在 echo 中使用这个变量\n```\n   geo $dollar {\n        default \"$\";\n    }\n\n    server {\n        listen 8080;\n\n        location /test {\n            echo \"This is a dollar sign: $dollar\";\n        }\n    }\n\n```\n\n3. 在“变量插值”的上下文中，还有一种特殊情况，即当引用的变量名之后紧跟着变量名的构成字符时（比如后跟字母、数字以及下划线），我们就需要使用特别的记法来消除歧义，例如：\n```\n    server {\n        listen 8080;\n\n        location /test {\n            set $first \"hello \";\n            echo \"${first}world\";\n        }\n    }\n```\n\n4. 变量创建和加载\n - Nginx 变量的创建和赋值操作发生在全然不同的时间阶段。\n - Nginx 变量的创建只能发生在 Nginx 配置加载的时候\n - 而赋值操作则只会发生在请求实际处理的时候。\n - 这意味着不创建而直接使用变量会导致启动失败。\n - 无法在请求处理时动态地创建新的 Nginx 变量。\n - Nginx 变量一旦创建可见范围就是整个Nginx配置，跨越不同虚拟主机的 server 配置块。\n\n5. 内部跳转\n\n使用第三方模块 ngx_echo 提供的 echo_exec 配置指令，发起到 location /bar 的“内部跳转”。所谓“内部跳转”，就是在处理请求的过程中，于服务器内部，从一个 location 跳转到另一个 location 的过程。不同于 301 和 302 会发生二次请求。 exec类似于linux中的exec 请求发出后不会在回到原理的location，而是直接接管。\n\n```\nserver {\n    listen 8080;\n\n    location /foo {\n        set $a hello;\n        echo_exec /bar;\n    }\n\n    location /bar {\n        echo \"a = [$a]\";\n    }\n}\n\n### 效果等同于rewrite实现\n\nserver {\n    listen 8080;\n\n    location /foo {\n        set $a hello;\n        rewrite ^ /bar;\n    }\n\n    location /bar {\n        echo \"a = [$a]\";\n    }\n}\n```\n\n6. 内置变量获取request参数\n- 取 arg 值的 `$arg_XXX`\n- 取 cookie 值的 `$cookie_XXX` \n- 取请求头的 `$http_XXX` \n- 取响应头的 `$sent_http_XXX` \n```\nlocation /test {\n    echo \"name: $arg_name\";\n    echo \"class: $arg_class\";\n}\n\n$ curl 'http://localhost:8080/test'\nname: \nclass: \n\n$ curl 'http://localhost:8080/test?name=Tom&class=3'\nname: Tom\nclass: 3\n\n$ curl 'http://localhost:8080/test?name=hello%20world&class=9'\nname: hello%20world\nclass: 9\n\n###Nginx 会在匹配参数名之前，自动把原始请求中的参数名调整为全部小写的形式\n\n$ curl 'http://localhost:8080/test?NAME=Marry'\nname: Marry\nclass: \n\n$ curl 'http://localhost:8080/test?Name=Jimmy'\nname: Jimmy\nclass: \n```\n\n7. `%XX` 字符解码\n- 可以使用第三方 ngx_set_misc 模块提供的 set_unescape_uri 配置指令：\n```\nlocation /test {\n    set_unescape_uri $name $arg_name;\n    set_unescape_uri $class $arg_class;\n\n    echo \"name: $name\";\n    echo \"class: $class\";\n}\n现在我们再看一下效果：\n\n$ curl 'http://localhost:8080/test?name=hello%20world&class=9'\nname: hello world\nclass: 9\n```\n\n8. 内建变量都是只读的，如 `$uri` 和 `$request_uri`. 应当避免对只读变量进行赋值。\n```\nlocation /bad {\n  set $uri /blah;\n  echo $uri;\n}\n\nNginx 在启动的时候报出错误：\n[emerg] the duplicate \"uri\" variable in ...\n```\n\n9. 多个同名arg变量取值\n- $arg_XXX 变量在请求 URL 中有多个同名 XXX 参数时，就只会返回最先出现的那个 XXX 参数的值，而默默忽略掉其他实例：\n```\nlocation /test {\n    content_by_lua '\n        if ngx.var.arg_name == nil then\n            ngx.say(\"name: missing\")\n        else\n            ngx.say(\"name: [\", ngx.var.arg_name, \"]\")\n        end\n    ';\n}\n\n$ curl 'http://localhost:8080/test?name=Tom&name=Jim&name=Bob'\nname: [Tom]\n要解决这些局限，可以直接在 Lua 代码中使用 ngx_lua 模块提供的 ngx.req.get_uri_args 函数。\n```","tags":["nginx"],"categories":["nginx"]},{"title":"zombie/orphan process","url":"//2020/01/22/zombie-and-orphan-process/","content":"### zombie process / orphan process\n\n父进程通过fork()函数来创建子进程。子进程会copy 当前父进程的状态和运行代码，独立运行，子进程和父进程的的运行和结束是一个异步的过程，fork出来之后运行的先后顺序也取决于系统的调度，不存在绝对的顺序。linux 为了进程退出能被正确回收，会维护一张进程的映射表，当进程结束时，系统会将进程的信息存储起来，等待父进程去处理回收子进程，处于退出状态且父进程没有调用wait()获取状态信息的进程就会陷入`Z` 状态，理论上所有的进程都会有一个短暂的`Z` 状态。\n\n* orphan process：一个父进程退出，而子进程还在运行，这些子进程将成为孤儿进程，被init进程所收养，并由init进程对它们完成状态收集工作。\n\n* zombie process：进程使用fork创建子进程，如果子进程退出，而父进程并没有调用wait或waitpid获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中。这种进程称之为僵死进程。\n\n#### 如何处理僵尸进程/孤儿进程\n\n* orphan process\n    理论上孤儿进程不会对系统有影响，init 进程会接管成为父进程，负责进程回收，linux中也会有很多主动的操作来避免子进程被主动回收，比如说`nohup`、`disown` ，都是通过屏蔽信号来使子进程成为orphan process最终被init接收，实现终端关闭但是进程不退出\n* zombie process\n    理论上zombie process是由于父进程不wait(), 可以直接干掉父进程，让init接手来处理\n\n\n#### fork()\n\n```\npid = fork()\n\nif pid > 0 { printf \"this is father\"}\nif pid = 0 { printf \"this is child\" }\nif pid < 0 { printf \"fork error\" }\n\nfork()后会copy一个进程出来，fork后的代码会分布在子进程和当前进程中独立运行。也就是会有2次输出。\nlinux man page的说明：\n\nDESCRIPTION\n       The fork extension adds three functions, as follows.\n\n       fork() This  function  creates  a new process. The return value is the zero in the child and the process-id number of the\n              child in the parent, or -1 upon  error.  In  the  latter  case,  ERRNO  indicates  the  problem.   In  the  child,\n              PROCINFO[\"pid\"] and PROCINFO[\"ppid\"] are updated to reflect the correct values.\n\n       waitpid()\n              This function takes a numeric argument, which is the process-id to wait for. The return value is that of the wait-\n              pid(2) system call.\n\n       wait() This function waits for the first child to die.  The return value is that of the wait(2) system call.\n\n```\n\n参考：https://www.cnblogs.com/anker/p/3271773.html","tags":["linux"],"categories":["linux"]},{"title":"k8s pod lifecycle","url":"//2020/01/21/k8s-pod-lifecycle/","content":"\n\n#### pod phase\n-----\n* pending:  Pod 已被 Kubernetes 系统接受，但有一个或者多个容器镜像尚未创建。等待时间包括调度 Pod 的时间和通过网络下载镜像\n* running: 该 Pod 已经绑定到了一个节点上，Pod 中所有的容器都已被创建。至少有一个容器正在运行，或者正处于启动或重启状态。\n* successed: Pod 中的所有容器都被成功终止，并且不会再重启。\n* Failed: Pod 中的所有容器都已终止了，并且至少有一个容器是因为失败终止。也就是说，容器以非0状态退出或者被系统终止。\n* unkown: 某些原因无法取得 Pod 的状态，通常是因为与 Pod 所在主机通信失败。\n\n#### 探针\n-----\n* `livenessProbe`：指示容器是否正在运行。如果存活探测失败，则 kubelet 会杀死容器，并且容器将受到其 [重启策略](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) 的影响。如果容器不提供存活探针，则默认状态为 `Success`。\n* `readinessProbe`：指示容器是否准备好服务请求。如果就绪探测失败，端点控制器将从与 Pod 匹配的所有 Service 的端点中删除该 Pod 的 IP 地址。初始延迟之前的就绪状态默认为 `Failure`。如果容器不提供就绪探针，则默认状态为 `Success`。\n* 探针支持的3种action事件\n  - [ExecAction](https://kubernetes.io/docs/resources-reference/v1.7/#execaction-v1-core)：在容器内执行指定命令。如果命令退出时返回码为 0 则认为诊断成功。\n  - [TCPSocketAction](https://kubernetes.io/docs/resources-reference/v1.7/#tcpsocketaction-v1-core)：对指定端口上的容器的 IP 地址进行 TCP 检查。如果端口打开，则诊断被认为是成功的。\n  - [HTTPGetAction](https://kubernetes.io/docs/resources-reference/v1.7/#httpgetaction-v1-core)：对指定的端口和路径上的容器的 IP 地址执行 HTTP Get 请求。如果响应的状态码大于等于200 且小于 400，则诊断被认为是成功的。\n\n#### restart policy\n-----\n * pod的spec中restartPolicy 有三种模式\n   \t* Always\n   \t* Never\n   \t* OnFailure\n * job 的spec中有2种\n   *  OnFailure\n   * Never","tags":["k8s","pod"],"categories":["k8s"]},{"title":"aws-vpc-cni","url":"//2020/01/21/aws-vpc-cni/","content":"### aws-vpc-cni\n\n详细信息可以参考一下aws的提案\n\n总结如下：\n\n1. AWS EKS运行在vpc中，因此所有节点能够运行的总pod数最终由vpc cidr中能够使用的ip地址数量来决定。\n2. EKS node能够运行的pod数量，由node上的secendary ip数量决定，secendary ip是由eni的数量来决定的，可以理解不同的机型能够attach的eni数量决定了，node上能够运行多少pod。\n3. pod使用cni来通讯，减少了中间网络层的封包/解包，网络性能上比较高。\n4. aws-vpc-cni由两个部分组成\n   1. Aws-vpc-cni 会在机器上嵌入一个进程`L-ipadm` 来管理pod的IP分配，eni的绑定和申请，ip pool的管理等等工作\n   2. cni plugin负责网络层的数据调度\n\n-----\n\naws 提案：https://github.com/aws/amazon-vpc-cni-k8s/blob/master/docs/cni-proposal.md","tags":["k8s","aws","network"],"categories":["k8s"]},{"title":"linux network namespace and bridge","url":"//2020/01/21/linux-network-namespace/","content":"### linux netns 和 bridge\n\n#### netns\n\n-----\n\n最近在学习k8s网络，看到权威指南中有讲到基础网络的实现，故而搬运一下重新学习network namespace的隔离技术，默认binary是放在iproute2这个套件中的\n\n```shell\nvagrant@ubuntu-xenial:~$ apt-file search $(which ip)\ncups-ipp-utils: /usr/sbin/ippserver\nfreeipa-client: /usr/sbin/ipa-certupdate\nfreeipa-client: /usr/sbin/ipa-client-automount\nfreeipa-client: /usr/sbin/ipa-client-install\nfreeipa-client: /usr/sbin/ipa-getkeytab\nfreeipa-client: /usr/sbin/ipa-join\nfreeipa-client: /usr/sbin/ipa-rmkeytab\niproute2: /sbin/ip\n```\n\n\n\n默认情况下，使用 `ip netns` 是没有网络 namespace 的，所以 `ip netns ls` 命令看不到任何输出。\n\n```bash\nvagrant@ubuntu-xenial:~$ ip netns help\nUsage: ip netns list\n       ip netns add NAME\n       ip netns set NAME NETNSID\n       ip [-all] netns delete [NAME]\n       ip netns identify [PID]\n       ip netns pids NAME\n       ip [-all] netns exec [NAME] cmd ...\n       ip netns monitor\n       ip netns list-id\n```\n\n新创建的 netns 会在`/var/run/netns/` 目录中生存对应名称的文件\n\n```shell\nvagrant@ubuntu-xenial:~$ sudo ip netns add xiemx1\nvagrant@ubuntu-xenial:~$ sudo ip netns add xiemx2\nvagrant@ubuntu-xenial:~$ sudo ip netns ls\nxiemx2\nxiemx1\nvagrant@ubuntu-xenial:~$ ll /var/run/netns/\ntotal 0\ndrwxr-xr-x  2 root root   80 Jan 21 03:19 ./\ndrwxr-xr-x 28 root root 1140 Jan 21 03:19 ../\n-r--r--r--  1 root root    0 Jan 21 03:19 xiemx1\n-r--r--r--  1 root root    0 Jan 21 03:19 xiemx2\n```\n\n\n\n由于netns 之间互相都是隔离的，因此要查看对应命名空间的网络设备、路由表就需要使用 `ip netns exec <netns name> bash`  开启子bash进入对应的命名空间，也可以直接执行命令\n\n```\nvagrant@ubuntu-xenial:~$ sudo ip a\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n    link/ether 02:97:71:8a:f0:d8 brd ff:ff:ff:ff:ff:ff\n    inet 10.0.2.15/24 brd 10.0.2.255 scope global enp0s3\n       valid_lft forever preferred_lft forever\n    inet6 fe80::97:71ff:fe8a:f0d8/64 scope link\n       valid_lft forever preferred_lft forever\n3: enp0s8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n    link/ether 08:00:27:f1:22:f6 brd ff:ff:ff:ff:ff:ff\n    inet 10.110.120.65/24 brd 10.110.120.255 scope global enp0s8\n       valid_lft forever preferred_lft forever\n    inet6 fe80::a00:27ff:fef1:22f6/64 scope link\n       valid_lft forever preferred_lft forever\n4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default\n    link/ether 02:42:da:5a:39:42 brd ff:ff:ff:ff:ff:ff\n    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0\n       valid_lft forever preferred_lft forever\n       \nvagrant@ubuntu-xenial:~$ sudo ip net exec xiemx1 ip a\n1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN group default qlen 1\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    \nvagrant@ubuntu-xenial:~$ sudo ip net exec xiemx2 ip a\n1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN group default qlen 1\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n```\n\n每个 namespace 在创建的时候会自动创建一个 `lo` ，默认时DOWN状态，如果需要启用记得UP一下：\n\n```bash\nvagrant@ubuntu-xenial:~$ sudo ip net exec xiemx1 ip a\n1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN group default qlen 1\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\nvagrant@ubuntu-xenial:~$ sudo ip netns exec xiemx1 ip link set lo up\nvagrant@ubuntu-xenial:~$ sudo ip netns exec xiemx1 ip a\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n```\n\n#### veth pair\n\n-----\n\nnetns 之间是相互隔离的，linux 提供了 `veth` 设备对来实现不同netns之间的往来通讯，veth 设备是成对出现的，类似于一根网线插到了两个隔离的ns之中，实现了两个隔离网络的互联。\n\n我们可以使用 `ip link add <name1> type veth peer name <name2>` 来创建一对 veth pair 出来，需要记住的是 veth pair 无法单独存在，删除其中一个，另一个也会自动消失。\n\n```\nvagrant@ubuntu-xenial:~$ sudo ip netns exec xiemx1 bash\nroot@ubuntu-xenial:~# ip add\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\nroot@ubuntu-xenial:~# ip link add xiemx-veth1 type veth peer name xiemx-veth2\nroot@ubuntu-xenial:~# ip add\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n4: xiemx-veth2@xiemx-veth1: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN group default qlen 1000\n    link/ether a6:76:6f:47:e1:f9 brd ff:ff:ff:ff:ff:ff\n5: xiemx-veth1@xiemx-veth2: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN group default qlen 1000\n    link/ether 42:6a:cb:19:0d:2a brd ff:ff:ff:ff:ff:ff\n    \n####如果对名称没有特别要求可以使用默认命令创建，会默认生存veth0/veth1 的设备对\nroot@ubuntu-xenial:~# ip link add type veth\nroot@ubuntu-xenial:~# ip add\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n4: xiemx-veth2@xiemx-veth1: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN group default qlen 1000\n    link/ether a6:76:6f:47:e1:f9 brd ff:ff:ff:ff:ff:ff\n5: xiemx-veth1@xiemx-veth2: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN group default qlen 1000\n    link/ether 42:6a:cb:19:0d:2a brd ff:ff:ff:ff:ff:ff\n6: veth0@veth1: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN group default qlen 1000\n    link/ether 3e:72:e3:48:25:69 brd ff:ff:ff:ff:ff:ff\n7: veth1@veth0: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN group default qlen 1000\n    link/ether ea:37:ea:92:a5:c5 brd ff:ff:ff:ff:ff:ff\n```\n\n给这对 veth pair 配置上 ip 地址，并up\n\n```\nroot@ubuntu-xenial:~# ip netns exec xiemx1 bash\nroot@ubuntu-xenial:~# ip add add 10.0.0.1/24 dev xiemx-veth1\nroot@ubuntu-xenial:~# ip add add 10.0.0.2/24 dev xiemx-veth2\nroot@ubuntu-xenial:~# ip add show dev xiemx-veth1 up\nroot@ubuntu-xenial:~# ip add show dev xiemx-veth2 up\nroot@ubuntu-xenial:~# ip add\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n4: xiemx-veth2@xiemx-veth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000\n    link/ether a6:76:6f:47:e1:f9 brd ff:ff:ff:ff:ff:ff\n    inet 10.0.0.2/24 scope global xiemx-veth2\n       valid_lft forever preferred_lft forever\n    inet6 fe80::a476:6fff:fe47:e1f9/64 scope link\n       valid_lft forever preferred_lft forever\n5: xiemx-veth1@xiemx-veth2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000\n    link/ether 42:6a:cb:19:0d:2a brd ff:ff:ff:ff:ff:ff\n    inet 10.0.0.1/24 scope global xiemx-veth1\n       valid_lft forever preferred_lft forever\n    inet6 fe80::406a:cbff:fe19:d2a/64 scope link\n       valid_lft forever preferred_lft forever\n6: veth0@veth1: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN group default qlen 1000\n    link/ether 3e:72:e3:48:25:69 brd ff:ff:ff:ff:ff:ff\n7: veth1@veth0: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN group default qlen 1000\n    link/ether ea:37:ea:92:a5:c5 brd ff:ff:ff:ff:ff:ff\n\nroot@ubuntu-xenial:~# ip route\n10.0.0.0/24 dev xiemx-veth1  proto kernel  scope link  src 10.0.0.1\n10.0.0.0/24 dev xiemx-veth2  proto kernel  scope link  src 10.0.0.2\n```\n\n目前所有的veth pair的两端都在xiemx1这个netns 中，现在移动设备的一段到xiemx2 这个netns中，实现网络互联\n\n```\nroot@ubuntu-xenial:~# ip link set xiemx-veth2 netns xiemx2\nroot@ubuntu-xenial:~# ip add\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n5: xiemx-veth1@if4: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state LOWERLAYERDOWN group default qlen 1000\n    link/ether 42:6a:cb:19:0d:2a brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet 10.0.0.1/24 scope global xiemx-veth1\n       valid_lft forever preferred_lft forever\n    inet6 fe80::406a:cbff:fe19:d2a/64 scope link\n       valid_lft forever preferred_lft forever\n6: veth0@veth1: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN group default qlen 1000\n    link/ether 3e:72:e3:48:25:69 brd ff:ff:ff:ff:ff:ff\n7: veth1@veth0: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN group default qlen 1000\n    link/ether ea:37:ea:92:a5:c5 brd ff:ff:ff:ff:ff:ff\nroot@ubuntu-xenial:~# ip netns exec xiemx2 ip add\n1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN group default qlen 1\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n4: xiemx-veth2@if5: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000\n    link/ether a6:76:6f:47:e1:f9 brd ff:ff:ff:ff:ff:ff link-netnsid 0\nroot@ubuntu-xenial:~# ip netns exec xiemx2 ifconfig xiemx-veth2 up\nroot@ubuntu-xenial:~# ip netns exec xiemx2 ip add\n1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN group default qlen 1\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n4: xiemx-veth2@if5: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000\n    link/ether a6:76:6f:47:e1:f9 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet6 fe80::a476:6fff:fe47:e1f9/64 scope link\n       valid_lft forever preferred_lft forever\nroot@ubuntu-xenial:~# ip netns exec xiemx2 ip add add 10.0.0.2/24 dev xiemx-veth2\nroot@ubuntu-xenial:~# ip netns exec xiemx2 ip add\n1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN group default qlen 1\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n4: xiemx-veth2@if5: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000\n    link/ether a6:76:6f:47:e1:f9 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet 10.0.0.2/24 scope global xiemx-veth2\n       valid_lft forever preferred_lft forever\n    inet6 fe80::a476:6fff:fe47:e1f9/64 scope link\n       valid_lft forever preferred_lft forever\nroot@ubuntu-xenial:~# ping 10.0.0.2\nPING 10.0.0.2 (10.0.0.2) 56(84) bytes of data.\n64 bytes from 10.0.0.2: icmp_seq=1 ttl=64 time=0.083 ms\n64 bytes from 10.0.0.2: icmp_seq=2 ttl=64 time=0.046 ms\n^C\n--- 10.0.0.2 ping statistics ---\n2 packets transmitted, 2 received, 0% packet loss, time 999ms\nrtt min/avg/max/mdev = 0.046/0.064/0.083/0.020 ms\n```\n\n#### bridge\n\n-----\n\nlinux 内核支持网口桥接，但是和传统的硬件网桥不同的是，linux 中的网桥设备不仅仅是二层设备，只是对报文进行转发，由于Linux 主机上运行的上层应用有可能就是报文的终点，因此还要求网桥能够将保数据包传递给linux网络协议栈。\n\nDocker bridge的网络就可以看成是通过bridge 来讲veth的设备对一端进行聚合，另一端放到容器的进程中，实现网络隔离和网络互联。再通过iptables的数据包转发功能来传递数据包，这里不讨论iptables层面的问题。\n\n手动模拟一下大概如下\n\n```shell\n###增加一个网桥（假设是docker bridge)\nroot@ubuntu-xenial:~# ip netns exec xiemx1 bash\nroot@ubuntu-xenial:~# brctl  show\nbridge name\tbridge id\t\tSTP enabled\tinterfaces\nroot@ubuntu-xenial:~# brctl addbr xiemx-br\nroot@ubuntu-xenial:~# brctl show\nbridge name\tbridge id\t\tSTP enabled\tinterfaces\nxiemx-br\t\t8000.000000000000\tno\n\n###将veth设备的一端绑定到网桥上，由于使用网桥进行通讯，所以veth设备在这里只需要当成二层设备来使用，不需要IP\nroot@ubuntu-xenial:~# brctl addif xiemx-br veth0\nroot@ubuntu-xenial:~# brctl show\nbridge name\tbridge id\t\tSTP enabled\tinterfaces\nxiemx-br\t\t8000.3e72e3482569\tno\t\tveth0\n\n###将veth的另一端移动到另一个netns中，可以理解为容器内的eth0\nroot@ubuntu-xenial:~# ip link set veth1 netns xiemx2\nroot@ubuntu-xenial:~# ip add\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n5: xiemx-veth1@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000\n    link/ether 42:6a:cb:19:0d:2a brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet 10.0.0.1/24 scope global xiemx-veth1\n       valid_lft forever preferred_lft forever\n    inet6 fe80::406a:cbff:fe19:d2a/64 scope link\n       valid_lft forever preferred_lft forever\n6: veth0@if7: <BROADCAST,MULTICAST> mtu 1500 qdisc noop master xiemx-br state DOWN group default qlen 1000\n    link/ether 3e:72:e3:48:25:69 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n8: xiemx-br: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000\n    link/ether 3e:72:e3:48:25:69 brd ff:ff:ff:ff:ff:ff\n    inet 11.0.0.1/24 brd 11.0.0.255 scope global xiemx-br\n       valid_lft forever preferred_lft forever\n\nroot@ubuntu-xenial:~# ip netns exec xiemx2 bash\nroot@ubuntu-xenial:~# ip add\n1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN group default qlen 1\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n4: xiemx-veth2@if5: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000\n    link/ether a6:76:6f:47:e1:f9 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet 10.0.0.2/24 scope global xiemx-veth2\n       valid_lft forever preferred_lft forever\n    inet6 fe80::a476:6fff:fe47:e1f9/64 scope link\n       valid_lft forever preferred_lft forever\n7: veth1@if6: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000\n    link/ether ea:37:ea:92:a5:c5 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n\n### 给veth1 分配ip 地址，并开启设备\nroot@ubuntu-xenial:~# ip add add 11.0.0.2/24 dev veth1\nroot@ubuntu-xenial:~# ifconfig veth1 up\nroot@ubuntu-xenial:~# ip add\n1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN group default qlen 1\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n4: xiemx-veth2@if5: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000\n    link/ether a6:76:6f:47:e1:f9 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet 10.0.0.2/24 scope global xiemx-veth2\n       valid_lft forever preferred_lft forever\n    inet6 fe80::a476:6fff:fe47:e1f9/64 scope link\n       valid_lft forever preferred_lft forever\n7: veth1@if6: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state LOWERLAYERDOWN group default qlen 1000\n    link/ether ea:37:ea:92:a5:c5 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet 11.0.0.2/24 scope global veth1\n       valid_lft forever preferred_lft forever\n\n### 给网桥分配IP，并开启veth0设备\nroot@ubuntu-xenial:~# ip netns exec xiemx1 bash\nroot@ubuntu-xenial:~# ifconfig xiemx-br 11.0.0.1/24\nroot@ubuntu-xenial:~# ifconfig veth0 up\nroot@ubuntu-xenial:~# ip add\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n5: xiemx-veth1@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000\n    link/ether 42:6a:cb:19:0d:2a brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet 10.0.0.1/24 scope global xiemx-veth1\n       valid_lft forever preferred_lft forever\n    inet6 fe80::406a:cbff:fe19:d2a/64 scope link\n       valid_lft forever preferred_lft forever\n6: veth0@if7: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master xiemx-br state UP group default qlen 1000\n    link/ether 3e:72:e3:48:25:69 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet6 fe80::3c72:e3ff:fe48:2569/64 scope link\n       valid_lft forever preferred_lft forever\n8: xiemx-br: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000\n    link/ether 3e:72:e3:48:25:69 brd ff:ff:ff:ff:ff:ff\n    inet 11.0.0.1/24 brd 11.0.0.255 scope global xiemx-br\n       valid_lft forever preferred_lft forever\n    inet6 fe80::3c72:e3ff:fe48:2569/64 scope link\n       valid_lft forever preferred_lft forever\n       \n### 测试网络联通\nroot@ubuntu-xenial:~# ping 11.0.0.2\nPING 11.0.0.2 (11.0.0.2) 56(84) bytes of data.\n64 bytes from 11.0.0.2: icmp_seq=1 ttl=64 time=0.244 ms\n64 bytes from 11.0.0.2: icmp_seq=2 ttl=64 time=0.047 ms\n64 bytes from 11.0.0.2: icmp_seq=3 ttl=64 time=0.048 ms\n64 bytes from 11.0.0.2: icmp_seq=4 ttl=64 time=0.047 ms\n64 bytes from 11.0.0.2: icmp_seq=5 ttl=64 time=0.051 ms\n^C\n--- 11.0.0.2 ping statistics ---\n5 packets transmitted, 5 received, 0% packet loss, time 4005ms\nrtt min/avg/max/mdev = 0.047/0.087/0.244/0.078 ms\n```\n\n\n\n","tags":["network","linux","docker"],"categories":["linux"]},{"title":"k8s Flannel","url":"//2020/01/21/k8s-fannel/","content":"#### K8S Flannel hostgw\n\nFlannel 的网络个人理解为，flannel接管了所有k8s node节点上的docker 网络的配置，在docker 启动之前，flannel通过在etcd 中共享 flannel的subnet等网段信息来给每个node的docker 预设网络信息，以及分配子网段和bridge地址，以保证在分布式的环境下不会出现网络冲突，因此flannel 可以看作是侵入了docker层面，在底层系统启动container的时候就处理了网络相关，构建了一个大内网。在宿主机的docker控制了所有节点的bridge，并更新所有node上的网段信息的对应路由表，在同一个大网段内，来保证网络连通性。\n\n-----\n\n以下配置参考信息转自：https://jimmysong.io/kubernetes-handbook/concepts/flannel.html\n\n```bash\n[root@node1 ~]# kubectl get nodes -o wide\nNAME      STATUS    ROLES     AGE       VERSION   EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME\nnode1     Ready     <none>    2d        v1.9.1    <none>        CentOS Linux 7 (Core)   3.10.0-693.11.6.el7.x86_64   docker://1.12.6\nnode2     Ready     <none>    2d        v1.9.1    <none>        CentOS Linux 7 (Core)   3.10.0-693.11.6.el7.x86_64   docker://1.12.6\nnode3     Ready     <none>    2d        v1.9.1    <none>        CentOS Linux 7 (Core)   3.10.0-693.11.6.el7.x86_64   docker://1.12.6\nCopy\n```\n\n当前Kubernetes集群中运行的所有Pod信息：\n\n```bash\n[root@node1 ~]# kubectl get pods --all-namespaces -o wide\nNAMESPACE     NAME                                              READY     STATUS    RESTARTS   AGE       IP            NODE\nkube-system   coredns-5984fb8cbb-sjqv9                          1/1       Running   0          1h        172.33.68.2   node1\nkube-system   coredns-5984fb8cbb-tkfrc                          1/1       Running   1          1h        172.33.96.3   node3\nkube-system   heapster-v1.5.0-684c7f9488-z6sdz                  4/4       Running   0          1h        172.33.31.3   node2\nkube-system   kubernetes-dashboard-6b66b8b96c-mnm2c             1/1       Running   0          1h        172.33.31.2   node2\nkube-system   monitoring-influxdb-grafana-v4-54b7854697-tw9cd   2/2       Running   2          1h        172.33.96.2   node3\nCopy\n```\n\n当前etcd中的注册的宿主机的pod地址网段信息：\n\n```bash\n[root@node1 ~]# etcdctl ls /kube-centos/network/subnets\n/kube-centos/network/subnets/172.33.68.0-24\n/kube-centos/network/subnets/172.33.31.0-24\n/kube-centos/network/subnets/172.33.96.0-24\nCopy\n```\n\n而每个node上的Pod子网是根据我们在安装flannel时配置来划分的，在etcd中查看该配置：\n\n```bash\n[root@node1 ~]# etcdctl get /kube-centos/network/config\n{\"Network\":\"172.33.0.0/16\",\"SubnetLen\":24,\"Backend\":{\"Type\":\"host-gw\"}}\nCopy\n```\n\n我们知道Kubernetes集群内部存在三类IP，分别是：\n\n- Node IP：宿主机的IP地址\n- Pod IP：使用网络插件创建的IP（如flannel），使跨主机的Pod可以互通\n- Cluster IP：虚拟IP，通过iptables规则访问服务\n\n在安装node节点的时候，节点上的进程是按照flannel -> docker -> kubelet -> kube-proxy的顺序启动的，我们下面也会按照该顺序来讲解，flannel的网络划分和如何与docker交互，如何通过iptables访问service。\n\n### Flannel\n\nFlannel是作为一个二进制文件的方式部署在每个node上，主要实现两个功能：\n\n- 为每个node分配subnet，容器将自动从该子网中获取IP地址\n- 当有node加入到网络中时，为每个node增加路由配置\n\n下面是使用`host-gw` backend的flannel网络架构图：\n\n[![flannel网络架构（图片来自openshift）](../images/flannel-networking-20200121145349208.png)](https://jimmysong.io/kubernetes-handbook/images/flannel-networking.png)图片 - flannel网络架构（图片来自openshift）。以上IP非本示例中的IP。\n\nNode1上的flannel配置如下:\n\n```bash\n[root@node1 ~]# cat /usr/lib/systemd/system/flanneld.service\n[Unit]\nDescription=Flanneld overlay address etcd agent\nAfter=network.target\nAfter=network-online.target\nWants=network-online.target\nAfter=etcd.service\nBefore=docker.service\n\n[Service]\nType=notify\nEnvironmentFile=/etc/sysconfig/flanneld\nEnvironmentFile=-/etc/sysconfig/docker-network\nExecStart=/usr/bin/flanneld-start $FLANNEL_OPTIONS\nExecStartPost=/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker\nRestart=on-failure\n\n[Install]\nWantedBy=multi-user.target\nRequiredBy=docker.service\nCopy\n```\n\n其中有两个环境变量文件的配置如下：\n\n```bash\n[root@node1 ~]# cat /etc/sysconfig/flanneld\n# Flanneld configuration options\nFLANNEL_ETCD_ENDPOINTS=\"http://172.17.8.101:2379\"\nFLANNEL_ETCD_PREFIX=\"/kube-centos/network\"\nFLANNEL_OPTIONS=\"-iface=eth2\"\nCopy\n```\n\n上面的配置文件仅供flanneld使用。\n\n```bash\n[root@node1 ~]# cat /etc/sysconfig/docker-network\n# /etc/sysconfig/docker-network\nDOCKER_NETWORK_OPTIONS=\nCopy\n```\n\n还有一个`ExecStartPost=/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker`，其中的`/usr/libexec/flannel/mk-docker-opts.sh`脚本是在flanneld启动后运行，将会生成两个环境变量配置文件：\n\n- /run/flannel/docker\n- /run/flannel/subnet.env\n\n我们再来看下`/run/flannel/docker`的配置。\n\n```bash\n[root@node1 ~]# cat /run/flannel/docker\nDOCKER_OPT_BIP=\"--bip=172.33.68.1/24\"\nDOCKER_OPT_IPMASQ=\"--ip-masq=true\"\nDOCKER_OPT_MTU=\"--mtu=1500\"\nDOCKER_NETWORK_OPTIONS=\" --bip=172.33.68.1/24 --ip-masq=true --mtu=1500\"\nCopy\n```\n\n如果你使用`systemctl`命令先启动flannel后启动docker的话，docker将会读取以上环境变量。\n\n我们再来看下`/run/flannel/subnet.env`的配置。\n\n```bash\n[root@node1 ~]# cat /run/flannel/subnet.env\nFLANNEL_NETWORK=172.33.0.0/16\nFLANNEL_SUBNET=172.33.68.1/24\nFLANNEL_MTU=1500\nFLANNEL_IPMASQ=false\nCopy\n```\n\n以上环境变量是flannel向etcd中注册的。\n\n### Docker\n\nNode1的docker配置如下：\n\n```bash\n[root@node1 ~]# cat /usr/lib/systemd/system/docker.service\n[Unit]\nDescription=Docker Application Container Engine\nDocumentation=http://docs.docker.com\nAfter=network.target rhel-push-plugin.socket registries.service\nWants=docker-storage-setup.service\nRequires=docker-cleanup.timer\n\n[Service]\nType=notify\nNotifyAccess=all\nEnvironmentFile=-/run/containers/registries.conf\nEnvironmentFile=-/etc/sysconfig/docker\nEnvironmentFile=-/etc/sysconfig/docker-storage\nEnvironmentFile=-/etc/sysconfig/docker-network\nEnvironment=GOTRACEBACK=crash\nEnvironment=DOCKER_HTTP_HOST_COMPAT=1\nEnvironment=PATH=/usr/libexec/docker:/usr/bin:/usr/sbin\nExecStart=/usr/bin/dockerd-current \\\n          --add-runtime docker-runc=/usr/libexec/docker/docker-runc-current \\\n          --default-runtime=docker-runc \\\n          --exec-opt native.cgroupdriver=systemd \\\n          --userland-proxy-path=/usr/libexec/docker/docker-proxy-current \\\n          $OPTIONS \\\n          $DOCKER_STORAGE_OPTIONS \\\n          $DOCKER_NETWORK_OPTIONS \\\n          $ADD_REGISTRY \\\n          $BLOCK_REGISTRY \\\n          $INSECURE_REGISTRY\\\n          $REGISTRIES\nExecReload=/bin/kill -s HUP $MAINPID\nLimitNOFILE=1048576\nLimitNPROC=1048576\nLimitCORE=infinity\nTimeoutStartSec=0\nRestart=on-abnormal\nMountFlags=slave\nKillMode=process\n\n[Install]\nWantedBy=multi-user.target\nCopy\n```\n\n查看Node1上的docker启动参数：\n\n```bash\n[root@node1 ~]# systemctl status -l docker\n● docker.service - Docker Application Container Engine\n   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)\n  Drop-In: /usr/lib/systemd/system/docker.service.d\n           └─flannel.conf\n   Active: active (running) since Fri 2018-02-02 22:52:43 CST; 2h 28min ago\n     Docs: http://docs.docker.com\n Main PID: 4334 (dockerd-current)\n   CGroup: /system.slice/docker.service\n           ‣ 4334 /usr/bin/dockerd-current --add-runtime docker-runc=/usr/libexec/docker/docker-runc-current --default-runtime=docker-runc --exec-opt native.cgroupdriver=systemd --userland-proxy-path=/usr/libexec/docker/docker-proxy-current --selinux-enabled --log-driver=journald --signature-verification=false --bip=172.33.68.1/24 --ip-masq=true --mtu=1500\nCopy\n```\n\n我们可以看到在docker在启动时有如下参数：`--bip=172.33.68.1/24 --ip-masq=true --mtu=1500`。上述参数flannel启动时运行的脚本生成的，通过环境变量传递过来的。\n\n我们查看下node1宿主机上的网络接口：\n\n```bash\n[root@node1 ~]# ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000\n    link/ether 52:54:00:00:57:32 brd ff:ff:ff:ff:ff:ff\n    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0\n       valid_lft 85095sec preferred_lft 85095sec\n    inet6 fe80::5054:ff:fe00:5732/64 scope link\n       valid_lft forever preferred_lft forever\n3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000\n    link/ether 08:00:27:7b:0f:b1 brd ff:ff:ff:ff:ff:ff\n    inet 172.17.8.101/24 brd 172.17.8.255 scope global eth1\n       valid_lft forever preferred_lft forever\n4: eth2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000\n    link/ether 08:00:27:ef:25:06 brd ff:ff:ff:ff:ff:ff\n    inet 172.30.113.231/21 brd 172.30.119.255 scope global dynamic eth2\n       valid_lft 85096sec preferred_lft 85096sec\n    inet6 fe80::a00:27ff:feef:2506/64 scope link\n       valid_lft forever preferred_lft forever\n5: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP\n    link/ether 02:42:d0:ae:80:ea brd ff:ff:ff:ff:ff:ff\n    inet 172.33.68.1/24 scope global docker0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::42:d0ff:feae:80ea/64 scope link\n       valid_lft forever preferred_lft forever\n7: veth295bef2@if6: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP\n    link/ether 6a:72:d7:9f:29:19 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet6 fe80::6872:d7ff:fe9f:2919/64 scope link\n       valid_lft forever preferred_lft forever\nCopy\n```\n\n我们分类来解释下该虚拟机中的网络接口。\n\n- lo：回环网络，127.0.0.1\n\n- eth0：NAT网络，虚拟机创建时自动分配，仅可以在几台虚拟机之间访问\n\n- eth1：bridge网络，使用vagrant分配给虚拟机的地址，虚拟机之间和本地电脑都可以访问\n\n- eth2：bridge网络，使用DHCP分配，用于访问互联网的网卡\n\n- docker0：bridge网络，docker默认使用的网卡，作为该节点上所有容器的虚拟交换机\n\n- veth295bef2@if6：veth pair，连接docker0和Pod中的容器。veth pair可以理解为使用网线连接好的两个接口，把两个端口放到两个namespace中，那么这两个namespace就能打通。\n\n  \n\n```bash\n[root@node1 ~]# docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\n940bb75e653b        bridge              bridge              local\nd94c046e105d        host                host                local\n2db7597fd546        none                null                local\nCopy\n```\n\n再检查下bridge网络`940bb75e653b`的信息。\n\n```bash\n[root@node1 ~]# docker network inspect 940bb75e653b\n[\n    {\n        \"Name\": \"bridge\",\n        \"Id\": \"940bb75e653bfa10dab4cce8813c2b3ce17501e4e4935f7dc13805a61b732d2c\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"EnableIPv6\": false,\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Options\": null,\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.33.68.1/24\",\n                    \"Gateway\": \"172.33.68.1\"\n                }\n            ]\n        },\n        \"Internal\": false,\n        \"Containers\": {\n            \"944d4aa660e30e1be9a18d30c9dcfa3b0504d1e5dbd00f3004b76582f1c9a85b\": {\n                \"Name\": \"k8s_POD_coredns-5984fb8cbb-sjqv9_kube-system_c5a2e959-082a-11e8-b4cd-525400005732_0\",\n                \"EndpointID\": \"7397d7282e464fc4ec5756d6b328df889cdf46134dbbe3753517e175d3844a85\",\n                \"MacAddress\": \"02:42:ac:21:44:02\",\n                \"IPv4Address\": \"172.33.68.2/24\",\n                \"IPv6Address\": \"\"\n            }\n        },\n        \"Options\": {\n            \"com.docker.network.bridge.default_bridge\": \"true\",\n            \"com.docker.network.bridge.enable_icc\": \"true\",\n            \"com.docker.network.bridge.enable_ip_masquerade\": \"true\",\n            \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\",\n            \"com.docker.network.bridge.name\": \"docker0\",\n            \"com.docker.network.driver.mtu\": \"1500\"\n        },\n        \"Labels\": {}\n    }\n]\nCopy\n```\n\n我们可以看到该网络中的`Config`与docker的启动配置相符。\n\nNode1上运行的容器：\n\n```bash\n[root@node1 ~]# docker ps\nCONTAINER ID        IMAGE                                                                                               COMMAND                  CREATED             STATUS              PORTS               NAMES\na37407a234dd        docker.io/coredns/coredns@sha256:adf2e5b4504ef9ffa43f16010bd064273338759e92f6f616dd159115748799bc   \"/coredns -conf /etc/\"   About an hour ago   Up About an hour                        k8s_coredns_coredns-5984fb8cbb-sjqv9_kube-system_c5a2e959-082a-11e8-b4cd-525400005732_0\n944d4aa660e3        docker.io/openshift/origin-pod                                                                      \"/usr/bin/pod\"           About an hour ago   Up About an hour                        k8s_POD_coredns-5984fb8cbb-sjqv9_kube-system_c5a2e959-082a-11e8-b4cd-525400005732_0\nCopy\n```\n\n我们可以看到当前已经有2个容器在运行。\n\nNode1上的路由信息：\n\n```bash\n[root@node1 ~]# route -n\nKernel IP routing table\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\n0.0.0.0         10.0.2.2        0.0.0.0         UG    100    0        0 eth0\n0.0.0.0         172.30.116.1    0.0.0.0         UG    101    0        0 eth2\n10.0.2.0        0.0.0.0         255.255.255.0   U     100    0        0 eth0\n172.17.8.0      0.0.0.0         255.255.255.0   U     100    0        0 eth1\n172.30.112.0    0.0.0.0         255.255.248.0   U     100    0        0 eth2\n172.33.68.0     0.0.0.0         255.255.255.0   U     0      0        0 docker0\n172.33.96.0     172.3.65   255.255.255.0   UG    0      0        0 eth2\nCopy\n```\n\n以上路由信息是由flannel添加的，当有新的节点加入到Kubernetes集群中后，每个节点上的路由表都将增加。\n\n我们在node上来`traceroute`下node3上的`coredns-5984fb8cbb-tkfrc`容器，其IP地址是`172.33.96.3`，看看其路由信息。\n\n```bash\n[root@node1 ~]# traceroute 172.33.96.3\ntraceroute to 172.33.96.3 (172.33.96.3), 30 hops max, 60 byte packets\n 1  172.30.118.65 (172.30.118.65)  0.518 ms  0.367 ms  0.398 ms\n 2  172.33.96.3 (172.33.96.3)  0.451 ms  0.352 ms  0.223 ms\nCopy\n```\n\n我们看到路由直接经过node3的公网IP后就到达了node3节点上的Pod。\n\nNode1的iptables信息：\n\n```bash\n[root@node1 ~]# iptables -L\nChain INPUT (policy ACCEPT)\ntarget     prot opt source               destination\nKUBE-FIREWALL  all  --  anywhere             anywhere\nKUBE-SERVICES  all  --  anywhere             anywhere             /* kubernetes service portals */\n\nChain FORWARD (policy ACCEPT)\ntarget     prot opt source               destination\nKUBE-FORWARD  all  --  anywhere             anywhere             /* kubernetes forward rules */\nDOCKER-ISOLATION  all  --  anywhere             anywhere\nDOCKER     all  --  anywhere             anywhere\nACCEPT     all  --  anywhere             anywhere             ctstate RELATED,ESTABLISHED\nACCEPT     all  --  anywhere             anywhere\nACCEPT     all  --  anywhere             anywhere\n\nChain OUTPUT (policy ACCEPT)\ntarget     prot opt source               destination\nKUBE-FIREWALL  all  --  anywhere             anywhere\nKUBE-SERVICES  all  --  anywhere             anywhere             /* kubernetes service portals */\n\nChain DOCKER (1 references)\ntarget     prot opt source               destination\n\nChain DOCKER-ISOLATION (1 references)\ntarget     prot opt source               destination\nRETURN     all  --  anywhere             anywhere\n\nChain KUBE-FIREWALL (2 references)\ntarget     prot opt source               destination\nDROP       all  --  anywhere             anywhere             /* kubernetes firewall for dropping marked packets */ mark match 0x8000/0x8000\n\nChain KUBE-FORWARD (1 references)\ntarget     prot opt source               destination\nACCEPT     all  --  anywhere             anywhere             /* kubernetes forwarding rules */ mark match 0x4000/0x4000\nACCEPT     all  --  10.254.0.0/16        anywhere             /* kubernetes forwarding conntrack pod source rule */ ctstate RELATED,ESTABLISHED\nACCEPT     all  --  anywhere             10.254.0.0/16        /* kubernetes forwarding conntrack pod destination rule */ ctstate RELATED,ESTABLISHED\n\nChain KUBE-SERVICES (2 references)\ntarget     prot opt source               destination\nCopy\n```\n\n","tags":["k8s","network","flannel"],"categories":["k8s"]},{"title":"k8s pause container","url":"//2020/01/20/k8s-pause-container/","content":"### pause container\n\nPause容器，也被称为infra容器，kubelet 启动是可以通过参数指定image`--pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0` \n\n在Unix系统中，PID为1的进程为init进程，即所有进程的父进程。它会维护一张进程表，不断地检查进程状态，来管理子进程。init 不会响应系统信号，可以防止init进程被误杀。\n\npause容器的架构图：\n\n![The pause container](/images/pause_container.png)\n\nkubernetes中的pause容器的功能：\n\n- 创建命名空间基础，给予后续容器使用\n- 创建init进程作为父进程来接管后续容器的进程，保证进程安全和回收\n\n-----\n\n参考：https://www.ianlewis.org/en/almighty-pause-container\n\n","tags":["k8s","docker"],"categories":["k8s"]},{"title":"Nginx URI参数%xx字符解码","url":"//2020/01/09/nginx-decode-uri/","content":"#### Nginx URI参数%xx字符解码\n\n对URI参数值中的`%XX`这样的编码序列进行解码，可以使用第三方 `ngx_set_misc` 模块提供的 `set_unescape_uri` 指令：\n\n```\n    location /decode {\n        set_unescape_uri $name $arg_name;\n        set_unescape_uri $class $arg_class;\n\n        echo \"name: $name\";\n        echo \"class: $class\";\n    }\n\n    location /test {\n      echo \"name: $arg_name\";\n      echo \"class: $arg_class\";\n    }\n\n\n$ curl 'http://localhost/test?name=hello%20xiemx&class=1'\nname: hello%20xiemx\nclass: 1\n\n$ curl 'http://localhost/decode?name=hello%20xiemx&class=1'\nname: hello xiemx\nclass: 1\n\n```","tags":["webserver","nginx"],"categories":["nginx"]},{"title":"如何在K8S环境中抓POD的包","url":"//2019/12/12/k8s-tcpdump/","content":"### 如何在K8S环境中抓POD的包\n\n1. kubectl get pod -o wide 获取pod所在的node信息\n```shell\n➜  Documents kubectl get pod -o wide \nNAME                                                 READY   STATUS             RESTARTS   AGE   IP             NODE                                              NOMINATED NODE\ninternal-nginx-ingress-controller-7fdf7f457d-bd59z   1/1     Running            0          42m   10.200.1.83    ip-10-200-1-202.ap-northeast-1.compute.internal   <none>\n\n2. kubectl describe pod/podname 获取pod的containid\n```shell\n➜  Documents k get pod -o jsonpath='{.status.containerStatuses[*].containerID}' internal-nginx-ingress-controller-7fdf7f457d-bd59z\ndocker://ae9a6df60584e797e56cc64d0df02e64d7731a0d852026fab0a76c920c608cbe\n```\n\n3. 登陆node节点，找到container查看eth0网卡的ID\n```shell\n[ec2-user@ip-10-200-1-202 net]$ docker exec -it ae9a6df60584e797e56cc64d0df02e64d7731a0d852026fab0a76c920c608cbe cat /sys/class/net/eth0/iflink\n88\n```\n\n4. 宿主机上查询对应ID的网卡设备号\n```\n[ec2-user@ip-10-200-1-202 net]$ cd /sys/class/net; for i in $(ls);do echo $i ;grep 88 $i/ifindex;done\neni0143b083c86\neni154a5470c40\neni1c162323f07\neni1d3e2ba2ce1\neni3fbceb3330b\neni457702aeb41\neni45f360a240e\neni50431e3a94f\neni619e29d4bac\neni66339821adf\neni6fe679d6356\neni79708a78f8b\neni7cc26b0b7d2\neni855ca0ba49b\neni8799376f27c\neni90208382a7b\neni909411bbf11\neni94c3d2bb833\nenia14f5f7c3e9\nenib70b44b2399\nenic2ad9523b38 ###容器所属网卡\n88\nenid60e48c6616\nenid8f13b5dd06\nenida858799e91\nenieee4f7696a1\nenif0d5e81d420\neth0\neth1\nlo\n```\n\n5. tcpdump 抓包即可\n```shell\n➜  ssh-keys git:(master) ✗ ssh -F ~/.matrix/jp/ssh.aux.config 10.200.1.202 -l ec2-user \"sudo tcpdump -vvv -i enic2ad9523b38 tcp port 80 -w -\" | wireshark -k -i -\nWarning: Permanently added '10.200.1.4' (ECDSA) to the list of known hosts.\nWarning: Permanently added '10.200.1.202' (ECDSA) to the list of known hosts.\ntcpdump: listening on eni86f5b593a42, link-type EN10MB (Ethernet), capture size 262144 bytes\ntcpdump: pcap_loop: The interface went down\n7408 packets captured\n7408 packets received by filter\n0 packets dropped by kernel\n```","tags":["k8s","network","linux","tcpdump"],"categories":["k8s"]},{"title":"nginx remote_port为空","url":"//2019/12/12/nginx-remote-port-is-null/","content":"### nginx remote_port为空\n\n#### 现象\n由于网安要求记录用户请求来源端口到日志中，因此为了实现这个需求在log_format中增加了\"ngx_remote_port: $remote_port\"字段(如下)， 但实际日志系统中收录到的日志`ngx_remote_port:\"\"`为空\n```\nlog_format json '{ \"time\": \"$time_iso8601\", '\n                    '\"ngx_true_client_ip\": \"$http_true_client_ip\", '\n                    '\"ngx_x_real_ip\": \"$http_x_real_ip\", '\n                    '\"ngx_cdn_src_ip\": \"$http_cdn_src_ip\", '\n                    '\"ngx_geo_location_ip\": \"$client_ip\", '\n                    '\"ngx_remote_addr\": \"$remote_addr\", '\n                    '\"ngx_remote_port\": \"$remote_port\", '\n                    '\"ngx_x_user_site_proxy\": \"$http_x_user_site_proxy\", '\n                    '\"ngx_x_forwarded_for\": \"$http_x_forwarded_for\", '\n                    '\"ngx_host\": \"$host\", '\n                    '\"ngx_http_user_agent\": \"$http_user_agent\", '\n                    '\"ngx_body_bytes_sent\": \"$body_bytes_sent\", '\n                    '\"ngx_request_time\": \"$request_time\", '\n                    '\"ngx_upstream_response_time\": \"$upstream_response_time\", '\n                    '\"ngx_upstream_connect_time\": \"$upstream_connect_time\", '\n                    '\"ngx_upstream_header_time:\": \"$upstream_header_time\", '\n                    '\"ngx_upstream_addr\": \"$upstream_addr\", '\n                    '\"ngx_upstream_content_length\": \"$sent_http_content_length\", '\n                    '\"ngx_status_code\": \"$status\", '\n                    '\"ngx_scheme\": \"$scheme\", '\n                    '\"ngx_request_method\": \"$request_method\", '\n                    '\"ngx_request_uri\": \"$request_uri\", '\n                    '\"ngx_request\": \"$request\", '\n                    '\"ngx_http_referrer\": \"$http_referer\"  }';\n```\n\n#### 解决\n\n由于$remote_port这个变量是nginx核心模块提供的，因此猜测是由第三方模块再次操作导致为空，经过测试发现通过proxy进来的请求remote_port为空，  \n而直接访问本地nginx的请求都能够正确获取port信息，对于以上2中情况的对比\n怀疑是由于经过proxy之类的组建request header中有x-forworder-for的头，从而触发了realip 模块（这个怀疑没有具体验证），解决方案就是使用realip模块提供的变量'realip_remote_port'来记录来源port, 修改log_format\n```\nlog_format json '{ \"time\": \"$time_iso8601\", '\n                    '\"ngx_true_client_ip\": \"$http_true_client_ip\", '\n                    '\"ngx_x_real_ip\": \"$http_x_real_ip\", '\n                    '\"ngx_cdn_src_ip\": \"$http_cdn_src_ip\", '\n                    '\"ngx_geo_location_ip\": \"$client_ip\", '\n                    '\"ngx_remote_addr\": \"$remote_addr\", '\n                    '\"ngx_remote_port\": \"$remote_port\", '\n```","tags":["webserver","nginx"],"categories":["nginx"]},{"title":"k8s ingress-nginx动态balance实现解析","url":"//2019/09/16/k8s-ingress-nginx/","content":"只节选了比较关键的代码，删除了比较多的干扰项。纯属个人理解！！！\n\n#### 1. 初始化balancer.init_worker()，使用balancer.balance()动态获取\n```lua\nhttp {\n        lua_package_path \"/etc/nginx/lua/?.lua;/etc/nginx/lua/vendor/?.lua;/usr/local/lib/lua/?.lua;;\";\n        init_by_lua_block {\n                ok, res = pcall(require, \"configuration\")\n        }\n\n        init_worker_by_lua_block {\n                balancer.init_worker()  #####创建定时任务 ngx.timer.every(BACKENDS_SYNC_INTERVAL, sync_backends)\n        }\n     \n###upstream configure\nupstream upstream_balancer {\n\t\tserver 0.0.0.1; # placeholder\n\n\t\tbalancer_by_lua_block {\n\t\t\tbalancer.balance()\n\t\t}\n\t\tkeepalive 32;\n\t\tkeepalive_timeout  60s;\n\t\tkeepalive_requests 100;\n\t}\n```\n#### 2. 获取backend信息，balancer.init_worker()\n```lua\n####https://sourcegraph.com/github.com/kubernetes/ingress-nginx@dd0fe4b458cc5520f25eb8bba25bbe6f0c72ee98/-/blob/rootfs/etc/nginx/lua/balancer.lua?utm_source=share#L223\n\nlocal configuration = require(\"configuration\")\n\nfunction _M.init_worker()\n  sync_backends() -- when worker starts, sync backends without delay\n  local _, err = ngx.timer.every(BACKENDS_SYNC_INTERVAL, sync_backends)\n  if err then\n    ngx.log(ngx.ERR, string.format(\"error when setting up timer.every for sync_backends: %s\", tostring(err)))\n  end\nend\n\n\nlocal function sync_backends()\n  local backends_data = configuration.get_backends_data()\n\n  local new_backends, err = cjson.decode(backends_data)\n\n\n  local balancers_to_keep = {}\n  for _, new_backend in ipairs(new_backends) do\n    sync_backend(new_backend)\n    balancers_to_keep[new_backend.name] = balancers[new_backend.name]\n  end\nend\n```\n```lua\n####https://sourcegraph.com/github.com/kubernetes/ingress-nginx@dd0fe4b458cc5520f25eb8bba25bbe6f0c72ee98/-/blob/rootfs/etc/nginx/lua/configuration.lua?utm_source=share#L10:10\n\nlocal configuration_data = ngx.shared.configuration_data\nlocal certificate_data = ngx.shared.certificate_data\n\nlocal _M = {}\n\nfunction _M.get_backends_data()\n  return configuration_data:get(\"backends\")\nend\n\nfunction _M.call()\n  if ngx.var.request_method ~= \"POST\" and ngx.var.request_method ~= \"GET\" then\n    ngx.status = ngx.HTTP_BAD_REQUEST\n    ngx.print(\"Only POST and GET requests are allowed!\")\n    return\n  end\n\n  if ngx.var.request_uri == \"/configuration/servers\" then\n    handle_servers()\n    return\n  end\n\n  if ngx.var.request_uri == \"/configuration/general\" then\n    handle_general()\n    return\n  end\n\n  if ngx.var.uri == \"/configuration/certs\" then\n    handle_certs()\n    return\n  end\n\n  if ngx.var.request_uri ~= \"/configuration/backends\" then ####只接受以上4类型URL\n    ngx.status = ngx.HTTP_NOT_FOUND\n    ngx.print(\"Not found!\")\n    return\n  end\n\n  local backends = fetch_request_body()\n\n  local success, err = configuration_data:set(\"backends\", backends)\n\nend\n```\n```lua\n### fetch_request_body()，从此函数可以看出此函数是一个外部调用，可以得出原始的数据来源为外部触发的POST，可以查询Call()函数的引用位置\nlocal function fetch_request_body()\n  ngx.req.read_body() ###防止ngx.req.get_body_data()返回nil,显示执行一下\n  local body = ngx.req.get_body_data()\n\n  if not body then\n    local file_name = ngx.req.get_body_file() ###读取cache file\n\n    local file = io.open(file_name, \"rb\")\n    if not file then\n      return nil\n    end\n\n    body = file:read(\"*all\")\n    file:close()\n  end\n\n  return body\nend\n```\n```\n####nginx.conf 查看nginx配置文件中显示调用call()函数的位置为当前server切url /configuration 符合函数要求，在查找外部调用的代码（基本可以定位为控制器的逻辑控制）\n        server {\n                listen unix:/tmp/nginx-status-server.sock;\n                set $proxy_upstream_name \"internal\";\n\n                keepalive_timeout 0;\n                gzip off;\n\n                access_log off;\n\n                location /configuration {\n                        # this should be equals to configuration_data dict\n                        client_max_body_size                    10m;\n                        client_body_buffer_size                 10m;\n                        proxy_buffering                         off;\n\n                        content_by_lua_block {\n                                configuration.call()\n                        }\n                }\n```\n#### 3. 通过上面的信息检索，在Ingress中监听pod变化信息，动态调用/configuration/backends, 函数为configureBackends()\n```go\n####https://github.com/kubernetes/ingress-nginx/blob/ce3e3d51c397ff6a0cd6731cc64360ecdb69ea54/internal/ingress/controller/nginx.go#L982\nfunc configureBackends(rawBackends []*ingress.Backend) error {\n\tbackends := make([]*ingress.Backend, len(rawBackends))\n\n\tfor i, backend := range rawBackends {\n\t\tvar service *apiv1.Service\n\t\tif backend.Service != nil {\n\t\t\tservice = &apiv1.Service{Spec: backend.Service.Spec}\n\t\t}\n\t\tluaBackend := &ingress.Backend{\n\t\t\tName:                 backend.Name,\n\t\t\tPort:                 backend.Port,\n\t\t\tSSLPassthrough:       backend.SSLPassthrough,\n\t\t\tSessionAffinity:      backend.SessionAffinity,\n\t\t\tUpstreamHashBy:       backend.UpstreamHashBy,\n\t\t\tLoadBalancing:        backend.LoadBalancing,\n\t\t\tService:              service,\n\t\t\tNoServer:             backend.NoServer,\n\t\t\tTrafficShapingPolicy: backend.TrafficShapingPolicy,\n\t\t\tAlternativeBackends:  backend.AlternativeBackends,\n\t\t}\n\n\t\tvar endpoints []ingress.Endpoint\n\t\tfor _, endpoint := range backend.Endpoints {\n\t\t\tendpoints = append(endpoints, ingress.Endpoint{\n\t\t\t\tAddress: endpoint.Address,\n\t\t\t\tPort:    endpoint.Port,\n\t\t\t})\n\t\t}\n\n\t\tluaBackend.Endpoints = endpoints\n\t\tbackends[i] = luaBackend\n\t}\n\n\tstatusCode, _, err := nginx.NewPostStatusRequest(\"/configuration/backends\", \"application/json\", backends) ####backends 为request.body,却内容为IP/PORT，以下给出了backend的struct\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif statusCode != http.StatusCreated {\n\t\treturn fmt.Errorf(\"unexpected error code: %d\", statusCode)\n\t}\n\n\treturn nil\n}\n\n####Backend struct\ntype Backend struct {\n\tName    string             `json:\"name\"`\n\tService *apiv1.Service     `json:\"service,omitempty\"`\n\tPort    intstr.IntOrString `json:\"port\"`\n\tSecureCACert resolver.AuthSSLCert `json:\"secureCACert\"`\n\tSSLPassthrough bool `json:\"sslPassthrough\"`\n\tEndpoints []Endpoint `json:\"endpoints,omitempty\"`\n\tSessionAffinity SessionAffinityConfig `json:\"sessionAffinityConfig\"`\n\tUpstreamHashBy UpstreamHashByConfig `json:\"upstreamHashByConfig,omitempty\"`\n\tLoadBalancing string `json:\"load-balance,omitempty\"`\n\tNoServer bool `json:\"noServer\"`\n\tTrafficShapingPolicy TrafficShapingPolicy `json:\"trafficShapingPolicy,omitempty\"`\n\tAlternativeBackends []string `json:\"alternativeBackends,omitempty\"`\n}\n```\n#### 4. ngx\\_balancer.set\\_current_peer()设置backend信息\n```lua\n####https://sourcegraph.com/github.com/kubernetes/ingress-nginx@dd0fe4b458cc5520f25eb8bba25bbe6f0c72ee98/-/blob/rootfs/etc/nginx/lua/balancer.lua?utm_source=share#L232:13\n\nfunction _M.balance()\n  local balancer = get_balancer()\n  if not balancer then\n    return\n  end\n\n  local peer = balancer:balance()\n  if not peer then\n    ngx.log(ngx.WARN, \"no peer was returned, balancer: \" .. balancer.name)\n    return\n  end\n\n  ngx_balancer.set_more_tries(1)\n\n  local ok, err = ngx_balancer.set_current_peer(peer) ####设置server信息\n  if not ok then\n    ngx.log(ngx.ERR, string.format(\"error while setting current upstream peer %s: %s\", peer, err))\n  end\nend\n\n\nlocal function get_balancer()\n  if ngx.ctx.balancer then\n    return ngx.ctx.balancer\n  end\n\n  local backend_name = ngx.var.proxy_upstream_name ###获取当前request上下文中共享的变量proxy_upstream_name\n\n  local balancer = balancers[backend_name] ###获取balancers信息由sync_backend()函数定时轮询\n  if not balancer then\n    return\n  end\n\n  if route_to_alternative_balancer(balancer) then\n    local alternative_backend_name = balancer.alternative_backends[1]\n    ngx.var.proxy_alternative_upstream_name = alternative_backend_name\n\n    balancer = balancers[alternative_backend_name]\n  end\n\n  ngx.ctx.balancer = balancer\n\n  return balancer\nend\n```\n```\n###nginx.conf\nset $proxy_upstream_name    \"dev-dev-auto-deploy-5000\";\nset $proxy_host             $proxy_upstream_name;\n\nproxy_pass http://upstream_balancer;\n```","tags":["k8s","ingress"],"categories":["k8s"]},{"title":"PostgreSQL查看复制状态","url":"//2019/07/08/postgresql-replica-status/","content":"#### postgresql查看复制状态，master上执行\n```sql\n#select * from pg_stat_replication; \npostgres=# select * from pg_stat_replication;\n-[ RECORD 1 ]----+------------------------------\npid              | 13321\nusesysid         | 17019\nusename          | replication\napplication_name | walreceiver\nclient_addr      | 10.0.0.81\nclient_hostname  | \nclient_port      | 42809\nbackend_start    | 2016-08-11 10:57:35.856289+08\nbackend_xmin     | \nstate            | streaming --同步状态\nsent_location    | 1/E0CE9750\nwrite_location   | 1/E0CE9750\nflush_location   | 1/E0CE9750\nreplay_location  | 1/E0CE9750\nsync_priority    | 0\nsync_state       | async  --同步模式\n\nstate: 同步状态\n    streaming : 同步\n    startup : 连接中\n    catchup: 同步中\n\nsync_state: 同步模式.\n    async : 异步\n    sync : 同步\n    potential: 虽然现在是异步,但有可能提升到同步\n```\n#### 查看复制的延迟有多少，字节单位，master上执行 \n```sql\n#select pg_xlog_location_diff(sent_location, replay_location) from pg_stat_replication; \n\nposrgresql=# select pg_xlog_location_diff(sent_location, replay_location) from pg_stat_replication; \n pg_xlog_location_diff \n-----------------------\n                      0\n(1 row)\n```\n#### slave上查看sql滞后时间\n```sql\nSELECT CASE WHEN pg_last_xlog_receive_location() = pg_last_xlog_replay_location()\n    THEN 0\n    ELSE EXTRACT (EPOCH FROM now() - pg_last_xact_replay_timestamp())\n    END AS log_delay;\n\npostgres=# SELECT CASE WHEN pg_last_xlog_receive_location() = pg_last_xlog_replay_location()\npostgres-#     THEN 0\npostgres-#     ELSE EXTRACT (EPOCH FROM now() - pg_last_xact_replay_timestamp())\npostgres-#     END AS log_delay;\n log_delay\n-----------\n         0\n(1 row)\n```\n#### slave上查看是否处于recovery模式\n```sql\nselect pg_is_in_recovery();\npostgres=# select pg_is_in_recovery();\n pg_is_in_recovery\n-------------------\n t\n(1 row)\n```\n#### slave上查看最新的reploy时间戳\n```sql\n#select pg_last_xact_replay_timestamp();\npostgres=# select pg_last_xact_replay_timestamp();\n pg_last_xact_replay_timestamp\n-------------------------------\n 2019-07-08 03:01:33.854131+00\n(1 row)\n```\n\n#### slave上查看最新的reploy位置\n```sql\n#select pg_last_xlog_replay_location();\npostgres=# select pg_last_xlog_replay_location();\n pg_last_xlog_replay_location\n------------------------------\n 220C/56EB4C10\n(1 row)\n```","tags":["database","postgresql"],"categories":["postgresql"]},{"title":"http cache","url":"//2019/05/13/http-cache/","content":"#### **cache流程图**\n\n![img](/images/image.png)\n\n### “no-cache”和“no-store”\n\n“no-cache”表示必须先与服务器确认返回的响应是否发生了变化，然后才能使用该响应来满足后续对同一网址的请求。 因此，如果存在合适的验证令牌 (ETag)，no-cache 会发起往返通信来验证缓存的响应，但如果资源未发生变化，则可避免下载。\n\n“no-store”禁止浏览器以及所有中间缓存存储任何版本的返回响应，例如，包含个人隐私数据或银行业务数据的响应。 每次用户请求该资产时，都会向服务器发送请求，并下载完整的响应。\n\n### “public”与 “private”\n\n“public”则即使它有关联的 HTTP 身份验证，甚至响应状态代码通常无法缓存，也可以缓存响应。 大多数情况下，“public”不是必需的，因为明确的缓存信息（例如“max-age”）已表示响应是可以缓存的。\n\n“private”浏览器可以缓存响应，不允许任何中间缓存对其进行缓存。 例如，用户的浏览器可以缓存包含用户私人信息的 HTML 网页，但 CDN 却不能缓存。\n\n### “max-age”\n\n指令指定从请求的时间开始，允许提取的响应被重用的最长时间（单位：秒）。 例如，“max-age=60”表示可在接下来的 60 秒缓存和重用响应。\n\n## 通过 ETag 验证缓存的响应\n\n在首次请求资源时服务器生成并返回\"ETag\" http请求头(通常是文件内容的哈希值或某个其他指纹)。 当120 秒后，浏览器又对该资源发起了新的请求。 首先，浏览器会检查本地缓存并找到之前的响应。如果发现缓存超过max-age, 浏览器将发起一个带有\"If-None-Match\"的http请求。 如果Etag相同，则返回304，使用本地缓存。\n\n![HTTP Cache-Control 示例](/images/http-cache-control.png)\n\n参考： https://developers.google.com/web/fundamentals/performance/optimizing-content-efficiency/http-caching","tags":["http","cache"],"categories":["http"]},{"title":"Mysqldump error","url":"//2019/05/06/mysqldump-error/","content":"### 现象\n```\n[root@FCHK-instance ~]# mysqldump --host rm-xxxxxxxxxxx.mysql.rds.aliyuncs.com -u xxxx -p --databases visa > hk.sql\nEnter password:\nWarning: A partial dump from a server that has GTIDs will by default include the GTIDs of all transactions, even those that changed suppressed parts of the database. If you don't want to restore GTIDs, pass --set-gtid-purged=OFF. To make a complete dump, pass --all-databases --triggers --routines --events.\nmysqldump: Couldn't execute 'SELECT COLUMN_NAME,                       JSON_EXTRACT(HISTOGRAM, '$.\"number-of-buckets-specified\"')                FROM information_schema.COLUMN_STATISTICS                WHERE SCHEMA_NAME = 'visa' AND TABLE_NAME = 'admin';': Unknown table 'column_statistics' in information_schema (1109\n```\n### 分析\n```\n\n[root@FCHK-instance ~]# mysql --version\nmysql  Ver 8.0.11 for Linux on x86_64 (MySQL Community Server - GPL)\n\n可能是由于mysqldump 8中默认启用（COLUMN_STATISTICS）\n\n官方文档解释\nMysql 8.0 The INFORMATION_SCHEMA COLUMN_STATISTICS Table\nhttps://dev.mysql.com/doc/refman/8.0/en/column-statistics-table.html\n```\n### 解决\n```\n[root@FCHK-instance ~]# mysqldump --host rm-xxxxxxxxxx2.mysql.rds.aliyuncs.com -u xxxx -p --databases visa --column-statistics=0 > hk.sql\nEnter password:\nWarning: A partial dump from a server that has GTIDs will by default include the GTIDs of all transactions, even those that changed suppressed parts of the database. If you don't want to restore GTIDs, pass --set-gtid-purged=OFF. To make a complete dump, pass --all-databases --triggers --routines --events.\n```","tags":["debug","mysql","database"],"categories":["mysql"]},{"title":"redis-dump使用","url":"//2019/05/06/redis-dump/","content":"* 安装redis-dump/redis-load\n\n```shell\n#需要ruby 2.2.2以上版本（可以直接使用ruby:2.2.3的docker images）\ngem install redis-dum\n```\n\n* dump redis\n```shell\nroot@5dba1bd8fa77:/# redis-dump -h\n  Try: /usr/local/bundle/bin/redis-dump show-commands\nUsage: /usr/local/bundle/bin/redis-dump [global options] COMMAND [command options]\n    -u, --uri=S                      Redis URI (e.g. redis://hostname[:port])\n    -d, --database=S                 Redis database (e.g. -d 15)\n    -a, --password=S                 Redis password (e.g. -a 'my@pass/word')\n    -s, --sleep=S                    Sleep for S seconds after dumping (for debugging)\n    -c, --count=S                    Chunk size (default: 10000)\n    -f, --filter=S                   Filter selected keys (passed directly to redis' KEYS command)\n    -b, --base64                     Encode key values as base64 (useful for binary values)\n    -O, --without_optimizations      Disable run time optimizations\n    -V, --version                    Display version\n    -D, --debug\n        --nosafe\n\nroot@5dba1bd8fa77:/# redis-dump -u aux-redis.1uvkyf.0001.cnn1.cache.amazonaws.com.cn > redis-uat.json\n```\n\n* load redis\n```shell\nroot@5dba1bd8fa77:/# redis-load -h\n  Try: /usr/local/bundle/bin/redis-load show-commands\nUsage: /usr/local/bundle/bin/redis-load [global options] COMMAND [command options]\n    -u, --uri=S                      Redis URI (e.g. redis://hostname[:port])\n    -d, --database=S                 Redis database (e.g. -d 15)\n    -a, --password=S                 Redis password (e.g. -a 'my@pass/word')\n    -s, --sleep=S                    Sleep for S seconds after dumping (for debugging)\n    -b, --base64                     Decode key values from base64 (used with redis-dump -b)\n    -n, --no_check_utf8\n    -V, --version                    Display version\n    -D, --debug\n        --nosafe\n\n\nroot@5dba1bd8fa77:/# redis-load -u aux-redis.1uvkyf.0001.cnn1.cache.amazonaws.com.cn:6379/0 < redis-uat.json\n```","tags":["redis"],"categories":["redis"]},{"title":"SSH的AuthorizedKeysCommand、AuthorizedKeysCommandUser","url":"//2019/05/06/ssh-authorizedkeyscommand/","content":"* AuthorizedKeysCommand 可以指定运行一个脚本，而这个脚本主要是寻找登录用户的publickey，默认传参为登录用户名，若未认证成功，将继续使用AuthorizedKeysFile文件来做认证。\n* AuthorizedKeysCommandUser就是指定以什么用户来运行这个脚本。 这两个配置选项的一个用处就是在用户管理上可以不再依靠本地管理，而可以通过脚本读取远程数据库系统中的用户的publickey进行认证，例如MySQL或者LDAP，这样的话，更便于用户的集中管理。","tags":["linux","ssh"],"categories":["linux"]},{"title":"通过lambda修改AWS CloudFront回源host","url":"//2018/10/11/aws-edge-lambda-modify-origin-host/","content":"在使用aws cloudfront时发现cloudfront默认不允许自定义回源请求头的Host字段，对于一些情况我们需要使用这个host+ip来回源的时候就有点坑了，这个时候我们可以通过使用aws的lambda@edge，去修改request的header来实现自定义host来回源。\n\naws lambda@edge文档：https://docs.aws.amazon.com/zh_cn/lambda/latest/dg/lambda-edge.html\n\n1. 在创建lambda函数，并发布一个版本注意只能在us-east-1这个区创建，否则在附加到cloudfront的时候会报错不支持的区域，代码如下\n\n![img](/images/img_5bbef98529a61.png)\n\n```\n'use strict';\n\n// force a specific Host header to be sent to the origin\n\nexports.handler = (event, context, callback) => {\n    const request = event.Records[0].cf.request;\n    request.headers.host[0].value = 'www.xiemx.com';\n    return callback(null, request);\n};\n```\n\n2. 在cloudfront的Behavior菜单中\n\n![img](/images/img_5bbefa0f833d9.png)\n\n3. 重新deploy cdn和刷新一次cdn缓存","tags":["aws","lambda","cloudfront"],"categories":["aws"]},{"title":"SLB 502报错Debug","url":"//2018/07/20/slb-502-debug/","content":"用户自定义站点502问题分析\n\n1. 现象：自定义域名用户反馈，打开网站返回502，如图\n\n![img](/images/img_5b5183b84d4dd.png)\n\n 根据response header判断，请求到达captain，怀疑captain返回的502页面。查看nginx proxy_pass得知后端的地址为bobcat.sxldns.com.\n\n```\nset $bobcat_backend \"bobcat.sxldns.com\";\nproxy_pass http://$bobcat_backend;\n```\n\n使用curl模拟请求，直接请求bobcat.sxldns.com,正常获得返回内容，具体针对每个服务器的IP的curl,不再单独列出。\n\n![img](/images/img_5b5184bfd49e3.png)\n\n通过上述返回基本判断，问题出在我们的代理层。具体查看代理层的nginx配置和系统资源利用率。\n\n查看当时系统的资源状态,查看到当时的系统磁盘空间使用完。查看nginx上有关于proxy的cache相关配置,nginx会cache的response的content的内容。怀疑nginx转发请求之后但是backend返回内容后，nginx cache到本地的时候无法写入disk导致会话结束，SLB的请求无返回包认为后端宕机抛出502。\n\n```\nproxy_cache_path /etc/nginx/china_cache levels=1:2 keys_zone=user_page_cache:100m max_size=20g inactive=60m use_temp_path=off;\n```\n\n具体DEBUG如下：\n\n 1. 获取container中的nginx worker进程的PID\n\n![img](/images/img_5b5184fb1c8d4.png)\n\n2. strace查看下系统调用具体信息\n\n![img](/images/img_5b518515e4fe7.png)\n\n通过上图strace追踪流程可以看到左侧是一个正常的请求的全部流程，右侧是故障状态的strace的系统调用全流程。\n\n通过对比左右两侧的系统调用流程可以看到，当nginx cache的时候写入`/etc/nginx/china_cache`目录时提示`no space`后续的writev()和sendfile()方法就没有调用，因此导致SLB无法获得返回包，抛出`badgateway`的错误.\n\nPS：\n\n1. 为什么单独请求返回头的时候正常返回？nginx返回请求头的时候并不会走proxy的cache流程。因此没有调用open()方法读写disk，正常返回，但是实际请求数据的时候cache写磁盘直接失败，后续直接退出。\n2. 上图的系统调用过程为了debug的方便使用了非折叠模式。因此一些no space的一些报错未显示出来可以。全部流程见附录。\n\n附录：\n\n```\n1.异常调用堆栈\n[root@iZ2ze2mzhjk3ou1vsnkthzZ rpm]# strace -p 3904 -v\nstrace: Process 3904 attached\nepoll_wait(8, [{EPOLLIN, {u32=69025808, u64=140003117973520}}], 512, -1) = 1\naccept4(6, {sa_family=AF_INET, sin_port=htons(59464), sin_addr=inet_addr(\"10.130.0.4\")}, [16], SOCK_NONBLOCK) = 11\nepoll_ctl(8, EPOLL_CTL_ADD, 11, {EPOLLIN|EPOLLRDHUP|EPOLLET, {u32=69027249, u64=140003117974961}}) = 0\nepoll_wait(8, [{EPOLLIN, {u32=69027249, u64=140003117974961}}], 512, 60000) = 1\nrecvfrom(11, \"GET /?key=testxxxx HTTP/1.1\\r\\nUse\"..., 1024, 0, NULL, NULL) = 88\nepoll_ctl(8, EPOLL_CTL_MOD, 11, {EPOLLIN|EPOLLOUT|EPOLLRDHUP|EPOLLET, {u32=69027249, u64=140003117974961}}) = 0\nopen(\"/etc/nginx/china_cache/2/7d/2ac65d1f9080e2baff45a4332f9017d2\", O_RDONLY|O_NONBLOCK) = 14\nfstat(14, {st_dev=makedev(253, 1), st_ino=655502, st_mode=S_IFREG|0600, st_nlink=1, st_uid=101, st_gid=101, st_blksize=4096, st_blocks=96, st_size=46750, st_atime=2018/07/17-16:34:43.967698739, st_mtime=2018/07/17-16:34:43.966698736, st_ctime=2018/07/17-16:34:43.967698739}) = 0\npread64(14, \"\\5\\0\\0\\0\\0\\0\\0\\0(\\252M[\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\"..., 636, 0) = 636\ngetsockname(11, {sa_family=AF_INET, sin_port=htons(80), sin_addr=inet_addr(\"172.17.0.2\")}, [16]) = 0\nsendto(13, \"nN\\1\\0\\0\\1\\0\\0\\0\\0\\0\\0\\6bobcat\\6sxldns\\3com\\0\\0\"..., 35, 0, NULL, 0) = 35\nsendto(13, \"\\375\\215\\1\\0\\0\\1\\0\\0\\0\\0\\0\\0\\6bobcat\\6sxldns\\3com\\0\\0\"..., 35, 0, NULL, 0) = 35\nepoll_wait(8, [{EPOLLOUT, {u32=69027249, u64=140003117974961}}, {EPOLLIN, {u32=69026288, u64=140003117974000}}], 512, 5000) = 2\nrecvfrom(13, \"nN\\201\\200\\0\\1\\0\\3\\0\\0\\0\\0\\6bobcat\\6sxldns\\3com\\0\\0\"..., 4096, 0, NULL, NULL) = 147\nrecvfrom(13, \"\\375\\215\\201\\200\\0\\1\\0\\1\\0\\1\\0\\0\\6bobcat\\6sxldns\\3com\\0\\0\"..., 4096, 0, NULL, NULL) = 168\nsocket(AF_INET, SOCK_STREAM, IPPROTO_IP) = 15\nioctl(15, FIONBIO, [1]) = 0\nepoll_ctl(8, EPOLL_CTL_ADD, 15, {EPOLLIN|EPOLLOUT|EPOLLRDHUP|EPOLLET, {u32=69027488, u64=140003117975200}}) = 0\nconnect(15, {sa_family=AF_INET, sin_port=htons(80), sin_addr=inet_addr(\"54.222.148.216\")}, 16) = -1 EINPROGRESS (Operation now in progress)\nrecvfrom(13, 0x7ffd1f603790, 4096, 0, NULL, NULL) = -1 EAGAIN (Resource temporarily unavailable)\nepoll_wait(8, [{EPOLLOUT, {u32=69027488, u64=140003117975200}}], 512, 20000) = 1\ngetsockopt(15, SOL_SOCKET, SO_ERROR, [0], [4]) = 0\nwritev(15, [{\"GET /?key=testxxxx HTTP/1.0\\r\\nHos\"..., 219}], 1) = 219\nepoll_wait(8, [{EPOLLIN|EPOLLOUT, {u32=69027488, u64=140003117975200}}], 512, 60000) = 1\nrecvfrom(15, \"HTTP/1.1 200 OK\\r\\nContent-Type: t\"..., 3723, 0, NULL, NULL) = 3723\nclose(14) = 0\nreadv(15, [{\"a.qnssl.com/images/265818/Fntncr\"..., 4096}], 1) = 4096\nreadv(15, [{\"w-card-price{color: #004aa0;}.s-\"..., 4096}], 1) = 4096\nreadv(15, [{\" {\\n font-family: \\\"Open Sans\"..., 4096}], 1) = 373\nopen(\"/etc/nginx/china_cache/2/7d/2ac65d1f9080e2baff45a4332f9017d2.0000014052\", O_RDWR|O_CREAT|O_EXCL, 0600) = 14\npwritev(14, [{\"\\5\\0\\0\\0\\0\\0\\0\\0\\356\\254M[\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\"..., 4096}, {\"a.qnssl.com/images/265818/Fntncr\"..., 4096}, {\"w-card-price{color: #004aa0;}.s-\"..., 4096}], 3, 0) = -1 ENOSPC (No space left on device)\ngettid() = 7\nwrite(4, \"2018/07/17 08:46:33 [crit] 7#7: \"..., 328) = 328\nclose(15) = 0\nunlink(\"/etc/nginx/china_cache/2/7d/2ac65d1f9080e2baff45a4332f9017d2.0000014052\") = 0\nwrite(5, \"{ \\\"time\\\": \\\"2018-07-17T08:46:33+0\"..., 382) = 382\nclose(14) = 0\nclose(11) = 0\n\n2.正常调用堆栈过程\n[root@iZ2ze2mzhjk3ou1vsnkthzZ rpm]# strace -p 3904 -v\nstrace: Process 3904 attached\nepoll_wait(8, [{EPOLLIN, {u32=69025808, u64=140003117973520}}], 512, -1) = 1\naccept4(6, {sa_family=AF_INET, sin_port=htons(59360), sin_addr=inet_addr(\"10.130.0.4\")}, [16], SOCK_NONBLOCK) = 11\nepoll_ctl(8, EPOLL_CTL_ADD, 11, {EPOLLIN|EPOLLRDHUP|EPOLLET, {u32=69027248, u64=140003117974960}}) = 0\nepoll_wait(8, [{EPOLLIN, {u32=69027248, u64=140003117974960}}], 512, 60000) = 1\nrecvfrom(11, \"GET /?key=testxxxx HTTP/1.1\\r\\nUse\"..., 1024, 0, NULL, NULL) = 88\nepoll_ctl(8, EPOLL_CTL_MOD, 11, {EPOLLIN|EPOLLOUT|EPOLLRDHUP|EPOLLET, {u32=69027248, u64=140003117974960}}) = 0\nopen(\"/etc/nginx/china_cache/2/7d/2ac65d1f9080e2baff45a4332f9017d2\", O_RDONLY|O_NONBLOCK) = 14\nfstat(14, {st_dev=makedev(253, 1), st_ino=655504, st_mode=S_IFREG|0600, st_nlink=1, st_uid=101, st_gid=101, st_blksize=4096, st_blocks=96, st_size=46750, st_atime=2018/07/17-16:14:38.127407158, st_mtime=2018/07/17-16:14:38.127407158, st_ctime=2018/07/17-16:14:38.128407161}) = 0\npread64(14, \"\\5\\0\\0\\0\\0\\0\\0\\0s\\245M[\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\"..., 636, 0) = 636\ngetsockname(11, {sa_family=AF_INET, sin_port=htons(80), sin_addr=inet_addr(\"172.17.0.2\")}, [16]) = 0\nsendto(12, \"\\210V\\1\\0\\0\\1\\0\\0\\0\\0\\0\\0\\6bobcat\\6sxldns\\3com\\0\\0\"..., 35, 0, NULL, 0) = 35\nsendto(12, \"\\27\\1\\1\\0\\0\\1\\0\\0\\0\\0\\0\\0\\6bobcat\\6sxldns\\3com\\0\\0\"..., 35, 0, NULL, 0) = 35\nepoll_wait(8, [{EPOLLOUT, {u32=69027248, u64=140003117974960}}, {EPOLLIN, {u32=69027728, u64=140003117975440}}], 512, 5000) = 2\nrecvfrom(12, \"\\210V\\201\\200\\0\\1\\0\\3\\0\\0\\0\\0\\6bobcat\\6sxldns\\3com\\0\\0\"..., 4096, 0, NULL, NULL) = 147\nrecvfrom(12, \"\\27\\1\\201\\200\\0\\1\\0\\1\\0\\1\\0\\0\\6bobcat\\6sxldns\\3com\\0\\0\"..., 4096, 0, NULL, NULL) = 168\nsocket(AF_INET, SOCK_STREAM, IPPROTO_IP) = 15\nioctl(15, FIONBIO, [1]) = 0\nepoll_ctl(8, EPOLL_CTL_ADD, 15, {EPOLLIN|EPOLLOUT|EPOLLRDHUP|EPOLLET, {u32=69027489, u64=140003117975201}}) = 0\nconnect(15, {sa_family=AF_INET, sin_port=htons(80), sin_addr=inet_addr(\"52.80.58.42\")}, 16) = -1 EINPROGRESS (Operation now in progress)\nrecvfrom(12, 0x7ffd1f603790, 4096, 0, NULL, NULL) = -1 EAGAIN (Resource temporarily unavailable)\nepoll_wait(8, [{EPOLLOUT, {u32=69027489, u64=140003117975201}}], 512, 20000) = 1\ngetsockopt(15, SOL_SOCKET, SO_ERROR, [0], [4]) = 0\nwritev(15, [{\"GET /?key=testxxxx HTTP/1.0\\r\\nHos\"..., 219}], 1) = 219\nepoll_wait(8, [{EPOLLIN|EPOLLOUT, {u32=69027489, u64=140003117975201}}], 512, 60000) = 1\nrecvfrom(15, \"HTTP/1.1 200 OK\\r\\nContent-Type: t\"..., 3723, 0, NULL, NULL) = 3723\nclose(14) = 0\nreadv(15, [{\"a.qnssl.com/images/265818/Fntncr\"..., 4096}], 1) = 4096\nreadv(15, [{\"w-card-price{color: #004aa0;}.s-\"..., 4096}], 1) = 4096\nreadv(15, [{\" {\\n font-family: \\\"Open Sans\"..., 4096}], 1) = 2565\nopen(\"/etc/nginx/china_cache/2/7d/2ac65d1f9080e2baff45a4332f9017d2.0000014051\", O_RDWR|O_CREAT|O_EXCL, 0600) = 14\npwritev(14, [{\"\\5\\0\\0\\0\\0\\0\\0\\0(\\252M[\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\"..., 4096}, {\"a.qnssl.com/images/265818/Fntncr\"..., 4096}, {\"w-card-price{color: #004aa0;}.s-\"..., 4096}], 3, 0) = 12288\nwritev(11, [{\"HTTP/1.1 200 OK\\r\\nServer: nginx/1\"..., 321}], 1) = 321\nsendfile(11, 14, [636] => [12288], 11652) = 11652\nepoll_wait(8, [{EPOLLOUT, {u32=69027489, u64=140003117975201}}], 512, 59955) = 1\nepoll_wait(8, [{EPOLLIN|EPOLLOUT, {u32=69027489, u64=140003117975201}}], 512, 59953) = 1\nreadv(15, [{\"rc=\\\"//nzr2ybsda.qnssl.com/images\"..., 1531}, {\"lass=\\\"s-nav-item\\\" target=\\\"_self\\\"\"..., 4096}, {\"lign: left; font-size: 160%;\\\">\\302\\240\"..., 4096}, {\"OQswmpPvoA0.jpg?imageMogr2/strip\"..., 4096}], 4) = 10136\npwritev(14, [{\" {\\n font-family: \\\"Open Sans\"..., 4096}, {\"lass=\\\"s-nav-item\\\" target=\\\"_self\\\"\"..., 4096}, {\"lign: left; font-size: 160%;\\\">\\302\\240\"..., 4096}], 3, 12288) = 12288\nsendfile(11, 14, [12288] => [24576], 12288) = 12288\nepoll_wait(8, [{EPOLLIN|EPOLLOUT, {u32=69027489, u64=140003117975201}}], 512, 59948) = 1\nreadv(15, [{\"olor-custom1\\\">\\347\\254\\254\\344\\270\\211\\346\\226\\271\\345\"..., 3683}, {\"container title-group-container\\\"\"..., 4096}, {\";cs=srgb&s=f71a6adc1e5953a52\"..., 4096}, {\")\\\" data-bg=\\\"//nzr2ybsda.qnssl.co\"..., 4096}], 4) = 15971\nreadv(15, [{\"\\230\\263\\345\\224\\257\\346\\231\\237\\\" title=\\\"\\346\\262\\210\\351\\230\\263\\345\\224\\257\\346\\231\\237\\345\\225\\206\"..., 4096}], 1) = 2853\npwritev(14, [{\"OQswmpPvoA0.jpg?imageMogr2/strip\"..., 4096}, {\"container title-group-container\\\"\"..., 4096}, {\";cs=srgb&s=f71a6adc1e5953a52\"..., 4096}, {\")\\\" data-bg=\\\"//nzr2ybsda.qnssl.co\"..., 4096}], 4, 24576) = 16384\nsendfile(11, 14, [24576] => [40960], 16384) = 16384\nepoll_wait(8, [{EPOLLIN|EPOLLOUT, {u32=69027489, u64=140003117975201}}], 512, 59947) = 1\nreadv(15, [{\"\n\n<d\"..., 1243}, {\"\\232\\204\\344\\270\\232\\347\\273\\251\\344\\271\\237\\345\\234\\250\\344\\270\\215\\346\\226\\255\\346\\224\\200\\345\\215\\207\\343\\200\\202</d\"..., 4096}, {\"\", 4096}, {\"\", 4096}, {\"\", 4096}], 5) = 2937\npwritev(14, [{\"\\230\\263\\345\\224\\257\\346\\231\\237\\\" title=\\\"\\346\\262\\210\\351\\230\\263\\345\\224\\257\\346\\231\\237\\345\\225\\206\"..., 4096}, {\"\\232\\204\\344\\270\\232\\347\\273\\251\\344\\271\\237\\345\\234\\250\\344\\270\\215\\346\\226\\255\\346\\224\\200\\345\\215\\207\\343\\200\\202</d\"..., 1694}], 2, 40960) = 5790 sendfile(11, 14, [40960] => [46750], 5790) = 5790\nchmod(\"/etc/nginx/china_cache/2/7d/2ac65d1f9080e2baff45a4332f9017d2.0000014051\", 0600) = 0\nrename(\"/etc/nginx/china_cache/2/7d/2ac65d1f9080e2baff45a4332f9017d2.0000014051\", \"/etc/nginx/china_cache/2/7d/2ac65d1f9080e2baff45a4332f9017d2\") = 0\nfstat(14, {st_dev=makedev(253, 1), st_ino=655502, st_mode=S_IFREG|0600, st_nlink=1, st_uid=101, st_gid=101, st_blksize=4096, st_blocks=96, st_size=46750, st_atime=2018/07/17-16:34:43.967698739, st_mtime=2018/07/17-16:34:43.966698736, st_ctime=2018/07/17-16:34:43.967698739}) = 0\nclose(15) = 0\nwrite(5, \"{ \\\"time\\\": \\\"2018-07-17T08:34:43+0\"..., 386) = 386\nclose(14) = 0\nsetsockopt(11, SOL_TCP, TCP_NODELAY, [1], 4) = 0\nepoll_wait(8, [{EPOLLIN|EPOLLOUT|EPOLLRDHUP, {u32=69027248, u64=140003117974960}}], 512, 65000) = 1\nrecvfrom(11, \"\", 1024, 0, NULL, NULL) = 0\nclose(11) = 0\n```","tags":["debug","linux","nginx"],"categories":["linux","nginx","debug"]},{"title":"TCP状态机","url":"//2018/01/23/tcp-state/","content":"### TCP状态分析\n* listen／close\n* syn-sent/syn-revd\n* established\n* fin_wait_1/close_wait\n* fin_wait_2/last_ack\n* time_wait/close\n\n```\nLISTEN\t        等待来自远程TCP应用程序的请求\nSYN_SENT\t发送连接请求后等待来自远程端点的确认。TCP第一次握手后客户端所处的状态\nSYN-RECEIVED\t该端点已经接收到连接请求并发送确认。该端点正在等待最终确认。TCP第二次握手后服务端所处的状态\nESTABLISHED\t代表连接已经建立起来了。这是连接数据传输阶段的正常状态\nFIN_WAIT_1\t等待来自远程TCP的终止连接请求或终止请求的确认\nFIN_WAIT_2\t在此端点发送终止连接请求后，等待来自远程TCP的连接终止请求\nCLOSE_WAIT\t该端点已经收到来自远程端点的关闭请求，此TCP正在等待本地应用程序的连接终止请求\nCLOSING\t        等待来自远程TCP的连接终止请求确认\nLAST_ACK\t等待先前发送到远程TCP的连接终止请求的确认\nTIME_WAIT\t等待足够的时间来确保远程TCP接收到其连接终止请求的确认\n```\n\n以上大致为一个Tcp从三次握手建立连接到四次挥手断开连接的整个过程C/S对应的TCP状态。\n### 详细流程\n\n```\n1. 客户端(close)发送syn连接请求给服务端(listen),客户端等待服务端ack(syn_sent)\n2. 服务端收到syn请求,发送ack/syn(syn_rec)\n3. 客户端收到ack(establelished)\n4. 传输数据\n5. 客户端数据交互完成请求关闭连接，发送fin请求(fin_wait_1)\n6. 服务端收到fin请求,发送ack(close_wait)\n7. 服务端数据交互完成,发送fin请求关闭连接(last_ack)\n8. 客户端收到服务端的ack请求(fin_wait_2)\n9. 客户端收到服务端的fin请求,发送ack确认断开(time_wait)\n10. 服务端收到客户端的ack,关闭连接(close)\n11. 客户端维护2个msl时间后回收socket\n```\n引用网上的一张图：\n\n![img](/images/img_5a66a577176c8.png)","tags":["linux","tcp"],"categories":["linux"]},{"title":"TCP TIME_WAIT","url":"//2018/01/23/tcp-time_wait/","content":"#### 维持TIME_WAIT有两个原因：\n\n1. 可靠地实现TCP的全双工连接终止\n\n    在四次挥手中，假设最后的ACK丢失了，被动关闭方会重发FIN。主动关闭端必须维护状态，来允许被动关闭方重发最后的ACK；如果它没有维护这个状态，将会对重发FIN返回RST，被动关闭方会认为这是个错误。如果TCP正在执行彻底终止数据流的两个方向所需的所有工作（即全双工关闭），则必须正确处理这四个段中任何一个的丢失。所以执行主动关闭的一方必须在结束时保持TIME_WAIT状态：因为它可能必须重传最后的ACK。\n\n2. 允许旧的重复数据段在网络中过期\n\n    假设在主机1.1.1.1的1111端口和2.2.2.2的2222端口之间有一个TCP连接。此连接关闭后，相同的地址和端口建立了一个新连接。由于IP地址和端口相同，TCP必须防止旧连接的数据包再次出现，被新的连接误收。为此，TCP将不会启动当前处于TIME_WAIT状态的连接。由于TIME_WAIT状态的持续时间是两倍的MSL，因此TCP允许一个方向的数据在MSL秒内丢失，也允许回复在一个MSL秒内丢失。通过此规则来保证当一个TCP连接成功建立时，来自先前连接的所有旧的副本在网络中已过期。","tags":["linux","tcp"],"categories":["linux"]},{"title":"zabbix distrubuted monitor","url":"//2017/09/21/zabbix-distrubuted-monitor/","content":"\nzabbix 分布式监控2种模式\n* node模式\n* proxy模式\n\nPS: node模式官方在2.4版本之后已经弃用，重点讨论proxy模式\n\n### proxy 模式\n\nZabbix proxy可以代替Zabbix服务器收集性能和可用性数据，一个代理可以承担一些收集数据的负载。使用代理是实现集中式和分布式监控的最简单方法。proxy需要使用单独的数据库来缓存agent数据，在发给server防止出现因网络问题造成的数据丢失。zabbix proxy只是一个数据收集组件，不会触发任何trigger／alert.\n\n![img](/images/img_59c35cd9ab71b.png)\n\n### 使用场景\n\n- Monitor remote locations\n- Monitor locations having unreliable communications\n- Offload the Zabbix server when monitoring thousands of devices\n- Simplify the maintenance of distributed monitoring\n\n### 安装配置\n\n```shell\n#同安装zabbix server 类似，不赘述,需要其他功能也可以在编译时自行开启。\n\n./configure --prefix=/opt/zabbix_proxy/ --enable-proxy --with-mysql --with-libcurl\nmake install\n\ncreate databases zabbix\ngrant all to zabbix.* to zabbix@'%' identified by \"zabbix\";\n#导入schema.sql\n\n#配置文件中hostname需要和zabbix上添加的保持一致\n#其它参考server设置参数\n\n#ps：设置适当的配置同步时间，默认一小时。建议设置短一点，这样如果有新机器加入配置修改都可以快速同步并监控。\nConfigFrequency=600\n```\n\nzabbix server 配置\n\n![img](/images/img_59c35d1d930c1.png)\n\n添加主机时选择指定的proxy\n\n![img](/images/img_59c35d389b1c5.png)","tags":["zabbix"],"categories":["zabbix"]},{"title":"graphite_relay sharding","url":"//2017/09/08/graphite_relay-sharding/","content":"\n当statsd发送超量的metrics到graphite中，graphite单节点无法负载的情况，可以使用consistent-hashing的模式来将数据分片到backend中。同样的consistent-hashing模式下可以自动剔除／加入节点。\n\n官方文档：\n\n```\ncarbon-relay.py serves two distinct purposes: replication and sharding.\n\nWhen running with RELAY_METHOD = rules, a carbon-relay.py instance can run in place of a carbon-cache.py server and relay all incoming metrics to multiple backend carbon-cache.py‘s running on different ports or hosts.\n\nIn RELAY_METHOD = consistent-hashing mode, a DESTINATIONS setting defines a sharding strategy across multiple carbon-cache.py backends. The same consistent hashing list can be provided to the graphite webapp via CARBONLINK_HOSTS to spread reads across the multiple backends.\n本例模拟一个双后端的carbon-cache instance.单机器运行,使用carbon-cache.py 的instance功能\n\nconfig文件：\n#####carbon.conf\n[cache:a]\nLINE_RECEIVER_PORT = 2203\nPICKLE_RECEIVER_PORT = 2204\nCACHE_QUERY_PORT = 7102\nLOCAL_DATA_DIR = /opt/graphite/storage/whisper_a/\n[cache:b]\nLINE_RECEIVER_PORT = 2205\nPICKLE_RECEIVER_PORT = 2206\nCACHE_QUERY_PORT = 7202\nLOCAL_DATA_DIR = /opt/graphite/storage/whisper_b/\n[relay]\nLINE_RECEIVER_INTERFACE = 0.0.0.0\nLINE_RECEIVER_PORT = 2003\nPICKLE_RECEIVER_INTERFACE = 0.0.0.0\nPICKLE_RECEIVER_PORT = 2004\nDESTINATIONS = 127.0.0.1:2204:a, 127.0.0.1:2206:b \n启动服务：\n\n启动instance:a\nxmx@xiemx-test:/opt/graphisudo ./carbon-cache.py --instance=a --config=/opt/graphite/conf/carbon.conf start\nStarting carbon-cache (instance a)\n\n启动instance:b\nxmx@xiemx-test:/opt/graphite/bin$ sudo ./carbon-cache.py --instance=b --config=/opt/graphite/conf/carbon.conf start\nStarting carbon-cache (instance b)\n\n启动relay:\nxmx@xiemx-test:/opt/graphite/conf$ sudo /opt/graphite/bin/carbon-relay.py --debug --config=/opt/graphite/conf/carbon.conf start\nStarting carbon-relay (instance a)\n08/09/2017 16:02:16 :: [console] Using sorted write strategy for cache\n08/09/2017 16:02:16 :: [clients] connecting to carbon daemon at 127.0.0.1:2204:a\n08/09/2017 16:02:16 :: [clients] connecting to carbon daemon at 127.0.0.1:2206:b\n08/09/2017 16:02:16 :: [console] twistd 16.4.1 (/usr/bin/python 2.7.6) starting up.\n08/09/2017 16:02:16 :: [console] reactor class: twisted.internet.epollreactor.EPollReactor.\n08/09/2017 16:02:16 :: [console] Starting factory CarbonClientFactory(127.0.0.1:2206:b)\n08/09/2017 16:02:16 :: [clients] CarbonClientFactory(127.0.0.1:2206:b)::startedConnecting (127.0.0.1:2206)\n08/09/2017 16:02:16 :: [console] Starting factory CarbonClientFactory(127.0.0.1:2204:a)\n08/09/2017 16:02:16 :: [clients] CarbonClientFactory(127.0.0.1:2204:a)::startedConnecting (127.0.0.1:2204)\n08/09/2017 16:02:16 :: [console] CarbonReceiverFactory starting on 2003\n08/09/2017 16:02:16 :: [console] Starting factory \n08/09/2017 16:02:16 :: [console] CarbonReceiverFactory starting on 2004\n08/09/2017 16:02:16 :: [console] Starting factory \n08/09/2017 16:02:16 :: [clients] CarbonClientProtocol(127.0.0.1:2206:b)::connectionMade\n08/09/2017 16:02:16 :: [clients] CarbonClientFactory(127.0.0.1:2206:b)::connectionMade (CarbonClientProtocol(127.0.0.1:2206:b))\n08/09/2017 16:02:16 :: [clients] Destination is up: 127.0.0.1:2206:b\n08/09/2017 16:02:16 :: [clients] CarbonClientProtocol(127.0.0.1:2204:a)::connectionMade\n08/09/2017 16:02:16 :: [clients] CarbonClientFactory(127.0.0.1:2204:a)::connectionMade (CarbonClientProtocol(127.0.0.1:2204:a))\n08/09/2017 16:02:16 :: [clients] Destination is up: 127.0.0.1:2204:a\n```\n\n测试：\n\n模拟5个客户端同时发送100个key\n\n![img](/images/img_59b26b3126991.png)\n\n![img](/images/img_59b26b4437729.png)\n\n模拟node掉线重连\n\n![img](/images/img_59b26b60dc7e7.png)\n\ngraphite-web数据聚合展示\n\n```\n修改local_settings.py\nCARBONLINK_HOSTS = [\"127.0.0.1:7102:a\", \"127.0.0.1:7202:b\"]\n\n启动django\nsudo PYTHONPATH=/opt/graphite/webapp django-admin.py runserver 0.0.0.0:5000 --settings\n```","categories":["statsd","graphite"]},{"title":"statsd cluster proxy","url":"//2017/09/08/statsd-cluster-proxy/","content":"通过UDP proxy程序将前端的数据通过一定的hash算法将相同的metric发送的固定的后aggregation数据。proxy代理支持健康检测自动剔除／加入后端statsd。\n\n1.配置\n本例展示一个3节点的statsd后端且将聚合数据发送到standout方便观测。\n\n```\nnode1\n######config文件：\n{\n port: 8127\n, mgmt_port: 8227\n, backends: [ \"./backends/console\" ]\n}\n\n启动:\nxiemx➜  statsd : master ✘ :✹✭ ᐅ  node stats.js config8127.js\n8 Sep 11:06:36 - [81604] reading config file: config8127.js\n8 Sep 11:06:36 - server is up INFO\n```\n```\nnode2\n######config文件：\n{\n port: 8128\n, mgmt_port: 8228\n, backends: [ \"./backends/console\" ]\n}\n\n启动：\nxiemx➜  statsd : master ✘ :✹✭ ᐅ  node stats.js config8128.js\n8 Sep 11:07:09 - [81665] reading config file: config8128.js\n8 Sep 11:07:09 - server is up INFO\n```\n```\nnode3\n######config文件：\n{\n port: 8129\n, mgmt_port: 8229\n, backends: [ \"./backends/console\" ]\n}\n\n启动：\nxiemx➜  statsd : master ✘ :✹✭ ᐅ  node stats.js config8129.js\n8 Sep 11:07:45 - [81723] reading config file: config8129.js\n8 Sep 11:07:45 - server is up INFO\n```\n```\nproxy\n######config文件：\n{\nnodes: [\n{host: '127.0.0.1', port: 8127, adminport: 8227},\n{host: '127.0.0.1', port: 8128, adminport: 8228},\n{host: '127.0.0.1', port: 8129, adminport: 8229}\n],\nserver: './servers/udp',\nhost:  '0.0.0.0',\nport: 8125,\nmgmt_port: 8126,\nforkCount: 0,\ncheckInterval: 1000,\ncacheSize: 10000,\ndeleteIdleStats: true\n}\n\n启动：\nxiemx➜  statsd : master ✘ :✹✭ ᐅ  node proxy.js proxyconfig.js\n8 Sep 11:09:02 - [81938] reading config file: proxyconfig.js\n8 Sep 11:09:02 - INFO: [81938] server is up\n```\n\n测试：\n5个线程同时推送500个metric到代理查看分片情况\n\n```\n测试命令：\nfor i in $(seq 1 500);do echo \"Ezbuy-$i:1|c\" | nc -u -w0 127.0.0.1 8125;done\n```\n\n![img](/images/img_59b269634a216.png)\n\n![img](/images/img_59b2697b89208.png)\n\n节点自动剔除和加入：\n\n![img](/images/img_59b2698d5658f.png)\n\n","tags":["database","statsd","graphite","monitor"],"categories":["statsd","graphite"]},{"title":"redis 淘汰策略","url":"//2017/07/14/redis-maxmemory-policy/","content":"redis 内存数据使用到maxmemory的时候，就会根据maxmemory_policy设定的淘汰策略进行内存整理数据回收。选择不同的策略，要根据redis的用途来区分。\n\nredis 提供 6种数据淘汰策略：\n\n* volatile-lru：从已设置过期时间的数据集中挑选最近最少使用的数据淘汰\n* volatile-ttl：从已设置过期时间的数据集中挑选将要过期的数据淘汰\n* volatile-random：从已设置过期时间的数据集中任意选择数据淘汰\n* allkeys-lru：从数据集中挑选最近最少使用的数据淘汰\n* allkeys-random：从数据集中任意选择数据淘汰\n* noenviction：禁止删除数据","tags":["redis"],"categories":["redis"]},{"title":"iptables count计数","url":"//2017/04/11/iptables-count/","content":"```\n两个参数：\n  --verbose\t-v\t\tverbose mode\n  --zero    -Z [chain [rulenum]]  Zero counters in chain or all chains\n\n\np-hsg-cache-6% sudo iptables -nL -v -t nat\nChain PREROUTING (policy ACCEPT 50741 packets, 2370K bytes)\n pkts bytes target     prot opt in     out     source               destination\n  436 22672 DNAT       tcp  --  *      *       0.0.0.0/0            192.168.10.226       tcp dpt:6379 to:192.168.10.82:6379\n    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            192.168.10.226       tcp dpt:6380 to:192.168.10.81:6380\n    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            192.168.10.226       tcp dpt:6381 to:192.168.10.81:6381\n    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            192.168.10.226       tcp dpt:6382 to:192.168.10.81:6382\n    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            192.168.10.226       tcp dpt:6383 to:192.168.10.81:6383\n\nChain INPUT (policy ACCEPT 18988 packets, 1082K bytes)\n pkts bytes target     prot opt in     out     source               destination\n\nChain OUTPUT (policy ACCEPT 10532 packets, 549K bytes)\n pkts bytes target     prot opt in     out     source               destination\n\nChain POSTROUTING (policy ACCEPT 0 packets, 0 bytes)\n pkts bytes target     prot opt in     out     source               destination\n10968  572K MASQUERADE  all  --  *      *       0.0.0.0/0            0.0.0.0/0\n\np-hsg-cache-6% sudo iptables -Z -t nat\n\np-hsg-cache-6% sudo iptables -nL -v -t nat\nChain PREROUTING (policy ACCEPT 12 packets, 624 bytes)\n pkts bytes target     prot opt in     out     source               destination\n    1    52 DNAT       tcp  --  *      *       0.0.0.0/0            192.168.10.226       tcp dpt:6379 to:192.168.10.82:6379\n    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            192.168.10.226       tcp dpt:6380 to:192.168.10.81:6380\n    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            192.168.10.226       tcp dpt:6381 to:192.168.10.81:6381\n    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            192.168.10.226       tcp dpt:6382 to:192.168.10.81:6382\n    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            192.168.10.226       tcp dpt:6383 to:192.168.10.81:6383\n\nChain INPUT (policy ACCEPT 12 packets, 624 bytes)\n pkts bytes target     prot opt in     out     source               destination\n\nChain OUTPUT (policy ACCEPT 6 packets, 312 bytes)\n pkts bytes target     prot opt in     out     source               destination\n\nChain POSTROUTING (policy ACCEPT 0 packets, 0 bytes)\n pkts bytes target     prot opt in     out     source               destination\n    7   364 MASQUERADE  all  --  *      *       0.0.0.0/0            0.0.0.0/0\np-hsg-cache-6%\n\n```","tags":["linux","iptables"],"categories":["linux"]},{"title":"iis备份还原","url":"//2017/03/31/iis-backup-restore/","content":"\n1. 打开我们的IIS管理器，在功能视图里找到共享的配置这个功能然后双击进入。\n\n![img](/images/img_58ddc18a1993a.png)\n\n2. 进入后单机右上方的“导出配置”选项，选择导出配置文件的物理路径，然后设置一个密码，密码必须是包含数字、符号、大小写字母组合并且至少为8个字符长的强密码，确定导出后会在你导出配置文件目录下生成administration.config、applicationHost.config和configEncKey.key共3个文件，这3个文件就是我们备份的IIS站点配置信息文件。\n\n![img](/images/img_58ddc1b2d8078.png)\n\n3. 现在是还原IIS的配置信息，首先将你导出后的administration.config、applicationHost.config和configEncKey.key这个3个文件复制到你需要恢复IIS配置信息的电脑或服务器上，然后打开IIS，同样在功能视图里找到“共享的配置”并打开。\n\n![img](/images/img_58ddc20c6f8c3.png)\n\n4. 把“启用共享的配置”勾选上，物理路径就选择你备份的文件所在目录，用户名、密码输入框都不需要填写，直接点击右上方的应用，然后它要你输入密码，确定后重启下我们的IIS 就可以看到以前的站点信息都还原了。","tags":["webserver","iis","windows"],"categories":["windows","iis"]},{"title":"进程状态","url":"//2017/03/29/linux-process-status/","content":"```\nD  不能中断的进程（通常为IO）\nR  正在运行中的进程\nS  已经中断的进程，通常情况下，系统中大部分进程都是这个状态\nT  已经停止或者暂停的进程，如果我们正在运行一个命令，比如说sleep 10，如果我们按一下ctrl -z 让他暂停，那么我们用ps查看就会显示T这个状态\nW 这个好像是说，从内核2.6xx 以后，表示为没有足够的内存页分配\nX  已经死掉的进程（这个好像从来不会出现）\nZ  僵尸进程\n\n下面一些是BSD风格的参数\n<  高优先级进程\nN  低优先级进程\nL   在内存中被锁了内存分页\ns   主进程\nl   多线程进程\n+  代表在前台运行的进程\n```","tags":["linux"],"categories":["linux"]},{"title":"redis repl_diskless_replication","url":"//2017/03/06/redis-repl_diskless_replication/","content":"redis diskless replication\n```conf\n# Replication\nrole:slave\nmaster_host:192.168.10.226\nmaster_port:6379\nmaster_link_status:down\nmaster_last_io_seconds_ago:-1\nmaster_sync_in_progress:1\nslave_repl_offset:1\nmaster_sync_left_bytes:-4022404224\nmaster_sync_last_io_seconds_ago:37\nmaster_link_down_since_seconds:1488785860\nslave_priority:100\nslave_read_only:1\nconnected_slaves:0\nmaster_repl_offset:0\nrepl_backlog_active:0\nrepl_backlog_size:1048576\nrepl_backlog_first_byte_offset:0\nrepl_backlog_histlen:0\n\n\n# Replication\nrole:master\nconnected_slaves:2\nslave0:ip=192.168.10.192,port=6379,state=online,offset=1134556373255,lag=1\nslave1:ip=192.168.10.82,port=6379,state=wait_bgsave,offset=0,lag=0\nmaster_repl_offset:1134556812751\nrepl_backlog_active:1\nrepl_backlog_size:1048576\nrepl_backlog_first_byte_offset:1134555764176\nrepl_backlog_histlen:1048576\n```\n\n\n\n```log\n6581:C 06 Mar 15:28:27.766 * DB saved on disk\n26581:C 06 Mar 15:28:27.990 * RDB: 212 MB of memory used by copy-on-write\n2251:M 06 Mar 15:28:28.379 * Background saving terminated with success\n2251:M 06 Mar 15:29:08.687 * Synchronization with slave 192.168.10.82:6379 succeeded\n2251:M 06 Mar 15:29:47.093 # Client id=314558845 addr=192.168.10.82:58002 fd=16 name= age=194 idle=1 flags=S db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=3617 omem=95797745 events=rw cmd=psync scheduled to be closed ASAP for overcoming of output buffer limits.\n2251:M 06 Mar 15:29:47.093 # Connection with slave 192.168.10.82:6379 lost.\n2251:M 06 Mar 15:30:15.999 * Slave 192.168.10.82:6379 asks for synchronization\n2251:M 06 Mar 15:30:15.999 * Unable to partial resync with slave 192.168.10.82:6379 for lack of backlog (Slave request was: 1134354232038).\n2251:M 06 Mar 15:30:15.999 * Starting BGSAVE for SYNC with target: disk\n2251:M 06 Mar 15:30:16.232 * Background saving started by pid 30494\n30494:C 06 Mar 15:31:52.413 * DB saved on disk\n30494:C 06 Mar 15:31:52.597 * RDB: 223 MB of memory used by copy-on-write\n2251:M 06 Mar 15:31:52.950 * Background saving terminated with success\n2251:M 06 Mar 15:32:33.375 * Synchronization with slave 192.168.10.82:6379 succeeded\n2251:M 06 Mar 15:33:08.106 # Client id=314561307 addr=192.168.10.82:38644 fd=16 name= age=173 idle=1 flags=S db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=4325 omem=102199438 events=rw cmd=psync scheduled to be closed ASAP for overcoming of output buffer limits.\n2251:M 06 Mar 15:33:08.106 # Connection with slave 192.168.10.82:6379 lost.\n2251:M 06 Mar 15:33:40.471 * Slave 192.168.10.82:6379 asks for synchronization\n2251:M 06 Mar 15:33:40.471 * Unable to partial resync with slave 192.168.10.82:6379 for lack of backlog (Slave request was: 1134453799057).\n2251:M 06 Mar 15:33:40.471 * Starting BGSAVE for SYNC with target: disk\n2251:M 06 Mar 15:33:40.715 * Background saving started by pid 1440\n2251:M 06 Mar 15:35:14.354 # Connection with slave 192.168.10.82:6379 lost.\n1440:C 06 Mar 15:35:20.564 * DB saved on disk\n1440:C 06 Mar 15:35:20.743 * RDB: 296 MB of memory used by copy-on-write\n2251:M 06 Mar 15:35:21.013 * Background saving terminated with success\n```\n\n\n### 原理分析\n如果设置了一个slave，不管是在第一次链接还是重新链接master的时候，slave会发送一个同步命令\n然后master开始后台保存，收集所有对修改数据的命令。当后台保存完成，master会将这个数据文件传送到slave，\n然后保存在磁盘，加载到内存中，master接着发送收集到的所有的修改数据的命令。\n\nRedis为避免输出缓冲区过度耗用内存，使用client-output-buffer-limit参数限制客户端输出缓冲区内存使用量。\nRedis数据复制过程中，Slave有个flags=S的客户端连接到Master; 它和其他客户端一样有输出缓冲区和缓冲区大小限制。\n`client-output-buffer-limit slave 256mb 64mb 60`\n当缓冲区使用超过256mb,Master会尽快杀掉它；\n当缓冲区使用大于64mb,且小于256mb的soft limit值时，并持续时间达60秒，也会被Master尽快杀掉。\n\n\n通过以上的原理分析我们可以得到一个解决方法：扩大buffer和timeout，但对于一个数据量比较大的db且服务器本身内存不足的情况下。\n我们可以采用diskless replication，直接通过网络将数据更新过去，不再让服务器去同步到disk再从disk输出到内存导致output报错\n\n\n### 解决\n开启redis diskless replication\n\n```shell\n127.0.0.1:6379> CONFIG set repl-diskless-sync yes\nOK\n(1.97s)\n```\n```log\n2251:M 06 Mar 15:35:34.896 * Slave 192.168.10.82:6379 asks for synchronization\n2251:M 06 Mar 15:35:34.896 * Full resync requested by slave 192.168.10.82:6379\n2251:M 06 Mar 15:35:34.896 * Delay next BGSAVE for SYNC\n2251:M 06 Mar 15:35:40.203 * Starting BGSAVE for SYNC with target: slaves sockets\n2251:M 06 Mar 15:35:40.469 * Background RDB transfer started by pid 3351\n3351:C 06 Mar 15:37:03.206 * RDB: 233 MB of memory used by copy-on-write\n2251:M 06 Mar 15:37:03.473 * Background RDB transfer terminated with success\n2251:M 06 Mar 15:37:03.473 # Slave 192.168.10.82:6379 correctly received the streamed RDB file.\n2251:M 06 Mar 15:37:03.473 * Streamed RDB transfer with slave 192.168.10.82:6379 succeeded (socket). Waiting for REPLCONF ACK from slave to enable streaming\n2251:M 06 Mar 15:37:24.848 * 50000 changes in 60 seconds. Saving...\n2251:M 06 Mar 15:37:25.097 * Background saving started by pid 5916\n2251:M 06 Mar 15:38:10.510 * Synchronization with slave 192.168.10.82:6379 succeeded\n5916:C 06 Mar 15:39:06.535 * DB saved on disk\n5916:C 06 Mar 15:39:06.742 * RDB: 250 MB of memory used by copy-on-write\n2251:M 06 Mar 15:39:07.093 * Background saving terminated with success\n```\n```conf\n# Replication\nrole:slave\nmaster_host:192.168.10.226\nmaster_port:6379\nmaster_link_status:up\nmaster_last_io_seconds_ago:1\nmaster_sync_in_progress:0\nslave_repl_offset:1134697190334\nslave_priority:100\nslave_read_only:1\nconnected_slaves:0\nmaster_repl_offset:0\nrepl_backlog_active:0\nrepl_backlog_size:1048576\nrepl_backlog_first_byte_offset:0\nrepl_backlog_histlen:0\n\n# Replication\nrole:master\nconnected_slaves:2\nslave0:ip=192.168.10.192,port=6379,state=online,offset=1135553694283,lag=1\nslave1:ip=192.168.10.82,port=6379,state=online,offset=1135553653727,lag=1\nmaster_repl_offset:1135554186742\nrepl_backlog_active:1\nrepl_backlog_size:1048576\nrepl_backlog_first_byte_offset:1135553138167\nrepl_backlog_histlen:1048576\n```","tags":["redis"],"categories":["redis"]},{"title":"redis-trib.rb工具使用","url":"//2017/02/27/redis-trib-rb/","content":"redis-trib.rb是redis官方推出的管理redis集群的工具，集成在redis的源码src目录下，是基于redis提供的集群命令封装成简单、便捷、实用的操作工具。redis-trib.rb是redis作者用ruby完成的。\n```\nroot@p-hsg-redis-1:~# redis-trib.rb\nUsage: redis-trib  \n\n  create          host1:port1 ... hostN:portN\n                  --replicas \n  check           host:port\n  info            host:port\n  fix             host:port\n                  --timeout \n  reshard         host:port\n                  --from \n                  --to \n                  --slots \n                  --yes\n                  --timeout \n                  --pipeline \n  rebalance       host:port\n                  --weight \n                  --auto-weights\n                  --use-empty-masters\n                  --timeout \n                  --simulate\n                  --pipeline \n                  --threshold \n  add-node        new_host:new_port existing_host:existing_port\n                  --slave\n                  --master-id \n  del-node        host:port node_id\n  set-timeout     host:port milliseconds\n  call            host:port command arg arg .. arg\n  import          host:port\n                  --from \n                  --copy\n                  --replace\n  help            (show this help)\n\nFor check, fix, reshard, del-node, set-timeout you can specify the host and port of any working node in the cluster.\n```\n可以看到redis-trib.rb具有以下功能：\n\n* create：创建集群\n* check：检查集群\n* info：查看集群信息\n* fix：修复集群\n* reshard：在线迁移slot\n* rebalance：平衡集群节点slot数量\n* add-node：将新节点加入集群\n* del-node：从集群中删除节点\n* set-timeout：设置集群节点间心跳连接的超时时间\n* call：在集群全部节点上执行命令\n* import：将外部redis数据导入集群\n\n### create创建集群\n\ncreate命令可选replicas参数，replicas表示需要有几个slave。最简单命令使用如下：\n```\n$ruby redis-trib.rb create 10.180.157.199:6379 10.180.157.200:6379 10.180.157.201:6379\n```\n有一个slave的创建命令如下：\n```\n$ruby redis-trib.rb create --replicas 1 10.180.157.199:6379 10.180.157.200:6379 10.180.157.201:6379 10.180.157.202:6379  10.180.157.205:6379  10.180.157.208:6379 \n```\n创建流程如下：\n```\n1、首先为每个节点创建ClusterNode对象，包括连接每个节点。检查每个节点是否为独立且db为空的节点。执行load_info方法导入节点信息。\n2、检查传入的master节点数量是否大于等于3个。只有大于3个节点才能组成集群。\n3、计算每个master需要分配的slot数量，以及给master分配slave。分配的算法大致如下：\n先把节点按照host分类，这样保证master节点能分配到更多的主机中。\n不停遍历遍历host列表，从每个host列表中弹出一个节点，放入interleaved数组。直到所有的节点都弹出为止。\nmaster节点列表就是interleaved前面的master数量的节点列表。保存在masters数组。\n计算每个master节点负责的slot数量，保存在slots_per_node对象，用slot总数除以master数量取整即可。\n遍历masters数组，每个master分配slots_per_node个slot，最后一个master，分配到16384个slot为止。\n接下来为master分配slave，分配算法会尽量保证master和slave节点不在同一台主机上。对于分配完指定slave数量的节点，还有多余的节点，也会为这些节点寻找master。分配算法会遍历两次masters数组。\n第一次遍历masters数组，在余下的节点列表找到replicas数量个slave。每个slave为第一个和master节点host不一样的节点，如果没有不一样的节点，则直接取出余下列表的第一个节点。\n第二次遍历是在对于节点数除以replicas不为整数，则会多余一部分节点。遍历的方式跟第一次一样，只是第一次会一次性给master分配replicas数量个slave，而第二次遍历只分配一个，直到余下的节点被全部分配出去。\n4、打印出分配信息，并提示用户输入“yes”确认是否按照打印出来的分配方式创建集群。\n5、输入“yes”后，会执行flush_nodes_config操作，该操作执行前面的分配结果，给master分配slot，让slave复制master，对于还没有握手（cluster meet）的节点，slave复制操作无法完成，不过没关系，flush_nodes_config操作出现异常会很快返回，后续握手后会再次执行flush_nodes_config。\n6、给每个节点分配epoch，遍历节点，每个节点分配的epoch比之前节点大1。\n7、节点间开始相互握手，握手的方式为节点列表的其他节点跟第一个节点握手。\n8、然后每隔1秒检查一次各个节点是否已经消息同步完成，使用ClusterNode的get_config_signature方法，检查的算法为获取每个节点cluster nodes信息，排序每个节点，组装成node_id1:slots|node_id2:slot2|...的字符串。如果每个节点获得字符串都相同，即认为握手成功。\n9、此后会再执行一次flush_nodes_config，这次主要是为了完成slave复制操作。\n10、最后再执行check_cluster，全面检查一次集群状态。包括和前面握手时检查一样的方式再检查一遍。确认没有迁移的节点。确认所有的slot都被分配出去了。\n11、至此完成了整个创建流程，返回[OK] All 16384 slots covered.。\n```\n\n### check检查集群\n\n检查集群状态的命令，没有其他参数，只需要选择一个集群中的一个节点即可。执行命令以及结果如下：\n```shell\n$ruby redis-trib.rb check 10.180.157.199:6379\n>>> Performing Cluster Check (using node 10.180.157.199:6379)\nM: b2506515b38e6bbd3034d540599f4cd2a5279ad1 10.180.157.199:6379\n   slots:0-5460 (5461 slots) master\n   1 additional replica(s)\nS: d376aaf80de0e01dde1f8cd4647d5ac3317a8641 10.180.157.205:6379\n   slots: (0 slots) slave\n   replicates e36c46dbe90960f30861af00786d4c2064e63df2\nM: 15126fb33796c2c26ea89e553418946f7443d5a5 10.180.157.201:6379\n   slots:10923-16383 (5461 slots) master\n   1 additional replica(s)\nS: 59fa6ee455f58a5076f6d6f83ddd74161fd7fb55 10.180.157.208:6379\n   slots: (0 slots) slave\n   replicates 15126fb33796c2c26ea89e553418946f7443d5a5\nS: 460b3a11e296aafb2615043291b7dd98274bb351 10.180.157.202:6379\n   slots: (0 slots) slave\n   replicates b2506515b38e6bbd3034d540599f4cd2a5279ad1\nM: e36c46dbe90960f30861af00786d4c2064e63df2 10.180.157.200:6379\n   slots:5461-10922 (5462 slots) master\n   1 additional replica(s)\n[OK] All nodes agree about slots configuration.\n>>> Check for open slots...\n>>> Check slots coverage...\n[OK] All 16384 slots covered. \n\n```\n\n检查前会先执行load_cluster_info_from_node方法，把所有节点数据load进来。load的方式为通过自己的cluster nodes发现其他节点，然后连接每个节点，并加入nodes数组。接着生成节点间的复制关系。\n\nload完数据后，开始检查数据，检查的方式也是调用创建时候使用的check_cluster。\n\n### info查看集群信息\n\ninfo命令用来查看集群的信息。info命令也是先执行load_cluster_info_from_node获取完整的集群信息。然后显示ClusterNode的info_string结果，示例如下：\n```shell\n$ruby redis-trib.rb info 10.180.157.199:6379\n10.180.157.199:6379 (b2506515...) -> 0 keys | 5461 slots | 1 slaves.\n10.180.157.201:6379 (15126fb3...) -> 0 keys | 5461 slots | 1 slaves.\n10.180.157.200:6379 (e36c46db...) -> 0 keys | 5462 slots | 1 slaves.\n[OK] 0 keys in 3 masters.\n0.00 keys per slot on average.\n\n```\n### fix修复集群\n\nfix命令的流程跟check的流程很像，显示加载集群信息，然后在check_cluster方法内传入fix为\ntrue的变量，会在集群检查出现异常的时候执行修复流程。目前fix命令能修复两种异常，一种是集群有处于迁移中的slot的节点，一种是slot未完全分配的异常。\n\nfix_open_slot方法是修复集群有处于迁移中的slot的节点异常。\n```\n1、先检查该slot是谁负责的，迁移的源节点如果没完成迁移，owner还是该节点。没有owner的slot无法完成修复功能。\n2、遍历每个节点，获取哪些节点标记该slot为migrating状态，哪些节点标记该slot为importing状态。对于owner不是该节点，但是通过cluster countkeysinslot获取到该节点有数据的情况，也认为该节点为importing状态。\n3、如果migrating和importing状态的节点均只有1个，这可能是迁移过程中redis-trib.rb被中断所致，直接执行move_slot继续完成迁移任务即可。传递dots和fix为true。\n4、如果migrating为空，importing状态的节点大于0，那么这种情况执行回滚流程，将importing状态的节点数据通过move_slot方法导给slot的owner节点，传递dots、fix和cold为true。接着对importing的节点执行cluster stable命令恢复稳定。\n5、如果importing状态的节点为空，有一个migrating状态的节点，而且该节点在当前slot没有数据，那么可以直接把这个slot设为stable。\n6、如果migrating和importing状态不是上述情况，目前redis-trib.rb工具无法修复，上述的三种情况也已经覆盖了通过redis-trib.rb工具迁移出现异常的各个方面，人为的异常情形太多，很难考虑完全。\nfix_slots_coverage方法能修复slot未完全分配的异常。未分配的slot有三种状态。\n\n1、所有节点的该slot都没有数据。该状态redis-trib.rb工具直接采用随机分配的方式，并没有考虑节点的均衡。本人尝试对没有分配slot的集群通过fix修复集群，结果slot还是能比较平均的分配，但是没有了连续性，打印的slot信息非常离散。\n2、有一个节点的该slot有数据。该状态下，直接把slot分配给该slot有数据的节点。\n3、有多个节点的该slot有数据。此种情况目前还处于TODO状态，不过redis作者列出了修复的步骤，对这些节点，除第一个节点，执行cluster migrating命令，然后把这些节点的数据迁移到第一个节点上。清除migrating状态，然后把slot分配给第一个节点。\n```\n\n### reshard在线迁移slot\n\nreshard命令可以在线把集群的一些slot从集群原来slot负责节点迁移到新的节点，利用reshard可以完成集群的在线横向扩容和缩容。\n\nreshard的参数很多，下面来一一解释一番：\n```\nreshard         host:port\n                --from \n                --to \n                --slots \n                --yes\n                --timeout \n                --pipeline \nhost:port：这个是必传参数，用来从一个节点获取整个集群信息，相当于获取集群信息的入口。\n--from ：需要从哪些源节点上迁移slot，可从多个源节点完成迁移，以逗号隔开，传递的是节点的node id，还可以直接传递--from all，这样源节点就是集群的所有节点，不传递该参数的话，则会在迁移过程中提示用户输入。\n--to ：slot需要迁移的目的节点的node id，目的节点只能填写一个，不传递该参数的话，则会在迁移过程中提示用户输入。\n--slots ：需要迁移的slot数量，不传递该参数的话，则会在迁移过程中提示用户输入。\n--yes：设置该参数，可以在打印执行reshard计划的时候，提示用户输入yes确认后再执行reshard。\n--timeout ：设置migrate命令的超时时间。\n--pipeline ：定义cluster getkeysinslot命令一次取出的key数量，不传的话使用默认值为10。\n```\n迁移的流程如下：\n```\n1、通过load_cluster_info_from_node方法装载集群信息。\n2、执行check_cluster方法检查集群是否健康。只有健康的集群才能进行迁移。\n3、获取需要迁移的slot数量，用户没传递--slots参数，则提示用户手动输入。\n4、获取迁移的目的节点，用户没传递--to参数，则提示用户手动输入。此处会检查目的节点必须为master节点。\n5、获取迁移的源节点，用户没传递--from参数，则提示用户手动输入。此处会检查源节点必须为master节点。--from all的话，源节点就是除了目的节点外的全部master节点。这里为了保证集群slot分配的平均，建议传递--from all。\n6、执行compute_reshard_table方法，计算需要迁移的slot数量如何分配到源节点列表，采用的算法是按照节点负责slot数量由多到少排序，计算每个节点需要迁移的slot的方法为：迁移slot数量 * (该源节点负责的slot数量 / 源节点列表负责的slot总数)。这样算出的数量可能不为整数，这里代码用了下面的方式处理：\n\nn = (numslots/source_tot_slots*s.slots.length)\nif i == 0\n    n = n.ceil\nelse\n    n = n.floor\n这样的处理方式会带来最终分配的slot与请求迁移的slot数量不一致，这个BUG已经在github上提给作者，https://github.com/antirez/redis/issues/2990。\n\n7、打印出reshard计划，如果用户没传--yes，就提示用户确认计划。\n8、根据reshard计划，一个个slot的迁移到新节点上，迁移使用move_slot方法，该方法被很多命令使用，具体可以参见下面的迁移流程。move_slot方法传递dots为true和pipeline数量。\n9、至此，就完成了全部的迁移任务。\n```\n```\nruby redis-trib.rb reshard --from all --to 80b661ecca260c89e3d8ea9b98f77edaeef43dcd --slots 11 10.180.157.199:6379\n>>> Performing Cluster Check (using node 10.180.157.199:6379)\nS: b2506515b38e6bbd3034d540599f4cd2a5279ad1 10.180.157.199:6379\n   slots: (0 slots) slave\n   replicates 460b3a11e296aafb2615043291b7dd98274bb351\nS: d376aaf80de0e01dde1f8cd4647d5ac3317a8641 10.180.157.205:6379\n   slots: (0 slots) slave\n   replicates e36c46dbe90960f30861af00786d4c2064e63df2\nM: 15126fb33796c2c26ea89e553418946f7443d5a5 10.180.157.201:6379\n   slots:10923-16383 (5461 slots) master\n   1 additional replica(s)\nS: 59fa6ee455f58a5076f6d6f83ddd74161fd7fb55 10.180.157.208:6379\n   slots: (0 slots) slave\n   replicates 15126fb33796c2c26ea89e553418946f7443d5a5\nM: 460b3a11e296aafb2615043291b7dd98274bb351 10.180.157.202:6379\n   slots:0-5460 (5461 slots) master\n   1 additional replica(s)\nM: 80b661ecca260c89e3d8ea9b98f77edaeef43dcd 10.180.157.200:6380\n   slots: (0 slots) master\n   0 additional replica(s)\nM: e36c46dbe90960f30861af00786d4c2064e63df2 10.180.157.200:6379\n   slots:5461-10922 (5462 slots) master\n   1 additional replica(s)\n[OK] All nodes agree about slots configuration.\n>>> Check for open slots...\n>>> Check slots coverage...\n[OK] All 16384 slots covered.\n\nReady to move 11 slots.\n  Source nodes:\n    M: 15126fb33796c2c26ea89e553418946f7443d5a5 10.180.157.201:6379\n   slots:10923-16383 (5461 slots) master\n   1 additional replica(s)\n    M: 460b3a11e296aafb2615043291b7dd98274bb351 10.180.157.202:6379\n   slots:0-5460 (5461 slots) master\n   1 additional replica(s)\n    M: e36c46dbe90960f30861af00786d4c2064e63df2 10.180.157.200:6379\n   slots:5461-10922 (5462 slots) master\n   1 additional replica(s)\n  Destination node:\n    M: 80b661ecca260c89e3d8ea9b98f77edaeef43dcd 10.180.157.200:6380\n   slots: (0 slots) master\n   0 additional replica(s)\n  Resharding plan:\n    Moving slot 5461 from e36c46dbe90960f30861af00786d4c2064e63df2\n    Moving slot 5462 from e36c46dbe90960f30861af00786d4c2064e63df2\n    Moving slot 5463 from e36c46dbe90960f30861af00786d4c2064e63df2\n    Moving slot 5464 from e36c46dbe90960f30861af00786d4c2064e63df2\n    Moving slot 0 from 460b3a11e296aafb2615043291b7dd98274bb351\n    Moving slot 1 from 460b3a11e296aafb2615043291b7dd98274bb351\n    Moving slot 2 from 460b3a11e296aafb2615043291b7dd98274bb351\n    Moving slot 10923 from 15126fb33796c2c26ea89e553418946f7443d5a5\n    Moving slot 10924 from 15126fb33796c2c26ea89e553418946f7443d5a5\n    Moving slot 10925 from 15126fb33796c2c26ea89e553418946f7443d5a5\nDo you want to proceed with the proposed reshard plan (yes/no)? yes\nMoving slot 5461 from 10.180.157.200:6379 to 10.180.157.200:6380:\nMoving slot 5462 from 10.180.157.200:6379 to 10.180.157.200:6380:\nMoving slot 5463 from 10.180.157.200:6379 to 10.180.157.200:6380:\nMoving slot 5464 from 10.180.157.200:6379 to 10.180.157.200:6380:\nMoving slot 0 from 10.180.157.202:6379 to 10.180.157.200:6380:\nMoving slot 1 from 10.180.157.202:6379 to 10.180.157.200:6380:\nMoving slot 2 from 10.180.157.202:6379 to 10.180.157.200:6380:\nMoving slot 10923 from 10.180.157.201:6379 to 10.180.157.200:6380:\nMoving slot 10924 from 10.180.157.201:6379 to 10.180.157.200:6380:\nMoving slot 10925 from 10.180.157.201:6379 to 10.180.157.200:6380:\n```\n\nmove_slot方法可以在线将一个slot的全部数据从源节点迁移到目的节点，fix、reshard、rebalance都需要调用该方法迁移slot。\n\nmove_slot接受下面几个参数，\n```\n1、pipeline：设置一次从slot上获取多少个key。\n2、quiet：迁移会打印相关信息，设置quiet参数，可以不用打印这些信息。\n3、cold：设置cold，会忽略执行importing和migrating。\n4、dots：设置dots，则会在迁移过程打印迁移key数量的进度。\n5、update：设置update，则会更新内存信息，方便以后的操作。\n```\nmove_slot流程如下：\n```\n1、如果没有设置cold，则对源节点执行cluster importing命令，对目的节点执行migrating命令。fix的时候有可能importing和migrating已经执行过来，所以此种场景会设置cold。\n2、通过cluster getkeysinslot命令，一次性获取远节点迁移slot的pipeline个key的数量.\n3、对这些key执行migrate命令，将数据从源节点迁移到目的节点。\n4、如果migrate出现异常，在fix模式下，BUSYKEY的异常，会使用migrate的replace模式再执行一次，BUSYKEY表示目的节点已经有该key了，replace模式可以强制替换目的节点的key。不是fix模式就直接返回错误了。\n5、循环执行cluster getkeysinslot命令，直到返回的key数量为0，就退出循环。\n6、如果没有设置cold，对每个节点执行cluster setslot命令，把slot赋给目的节点。\n7、如果设置update，则修改源节点和目的节点的slot信息。\n8、至此完成了迁移slot的流程。\n```\n### rebalance平衡集群节点slot数量\n\nrebalance命令可以根据用户传入的参数平衡集群节点的slot数量，rebalance功能非常强大，可以传入的参数很多，以下是rebalance的参数列表和命令示例。\n```\nrebalance       host:port\n                --weight \n                --auto-weights\n                --threshold \n                --use-empty-masters\n                --timeout \n                --simulate\n                --pipeline \n\n$ruby redis-trib.rb rebalance --threshold 1 --weight b31e3a2e=5 --weight 60b8e3a1=5 --use-empty-masters  --simulate 10.180.157.199:6379\n下面也先一一解释下每个参数的用法：\n\nhost:port：这个是必传参数，用来从一个节点获取整个集群信息，相当于获取集群信息的入口。\n--weight ：节点的权重，格式为node_id=weight，如果需要为多个节点分配权重的话，需要添加多个--weight 参数，即--weight b31e3a2e=5 --weight 60b8e3a1=5，node_id可为节点名称的前缀，只要保证前缀位数能唯一区分该节点即可。没有传递–weight的节点的权重默认为1。\n--auto-weights：这个参数在rebalance流程中并未用到。\n--threshold ：只有节点需要迁移的slot阈值超过threshold，才会执行rebalance操作。具体计算方法可以参考下面的rebalance命令流程的第四步。\n--use-empty-masters：rebalance是否考虑没有节点的master，默认没有分配slot节点的master是不参与rebalance的，设置--use-empty-masters可以让没有分配slot的节点参与rebalance。\n--timeout ：设置migrate命令的超时时间。\n--simulate：设置该参数，可以模拟rebalance操作，提示用户会迁移哪些slots，而不会真正执行迁移操作。\n--pipeline ：与reshar的pipeline参数一样，定义cluster getkeysinslot命令一次取出的key数量，不传的话使用默认值为10。\n```\nrebalance命令流程如下：\n```\n1、load_cluster_info_from_node方法先加载集群信息。\n2、计算每个master的权重，根据参数--weight ，为每个设置的节点分配权重，没有设置的节点，则权重默认为1。\n3、根据每个master的权重，以及总的权重，计算自己期望被分配多少个slot。计算的方式为：总slot数量 * （自己的权重 / 总权重）。\n4、计算每个master期望分配的slot是否超过设置的阈值，即--threshold 设置的阈值或者默认的阈值。计算的方式为：先计算期望移动节点的阈值，算法为：(100-(100.0*expected/n.slots.length)).abs，如果计算出的阈值没有超出设置阈值，则不需要为该节点移动slot。只要有一个master的移动节点超过阈值，就会触发rebalance操作。\n5、如果触发了rebalance操作。那么就开始执行rebalance操作，先将每个节点当前分配的slots数量减去期望分配的slot数量获得balance值。将每个节点的balance从小到大进行排序获得sn数组。\n6、用dst_idx和src_idx游标分别从sn数组的头部和尾部开始遍历。目的是为了把尾部节点的slot分配给头部节点。\n\nsn数组保存的balance列表排序后，负数在前面，正数在后面。负数表示需要有slot迁入，所以使用dst_idx游标，正数表示需要有slot迁出，所以使用src_idx游标。理论上sn数组各节点的balance值加起来应该为0，不过由于在计算期望分配的slot的时候只是使用直接取整的方式，所以可能出现balance值之和不为0的情况，balance值之和不为0即为节点不平衡的slot数量，由于slot总数有16384个，不平衡数量相对于总数，基数很小，所以对rebalance流程影响不大。\n\n7、获取sn[dst_idx]和sn[src_idx]的balance值较小的那个值，该值即为需要从sn[src_idx]节点迁移到sn[dst_idx]节点的slot数量。\n8、接着通过compute_reshard_table方法计算源节点的slot如何分配到源节点列表。这个方法在reshard流程中也有调用，具体步骤可以参考reshard流程的第六步。\n9、如果是simulate模式，则只是打印出迁移列表。\n10、如果没有设置simulate，则执行move_slot操作，迁移slot，传入的参数为:quiet=>true,:dots=>false,:update=>true。\n11、迁移完成后更新sn[dst_idx]和sn[src_idx]的balance值。如果balance值为0后，游标向前进1。\n12、直到dst_idx到达src_idx游标，完成整个rebalance操作。\n```\n### add-node将新节点加入集群\n\nadd-node命令可以将新节点加入集群，节点可以为master，也可以为某个master节点的slave。\n```shell\nadd-node    new_host:new_port existing_host:existing_port\n          --slave\n          --master-id \nadd-node有两个可选参数：\n\n--slave：设置该参数，则新节点以slave的角色加入集群\n--master-id：这个参数需要设置了--slave才能生效，--master-id用来指定新节点的master节点。如果不设置该参数，则会随机为节点选择master节点。\n```\n可以看下add-node命令的执行示例：\n```\n$ruby redis-trib.rb add-node --slave --master-id dcb792b3e85726f012e83061bf237072dfc45f99 10.180.157.202:6379 10.180.157.199:6379\n>>> Adding node 10.180.157.202:6379 to cluster 10.180.157.199:6379\n>>> Performing Cluster Check (using node 10.180.157.199:6379)\nM: dcb792b3e85726f012e83061bf237072dfc45f99 10.180.157.199:6379\n   slots:0-5460 (5461 slots) master\n   0 additional replica(s)\nM: 464d740bf48953ebcf826f4113c86f9db3a9baf3 10.180.157.201:6379\n   slots:10923-16383 (5461 slots) master\n   0 additional replica(s)\nM: befa7e17b4e5f239e519bc74bfef3264a40f96ae 10.180.157.200:6379\n   slots:5461-10922 (5462 slots) master\n   0 additional replica(s)\n[OK] All nodes agree about slots configuration.\n>>> Check for open slots...\n>>> Check slots coverage...\n[OK] All 16384 slots covered.\n>>> Send CLUSTER MEET to node 10.180.157.202:6379 to make it join the cluster.\nWaiting for the cluster to join.\n>>> Configure node as replica of 10.180.157.199:6379.\n[OK] New node added correctly.\n```\nadd-node流程如下：\n```\n1、通过load_cluster_info_from_node方法转载集群信息，check_cluster方法检查集群是否健康。\n2、如果设置了--slave，则需要为该节点寻找master节点。设置了--master-id，则以该节点作为新节点的master，如果没有设置--master-id，则调用get_master_with_least_replicas方法，寻找slave数量最少的master节点。如果slave数量一致，则选取load_cluster_info_from_node顺序发现的第一个节点。load_cluster_info_from_node顺序的第一个节点是add-node设置的existing_host:existing_port节点，后面的顺序根据在该节点执行cluster nodes返回的结果返回的节点顺序。\n3、连接新的节点并与集群第一个节点握手。\n4、如果没设置–slave就直接返回ok，设置了–slave，则需要等待确认新节点加入集群，然后执行cluster replicate命令复制master节点。\n5、至此，完成了全部的增加节点的流程。\n```\n### del-node从集群中删除节点\n\ndel-node可以把某个节点从集群中删除。del-node只能删除没有分配slot的节点。删除命令传递两个参数：\n```\nhost:port：从该节点获取集群信息。\nnode_id：需要删除的节点id。\n```\n\ndel-node执行结果示例如下：\n```\n$ruby redis-trib.rb del-node 10.180.157.199:6379 d5f6d1d17426bd564a6e309f32d0f5b96962fe53\n>>> Removing node d5f6d1d17426bd564a6e309f32d0f5b96962fe53 from cluster 10.180.157.199:6379\n>>> Sending CLUSTER FORGET messages to the cluster...\n>>> SHUTDOWN the node.\n```\n\ndel-node流程如下：\n```\n1、通过load_cluster_info_from_node方法转载集群信息。\n2、根据传入的node id获取节点，如果节点没找到，则直接提示错误并退出。\n3、如果节点分配的slot不为空，则直接提示错误并退出。\n4、遍历集群内的其他节点，执行cluster forget命令，从每个节点中去除该节点。如果删除的节点是master，而且它有slave的话，这些slave会去复制其他master，调用的方法是get_master_with_least_replicas，与add-node没设置--master-id寻找master的方法一样。\n5、然后关闭该节点。\n```\n### set-timeout设置集群节点间心跳连接的超时时间\n\nset-timeout用来设置集群节点间心跳连接的超时时间，单位是毫秒，不得小于100毫秒，因为100毫秒对于心跳时间来说太短了。该命令修改是节点配置参数cluster-node-timeout，默认是15000毫秒。通过该命令，可以给每个节点设置超时时间，设置的方式使用config set命令动态设置，然后执行config rewrite命令将配置持久化保存到硬盘。以下是示例：\n```\nruby redis-trib.rb set-timeout 10.180.157.199:6379 30000\n>>> Reconfiguring node timeout in every cluster node...\n*** New timeout set for 10.180.157.199:6379\n*** New timeout set for 10.180.157.205:6379\n*** New timeout set for 10.180.157.201:6379\n*** New timeout set for 10.180.157.200:6379\n*** New timeout set for 10.180.157.208:6379\n>>> New node timeout set. 5 OK, 0 ERR.\n```\n### call在集群全部节点上执行命令\n\ncall命令可以用来在集群的全部节点执行相同的命令。call命令也是需要通过集群的一个节点地址，连上整个集群，然后在集群的每个节点执行该命令。\n```\n$ruby redis-trib.rb call 10.180.157.199:6379 get key\n>>> Calling GET key\n10.180.157.199:6379: MOVED 12539 10.180.157.201:6379\n10.180.157.205:6379: MOVED 12539 10.180.157.201:6379\n10.180.157.201:6379:\n10.180.157.200:6379: MOVED 12539 10.180.157.201:6379\n10.180.157.208:6379: MOVED 12539 10.180.157.201:6379\n```\n### import将外部redis数据导入集群\n\nimport命令可以把外部的redis节点数据导入集群。导入的流程如下：\n```\n1、通过load_cluster_info_from_node方法转载集群信息，check_cluster方法检查集群是否健康。\n2、连接外部redis节点，如果外部节点开启了cluster_enabled，则提示错误。\n3、通过scan命令遍历外部节点，一次获取1000条数据。\n4、遍历这些key，计算出key对应的slot。\n5、执行migrate命令,源节点是外部节点,目的节点是集群slot对应的节点，如果设置了--copy参数，则传递copy参数，如果设置了--replace，则传递replace参数。\n6、不停执行scan命令，直到遍历完全部的key。\n7、至此完成整个迁移流程\n这中间如果出现异常，程序就会停止。没使用--copy模式，则可以重新执行import命令，使用--copy的话，最好清空新的集群再导入一次。\n```\nimport命令更适合离线的把外部redis数据导入，在线导入的话最好使用更专业的导入工具，以slave的方式连接redis节点去同步节点数据应该是更好的方式。","tags":["redis"],"categories":["redis"]},{"title":"redis-trib.rb构建redis集群","url":"//2017/02/27/redis-trib-rb-create-cluster/","content":"redis-cluster\n\n使用官方的redis-trib.rb构建redis集群,要让redis cluster集群正常工作需要有三个主节点，因此我们在三台机器上部署6个节点，每台机器一个master一个slave。\n\n环境：\np-hsg-redis-1  192.168.10.81   7000/7001\np-hsg-redis-2\t 192.168.10.82   7000/7001\np-hsg-redis-3\t 192.168.10.83   7000/7001\n\n\n部署流程：\n\n1.安装redis－server\n\n我这里已经有saltstack的自动化构建脚本，因此使用salt安装软件。具体redis安装方法不再赘述。\nsalt-call state.sls redis.cluster\n\n2.修改配置文件\n```shell\n\troot@p-hsg-redis-1:/etc/redis# cat /etc/redis/7000.conf\n\tprotected-mode no\n\tdaemonize yes\n\tport 7000\n\tcluster-enabled yes\n\tcluster-config-file nodes-7000.conf\n\tcluster-node-timeout 15000\n\tappendonly yes\n\tdir /var/lib/redis/7000\n```\n3.启动redis节点\n```shell\n\troot@p-hsg-redis-3:/etc/redis# ps -ef | grep redis\n\troot     30431     1  0 15:12 ?        00:00:00 /usr/local/bin/redis-server *:7001 [cluster]\n\troot     30446     1  0 15:12 ?        00:00:00 /usr/local/bin/redis-server *:7000 [cluster]\n\troot     30450 11073  0 15:12 pts/1    00:00:00 grep --color=auto redis\n```\n4.创建集群\n```shell\n\tredis-trib.rb create --replicas 1 192.168.10.81:7001 192.168.10.81:7000 192.168.10.82:7000 192.168.10.82:7001 192.168.10.83:7000 192.168.10.83:7001\n\t>>> Creating cluster\n\t>>> Performing hash slots allocation on 6 nodes...\n\tUsing 3 masters:\n\t192.168.10.81:7001\n\t192.168.10.82:7000\n\t192.168.10.83:7000\n\tAdding replica 192.168.10.82:7001 to 192.168.10.81:7001\n\tAdding replica 192.168.10.81:7000 to 192.168.10.82:7000\n\tAdding replica 192.168.10.83:7001 to 192.168.10.83:7000\n\tM: 5fd250591aa474d6370e1df547959c5895716192 192.168.10.81:7001\n\t   slots:0-5460 (5461 slots) master\n\tS: ef8bf8a326894fee37a398d3f88de3120fc71ea5 192.168.10.81:7000\n\t   replicates dc62f19b8894db1816f7b92e3fe05b8244832d3e\n\tM: dc62f19b8894db1816f7b92e3fe05b8244832d3e 192.168.10.82:7000\n\t   slots:5461-10922 (5462 slots) master\n\tS: eb2c7cc3cb56bb5ee124d80e353a91331d092042 192.168.10.82:7001\n\t   replicates 5fd250591aa474d6370e1df547959c5895716192\n\tM: 1773d96052f573361a13b5e9015f947b340d45cd 192.168.10.83:7000\n\t   slots:10923-16383 (5461 slots) master\n\tS: 670f158355b93b4a7ac30d69dc3e1e8c4c484a9f 192.168.10.83:7001\n\t   replicates 1773d96052f573361a13b5e9015f947b340d45cd\n\tCan I set the above configuration? (type 'yes' to accept): yes\n\t>>> Nodes configuration updated\n\t>>> Assign a different config epoch to each node\n\t>>> Sending CLUSTER MEET messages to join the cluster\n\tWaiting for the cluster to join...\n\t>>> Performing Cluster Check (using node 192.168.10.81:7001)\n\tM: 5fd250591aa474d6370e1df547959c5895716192 192.168.10.81:7001\n\t   slots:0-5460 (5461 slots) master\n\t   1 additional replica(s)\n\tM: 1773d96052f573361a13b5e9015f947b340d45cd 192.168.10.83:7000\n\t   slots:10923-16383 (5461 slots) master\n\t   1 additional replica(s)\n\tS: ef8bf8a326894fee37a398d3f88de3120fc71ea5 192.168.10.81:7000\n\t   slots: (0 slots) slave\n\t   replicates dc62f19b8894db1816f7b92e3fe05b8244832d3e\n\tS: 670f158355b93b4a7ac30d69dc3e1e8c4c484a9f 192.168.10.83:7001\n\t   slots: (0 slots) slave\n\t   replicates 1773d96052f573361a13b5e9015f947b340d45cd\n\tM: dc62f19b8894db1816f7b92e3fe05b8244832d3e 192.168.10.82:7000\n\t   slots:5461-10922 (5462 slots) master\n\t   1 additional replica(s)\n\tS: eb2c7cc3cb56bb5ee124d80e353a91331d092042 192.168.10.82:7001\n\t   slots: (0 slots) slave\n\t   replicates 5fd250591aa474d6370e1df547959c5895716192\n\t[OK] All nodes agree about slots configuration.\n\t>>> Check for open slots...\n\t>>> Check slots coverage...\n\t[OK] All 16384 slots covered.\n\t```","tags":["cluster","redis"],"categories":["redis"]},{"title":"magent构建memcache集群","url":"//2017/02/09/magent-create-memcache-cluster/","content":"magent功能相当于一个proxy调度器，通过一定的算法将数据存到不同后端。magent只是一个调度器，不处理数据同步问题。（另外一个由twitter开源的twemproxy，功能作用类似。可参考：[twemproxy安装配置](/2017/02/09/2017-02-09-twemproxu/)）\n\n部署：\n  \nmemcached部署自行百度。\n  \nmagent参考：[repcache＋magent构建高可用memcache](/2017/02/09/2017-02-09-repcache-magent/)\n\n1. 启动3个Memcached进程,端口11221、11222、11223.\n```shell\n# memcached -u xiemx -d -p 11221\n# memcached -u xiemx -d -p 11222\n# memcached -u xiemx -d -p 11223\n\n```\n2. 启动magent进程，端口11211，11221、11222为master，11223为backup；\n```shell\nmagent -p 12000 -s 127.0.0.1:11221 -s127.0.0.1:11222 -b 127.0.0.1:11223\n```\n\n3. telnet 11211的magent，set key1和set key2，根据算法，key1/key2将被分配到11221/11222和11213端口的Memcached；\n```shell\n[root@memcache ~]# telnet 127.0.0.1 11211\nTrying 127.0.0.1...\nConnected to 127.0.0.1.\nEscape character is '^]'.\nstats\nmemcached agent v0.4\nmatrix 1 -> 127.0.0.1:11221, pool size 0\nmatrix 2 -> 127.0.0.1:11222, pool size 0\nEND\nset key1 0 0 1\n1\nSTORED\nset key2 0 0 1\n2\nSTORED\nquit\nConnection closed by foreign host.\n\n[root@memcache ~]# telnet 127.0.0.1 11221\nTrying 127.0.0.1...\nConnected to 127.0.0.1.\nEscape character is '^]'.\nget key1\nEND\nget key2\nVALUE key2 0 1\n2\nEND\nquit\nConnection closed by foreign host.\n\n[root@memcache ~]# telnet 127.0.0.1 11222\nTrying 127.0.0.1...\nConnected to 127.0.0.1.\nEscape character is '^]'.\nget key1\nVALUE key1 0 1\n1\nEND\nget key2\nEND\nquit\nConnection closed by foreign host.\n\n[root@memcache ~]# telnet 127.0.0.1 11223\nTrying 127.0.0.1...\nConnected to 127.0.0.1.\nEscape character is '^]'.\nget key1\nVALUE key1 0 1\n1\nEND\nget key2\nVALUE key2 0 1\n2\nEND\nquit\nConnection closed by foreign host.\n```\n4. 模拟11211、11212端口的Memcached挂掉  \n```shell\n[root@memcache ~]# ps -ef | grep memcached\nroot       6589    1   0 01:25 ?        00:00:00 memcached -u xiemx -d -p 11221\nroot       6591    1   0 01:25 ?        00:00:00 memcached -u xiemx -d -p 11222\nroot       6593    1   0 01:25 ?        00:00:00 memcached -u xiemx -d -p 11223\nroot       6609   6509   001:44 pts/0     00:00:00 grep memcached\n[root@memcache ~]# kill -9 6589\n[root@memcache ~]# kill -9 6591\n[root@memcache ~]# telnet 127.0.0.1 11211\nTrying 127.0.0.1...\nConnected to 127.0.0.1.\nEscape character is '^]'.\nget key1 \nVALUE key1 0 1\n1\nEND\nget key2\nVALUE key2 0 1\n2\nEND\nquit\nConnection closed by foreign host.\n```\n\n5. 模拟11211、11212端口的Memcached重启复活(恢复后的memcache是没有数据的，需要逻辑处理启动后导入数据)\n```shell\n[root@memcache ~]# memcached -m 1 -u root -d -l 127.0.0.1 -p 11221\n[root@memcache ~]# memcached -m 1 -u root -d -l 127.0.0.1 -p 11222\n[root@memcache ~]# telnet 127.0.0.1 11211\nTrying 127.0.0.1...\nConnected to 127.0.0.1.\nEscape character is '^]'.\nget key1\nEND\nget key2\nEND\nquit\nConnection closed by foreign host.\n```","tags":["database","cluster","memcached"],"categories":["memcached"]},{"title":"repcache＋magent构建高可用memcache","url":"//2017/02/09/repcache-magent/","content":"repcache+magnet\n\nmagent：一款开源的Memcached代理服务器软件，功能和mysql-proxy类似。\n  \nrepcache：日本开发的一款开源工具，使memcache能够做主从复制。可以通过patch包升级memcache，也可以下载包含replication的memcache版本\n\n先将master/slave通过replication构建自动复制的主主，通过magent将K／V，写入到master和backup中，当master宕机时，magent将所有读写请求交给slave，slave等待master启动，默认情况下master宕机重启后内存中的数据回丢失，但由于repcache的自动主主，master启动时会自动从salve端复制所有数据。\n\n### 安装部署\n```shell\nlibevent:\n  apt-get install -y libevent-dev\n\nmagent：\n  wget http://memagent.googlecode.com/files/magent-0.5.tar.gz  \n  tar zxvf magent-0.5.tar.gz  \n  /sbin/ldconfig  \n  sed -i \"s#LIBS = -levent#LIBS = -levent -lm#g\" Makefile  \n  make  \n  cp magent /usr/bin/magent  \n  \nrepcache:\n  wget http://downloads.sourceforge.net/repcached/memcached-1.2.8-repcached-2.2.tar.gz\n  tar xf memcached-1.2.8-repcached-2.2.tar.gz\n  cd memcached-1.2.8-repcached-2.2/\n  ./configure --prefix=/opt/repcached/ --enable-replication --program-transform-name=s/memcached/repcached/\n  make&&make install\n  ln -s /opt/repcached/bin/repcached /usr/bin/\n```\n\n### 编译错误汇总\n```shell\n### magent\n\n[root@test magent]# make \ngcc -Wall -g -O2 -I/usr/local/include -m64 -c -o magent.o magent.c\nmagent.c: In function ‘writev_list’:\nmagent.c:729: error: ‘SSIZE_MAX’ undeclared (first use in this function)\nmagent.c:729: error: (Each undeclared identifier is reported only once\nmagent.c:729: error: for each function it appears in.)\nmake: *** [magent.o] Error 1\n解决方法:\n[root@test magent]# vim ketama.h\n#ifndef SSIZE_MAX\n#define SSIZE_MAX      32767\n#endif\n#ifndef _KETAMA_H\n#define _KETAMA_H\n\n[root@test magent]# make \ngcc -Wall -O2 -g  -c -o magent.o magent.c\ngcc -Wall -O2 -g  -c -o ketama.o ketama.c\ngcc -Wall -O2 -g -o magent magent.o ketama.o -levent\nketama.o: In function `create_ketama':\n/opt/root/magent-0.5/ketama.c:399: undefined reference to `floorf'\ncollect2: ld returned 1 exit status\nmake: *** [magent] Error 1\n\n修改Makefile\nLIBS = -levent 为LIBS = -levent -lm\n#sed -i \"s#LIBS = -levent#LIBS = -levent -lm#g\" Makefile \n```\n```shell\n### repcached\n\n[root@test magent]# make \nmake all-recursive    \nmake[1]: Entering directory `/usr/local/memcached'    \nMaking all in doc    \nmake[2]: Entering directory `/usr/local/memcached/doc'    \nmake[2]: Nothing to be done for `all'.    \nmake[2]: Leaving directory `/usr/local/memcached/doc'    \nmake[2]: Entering directory `/usr/local/memcached'    \ngcc -DHAVE_CONFIG_H -I. -DNDEBUG -m64 -g -O2 -MT memcached-memcached.o -MD     \nMP -MF .deps/memcached-memcached.Tpo -c -o memcached-memcached.o `test -f     \nmemcached.c' || echo './'`memcached.c    \nmemcached.c: In function ‘add_iov’:    \nmemcached.c:697: error: ‘IOV_MAX’ undeclared (first use in this function)    \nmemcached.c:697: error: (Each undeclared identifier is reported only once    \nmemcached.c:697: error: for each function it appears in.)    \nmake[2]: *** [memcached-memcached.o] Error 1    \nmake[2]: Leaving directory `/usr/local/memcached'    \nmake[1]: *** [all-recursive] Error 1    \nmake[1]: Leaving directory `/usr/local/memcached'    \nmake: *** [all] Error 2\n    \n解决方案：    \nvi memcached.c\n    \n将下面的几行    \n/* FreeBSD 4.x doesn't have IOV_MAX exposed. */    \n#ifndef IOV_MAX    \n#if defined(__FreeBSD__) || defined(__APPLE__)    \n# define IOV_MAX 1024    \n#endif    \n#endif\n    \n修改为\n/* FreeBSD 4.x doesn't have IOV_MAX exposed. */    \n#ifndef IOV_MAX    \n# define IOV_MAX 1024    \n#endif\n```\n### 启动服务\n```shell\n### 启动repcached\n启动master：\nrepcached -d -v -x 127.0.0.1 -u vagrant  \n启动slave：\nrepcached -d -v -x 127.0.0.1 -u vagrant -p 11222\n\n参数说明：\n  -x 设置从哪个IP上进行同步。\n  -X 指定数据同步的端口。\n  Memcached默认服务端口是11211，默认同步监听端口是11212。\n\n### 启动magent\nmagent -u root -p 11233 -s 127.0.0.1:11211 -s 127.0.0.1:11222\n```\n","tags":["memcached","repcache","magent"],"categories":["memcached"]},{"title":"vagrant创建多个虚拟机","url":"//2017/02/09/vagrant-mulit-vm/","content":"建立多台VM，並且让他们之间能够相通信，一台是应用服务器、一台是DB服务器,参考如下\n\n```markdown\nVagrant.configure(\"2\") do |config|\n  config.vm.define :web do |web|\n    web.vm.provider \"virtualbox\" do |v|\n          v.customize [\"modifyvm\", :id, \"--name\", \"web\", \"--memory\", \"512\"]\n    end\n    web.vm.box = \"base\"\n    web.vm.hostname = \"xiemx-website\"\n    web.vm.network :private_network, ip: \"10.11.1.1\"\n  end\n\n  config.vm.define :db do |db|\n    db.vm.provider \"virtualbox\" do |v|\n          v.customize [\"modifyvm\", :id, \"--name\", \"db\", \"--memory\", \"512\"]\n    end\n    db.vm.box = \"base\"\n    db.vm.hostname = \"xiemx-db\"\n    db.vm.network :private_network, ip: \"10.11.1.2\"\n  end\nend\n```","categories":["vagrant"]},{"title":"vagrant使用","url":"//2017/02/09/vagrant/","content":"删除文档，参考官方：[https://www.vagrantup.com/docs/cli/](https://www.vagrantup.com/docs/cli/)\n\n","tags":["vagrant"],"categories":["vagrant"]},{"title":"twemproxy配置文件详解","url":"//2017/02/09/twemproxy-configure/","content":"配置文件详解：\n```conf\nlisten\ntwemproxy监听的端口。可以以ip:port或name:port的形式来书写。\n \nhash\n可以选择的key值的hash算法：\n> one_at_a_time\n> md5\n> crc16\n> crc32(crc32 implementation compatible with libmemcached)\n> crc32a(correct crc32 implementation as per the spec)\n> fnv1_64\n> fnv1a_64\n> fnv1_32\n> fnv1a_32\n> hsieh\n> murmur\n> jenkins\n如果没选择，默认是fnv1a_64。\n\nhash_tag\nhash_tag允许根据key的一个部分来计算key的hash值。hash_tag由两个字符组成，一个是hash_tag的开始，另外一个是hash_tag的结束，在hash_tag的开始和结束之间，是将用于计算key的hash值的部分，计算的结果会用于选择服务器。\n \n例如：如果hash_tag被定义为”{}”，那么key值为\"user:{user1}:ids\"和\"user:{user1}:tweets\"的hash值都是基于”user1”，最终会被映射到相同的服务器。而\"user:user1:ids\"将会使用整个key来计算hash，可能会被映射到不同的服务器。\n \ndistribution\n存在ketama、modula和random3种可选的配置。其含义如下：\n \n\t(1)ketama\n\tketama一致性hash算法，会根据服务器构造出一个hash ring，并为ring上的节点分配hash范围。ketama的优势在于单个节点添加、删除之后，会最大程度上保持整个群集中缓存的key值可以被重用。\n\t \n\t(2)modula\n\tmodula非常简单，就是根据key值的hash值取模，根据取模的结果选择对应的服务器。\n\t \n\t(3)random\n\trandom是无论key值的hash是什么，都随机的选择一个服务器作为key值操作的目标。\n \ntimeout\n单位是毫秒，是连接到server的超时值。默认是永久等待。\n \nbacklog\n监听TCP 的backlog（连接等待队列）的长度，默认是512。\n \npreconnect\n是一个boolean值，指示twemproxy是否应该预连接pool中的server。默认是false。\n \nredis\n是一个boolean值，用来识别到服务器的通讯协议是redis还是memcached。默认是false。\n \nserver_connections\n每个server可以被打开的连接数。默认，每个服务器开一个连接。\n \nauto_eject_hosts\n是一个boolean值，用于控制twemproxy是否应该根据server的连接状态重建群集。这个连接状态是由server_failure_limit 阀值来控制。\n默认是false。\n \nserver_retry_timeout\n单位是毫秒，控制服务器连接的时间间隔，在auto_eject_host被设置为true的时候产生作用。默认是30000 毫秒。\n \nserver_failure_limit\n控制连接服务器的次数，在auto_eject_host被设置为true的时候产生作用。默认是2。\n \nservers\n一个pool中的服务器的地址、端口和权重的列表，包括一个可选的服务器的名字，如果提供服务器的名字，将会使用它决定server的次序，从而提供对应的一致性hash的hash ring。否则，将使用server被定义的次序\n```\n\n","tags":["redis","twemproxy","memcache"],"categories":["memcached","redis"]},{"title":"twemproxy构建redis/memcache集群","url":"//2017/02/09/twemproxu/","content":"twemproxy构建memcache集群，也可以用来构建reids集群。方法类似只是配置文件不同。\n\n官网描述：\n\t`A fast, light-weight proxy for memcached and redis`\n\n### 安装配置\n```shell\n$ apt-get install libtool automake -y\n$ git clone https://github.com/twitter/twemproxy.git\n$ cd twemproxy\n$ autoreconf -fvi\n$ ./configure\n$ make\n$ src/nutcracker -h\n\n\nroot@vagrant-ubuntu-trusty-64:~# nutcracker -h\nThis is nutcracker-0.4.1\n\nUsage: nutcracker [-?hVdDt] [-v verbosity level] [-o output file]\n                  [-c conf file] [-s stats port] [-a stats addr]\n                  [-i stats interval] [-p pid file] [-m mbuf size]\n\nOptions:\n  -h, --help             : this help\n  -V, --version          : show version and exit\n  -t, --test-conf        : test configuration for syntax errors and exit\n  -d, --daemonize        : run as a daemon\n  -D, --describe-stats   : print stats description and exit\n  -v, --verbose=N        : set logging level (default: 5, min: 0, max: 11)\n  -o, --output=S         : set logging file (default: stderr)\n  -c, --conf-file=S      : set configuration file (default: conf/nutcracker.yml)\n  -s, --stats-port=N     : set stats monitoring port (default: 22222)\n  -a, --stats-addr=S     : set stats monitoring ip (default: 0.0.0.0)\n  -i, --stats-interval=N : set stats aggregation interval in msec (default: 30000 msec)\n  -p, --pid-file=S       : set pid file (default: off)\n  -m, --mbuf-size=N      : set size of mbuf chunk in bytes (default: 16384 bytes)\n```\n\n  \n### 创建集群\n```shell\n#用tweaproxy启动代理一组memcached／redis非常简单，只需要指定配置文件即可，官方也提供了范本在仓库的conf/下\nvagrant@memcache-server:~$ cat nutcracker.yml\nmemcache:\n  listen: 127.0.0.1:11211\n  hash: fnv1a_64\n  distribution: ketama\n  auto_eject_hosts: true\n  server_retry_timeout: 2000\n  server_failure_limit: 1\n  servers:\n   - 127.0.0.1:11221:1\n   - 127.0.0.1:11222:1\n   - 127.0.0.1:11223:1\n\n#启动后端的memcached\nvagrant@memcache-server:~$ ps -ef | grep memcache\nvagrant  17934     1  0 09:07 ?        00:00:00 memcached -d -p 11221\nvagrant  17942     1  0 09:07 ?        00:00:00 memcached -d -p 11222\nvagrant  17950     1  0 09:07 ?        00:00:00 memcached -d -p 11223\nvagrant  18295 18083  0 09:20 pts/0    00:00:00 grep --color=auto memcache\n\nvagrant@memcache-server:~$ nutcracker -c nutcracker.yml -d\n\nvagrant@memcache-server:~$ telnet localhost 11211\nTrying 127.0.0.1...\nConnected to localhost.\nEscape character is '^]'.\nset test 0 0 1\n1\nSTORED\nget test\nVALUE test 0 1\n1\nEND\n```","tags":["cluster","memcached","redis","twemproxy"],"categories":["memcached","redis"]},{"title":"Vagrantfile配置","url":"//2017/02/09/vagrantfile/","content":"\n删除原始文档，参考官方文档：[https://www.vagrantup.com/docs/vagrantfile/](https://www.vagrantup.com/docs/vagrantfile/)\n\n","tags":["vagrant"],"categories":["vagrant"]},{"title":"vcenter扩容Linux虚拟机磁盘","url":"//2017/01/19/vcenter-expand-disk-to-linux/","content":"\n![img](/images/img_58802ffca93b9.png)\n\n![img](/images/img_5880322f10890.png)\n\n磁盘扩容100G\n1.vcenter扩容磁盘\nvcenter增加步骤参考：[vcenter扩容window磁盘](/2017/01/12/vcenter-expand-disk-to-windows)\n\n2.Linux需要rescan磁盘，重新读到大小\n```shell\n方法1:\nreboot\n方法2:\n[root@localhost ~]# echo 1 > /sys/block/sdc/device/rescan\n```\n3.fdisk创建新分区\n\t注意创建新的磁盘后内核可能无法识别到分区表，需要运行partprobe刷新下分区表信息\n\n4.lvm动态扩容\nlvm参考：[lvm动态磁盘管理](http://www.xiemx.com/2015/11/29/linux动态磁盘管理（lvm）/)\n\n完成后：\n\n![img](/images/img_588031f4a62e1.png)\n\n![img](/images/img_58803215d9370.png)\n\n","tags":["vcenter"],"categories":["vcenter"]},{"title":"nginx server_name_hash_bucket_size","url":"//2017/01/18/nginx-server_name_hash_bucket_size/","content":"\nnginx默认的 `server_name_hash_bucket_size` 一般为32/64，\n如果配置了超长的server_name建议适当增大此参数\n```\nhttp {\n    server_names_hash_bucket_size 512;\n}\n```","tags":["nginx"],"categories":["nginx"]},{"title":"nginx防盗链配置","url":"//2017/01/18/nginx-referer/","content":"```\nlocation ~* \\.(gif|jpg|jpeg|png|ico)$ {\n    valid_referers none blocked server_names *.xiemx.com;\n    if ($invalid_referer) {\n        return 444\n    }\n}\n```","tags":["nginx"],"categories":["nginx"]},{"title":"docker privileged","url":"//2017/01/18/docker-privileged/","content":"#### docker 特权模式\n```\n$ docker help run \n...\n--privileged=false         Give extended privileges to this container\n...\n```\n使用该参数，container内的root拥有真正的root权限。否则，container内的root只是外部的一个普通用户权限。privileged启动的容器，可以看到很多host上的设备，并且可以执行mount,iptables等操作。甚至允许你在docker容器中启动docker容器。\n    \n    \n* 未设置privileged启动的容器：\n```\n[root@localhost ~]# docker run -t -i centos:latest bash\n[root@65acccbba42f /]# ls /dev\nconsole  fd  full  fuse  kcore  null  ptmx  pts  random  shm  stderr  stdin  stdout  tty  urandom  zero\n[root@65acccbba42f /]# mkdir /home/test/\n[root@65acccbba42f /]# mkdir /home/test2/\n[root@65acccbba42f /]# mount -o bind /home/test  /home/test2\nmount: permission denied\n```\n\n* 设置privileged启动的容器：\n```\n[root@localhost ~]# docker run -t -i --privileged centos:latest bash\n[root@c39330902b45 /]# ls /dev/\nautofs           dm-1  hidraw0       loop1               null    ptp3    sg0  shm       tty10  tty19  tty27  tty35  tty43  tty51  tty6   ttyS1    usbmon3  vcs5   vfio\nbsg              dm-2  hidraw1       loop2               nvram   pts     sg1  snapshot  tty11  tty2   tty28  tty36  tty44  tty52  tty60  ttyS2    usbmon4  vcs6   vga_arbiter\nbtrfs-control    dm-3  hpet          loop3               oldmem  random  sg2  snd       tty12  tty20  tty29  tty37  tty45  tty53  tty61  ttyS3    usbmon5  vcsa   vhost-net\nbus              dm-4  input         mapper              port    raw     sg3  stderr    tty13  tty21  tty3   tty38  tty46  tty54  tty62  uhid     usbmon6  vcsa1  watchdog\nconsole          dm-5  kcore         mcelog              ppp     rtc0    sg4  stdin     tty14  tty22  tty30  tty39  tty47  tty55  tty63  uinput   vcs      vcsa2  watchdog0\ncpu              dm-6  kmsg          mem                 ptmx    sda     sg5  stdout    tty15  tty23  tty31  tty4   tty48  tty56  tty7   urandom  vcs1     vcsa3  zero\ncpu_dma_latency  fd    kvm           net                 ptp0    sda1    sg6  tty       tty16  tty24  tty32  tty40  tty49  tty57  tty8   usbmon0  vcs2     vcsa4\ncrash            full  loop-control  network_latency     ptp1    sda2    sg7  tty0      tty17  tty25  tty33  tty41  tty5   tty58  tty9   usbmon1  vcs3     vcsa5\ndm-0             fuse  loop0         network_throughput  ptp2    sda3    sg8  tty1      tty18  tty26  tty34  tty42  tty50  tty59  ttyS0  usbmon2  vcs4     vcsa6\n[root@c39330902b45 /]# mkdir /home/test/\n[root@c39330902b45 /]# mkdir /home/test2/\n[root@c39330902b45 /]# mount -o bind /home/test  /home/test2\n```","categories":["docker"]},{"title":"iis7如何查看进程号对应的网站","url":"//2017/01/17/iis-get-pid/","content":"1.获取pid\n\n![img](/images/img_587ddd23c02a3.png)\n\n2.查询程序池对应的pid，根据applicationpool来查找对应网站\n\n```\n%windir%/system32/inetsrv/appcmd list wp \n```\n\n![img](/images/img_587dde19b6b47.png)","tags":["webserver","iis","windows"],"categories":["windows","iis"]},{"title":"vcenter扩容window磁盘","url":"//2017/01/12/vcenter-expand-disk-to-windows/","content":"1.vcenter中增加磁盘大小，原本是80G现在扩容100G\n\n![img](/images/img_587733a5ae0ba.png)\n\n2.Linux系统中重新扫描磁盘\n\n![img](/images/img_5877340a43dea.png)\n\n![img](/images/img_5877344e51cd6.png)\n\n ","tags":["windows","vcenter"],"categories":["windows","vcenter"]},{"title":"start-stop-daemon 用法","url":"//2017/01/07/linux-start-stop-daemon/","content":"start-stop-daemon，用来启动停止系统中的守护进程，/etc/init.d/下的脚本中会用到这个命令去启动、停止、查询这个命令。可以查询学习下高玩的用法。\n```\n参数：\n-S|--start -- <argument> ... 开启一个系统守护程序，并传递参数给它 \n-K|--stop 停止一个程序 \n-T|--status 得到程序的状态 \n-H|--help 显示帮助信息 \n-V|--version 打印版本信息 \nMatching options (at least one is required): \n-p|--pidfile <pid-file> pid file to check \n-x|--exec <executable> program to start/check if it is running \n-n|--name <process-name> process name to check \n-u|--user <username|uid> process owner to check \nOptions: \n-g|--group <group|gid> 按指定用户组权限运行程序 \n-c|--chuid <name|uid[:group|gid]> \n按指定用户、用户组权限运行程序 \n-s|--signal <signal> signal to send (default TERM) \n-a|--startas <pathname> program to start (default is <executable>) \n-r|--chroot <directory> chroot to <directory> before starting \n-d|--chdir <directory> change to <directory> (default is /) \n-N|--nicelevel <incr> add incr to the process' nice level \n-P|--procsched <policy[:prio]> \nuse <policy> with <prio> for the kernel \nprocess scheduler (default prio is 0) \n-I|--iosched <class[:prio]> use <class> with <prio> to set the IO \nscheduler (default prio is 4) \n-k|--umask <mask> 在开始运行前设置<mask> \n-b|--background 后台运行 \n-m|--make-pidfile 当命令本身不创建pidfile时，由start-stop-daemon创建 \n-R|--retry <schedule> 等待timeout的时间，检查进程是否停止，如果没有发送KILL信号； \n-t|--test 测试模式 \n-o|--oknodo exit status 0 (not 1) if nothing done \n-q|--quiet 不要输出警告 \n-v|--verbose 显示运行过程信息 \n```\n\n开启一个daemon进程 \n```\nstart-stop-daemon --start --background --exec /test.py  -- -c /test.conf  \n# -- 后的参数会传递给 /test.py,相当于/test.py -c /test.conf\n```\n\n关闭一个daemon进程 \n```\nstart-stop-daemon --stop --name proxy.py\n```","tags":["linux"],"categories":["linux"]},{"title":"nf_conntrack丢包","url":"//2016/12/20/linux-nf-conntrack-drop-package/","content":"#### 现象\n\n![img](/images/QQ20161220-0.png)\n\nnf_conntrack/ip_conntrack 跟 nat 有关，用来跟踪连接条目，它会使用一个哈希表来记录 established 的记录。如果该哈希表满了，就会出现问题。\n\n原因是nf_conntrack: table full, dropping packet，CONNTRACK_MAX 允许的最大跟踪连接条目超过了我们设置的阀值65535\n\n#### 解决：\n\n```\necho \"net.nf_conntrack_max=655350\" >> /etc/sysctl.conf&&sysctl -p \n```","tags":["debug","linux"],"categories":["linux"]},{"title":"zabbix监控redis","url":"//2016/12/19/zabbix-monitor-redis/","content":"可配合zabbix自动发现，来完成自动监控\n\n### scripts\n\n```shell\ncat redis.sh\n#!/bin/bash\nredis_conn=\"/usr/local/bin/redis-cli\"\nport=$1\n\ncase $2 in \n  \"used_memory\")\n    used_memory=`$redis_conn -p $port info | grep used_memory:|awk -F':' '{print $2}'`\n    echo $used_memory\n    ;;\n  \"ops_sec\")\n    ops=`$redis_conn -p $port info|grep instantaneous_ops_per_sec:|awk -F':' '{print $2}'`\n    echo $ops\n    ;;\n  \"connected_clients\")\n    connected_clients=`$redis_conn -p $port info|grep connected_clients: | awk -F ':' '{print $2}'`\n    echo $connected_clients\n    ;;\n  \"blocked_clients\")\n    blocked_clients=`$redis_conn -p $port info|grep blocked_clients:|awk -F':' '{print $2}'`\n    echo $blocked_clients\n    ;; \n  *)\n    echo \"please input used_memory|ops_sec|connected_clients|blocked_clients\"\n    ;; \nesac\n```\n\n### conf文件\n\n```yaml\ncat zabbix_redis.conf\n\nUserParameter=redis[*],/opt/script/zabbix/redis.sh $1 $2\n```\n","tags":["zabbix","scripts"],"categories":["zabbix"]},{"title":"zabbix监控mongo","url":"//2016/12/19/zabbix-monitor-mongo/","content":"可配合zabbix自动发现，自动监控服务\n### script\n\n```shell\ncat mongo.sh\n\n#!/bin/bash\n\ncase $1 in \n#  use_memory)\n#    used_memory=`echo \"db.serverStatus().mem\"|mongo admin|grep resident|awk -F':' '{print $2}'|tr -d \" ,\"`\n#    echo $used_memory\n#    ;;\n#  \n#  use_vmemory)\n#    used_vmemory=`echo \"db.serverStatus().mem\"|mongo admin|grep virtual|awk -F':' '{print $2}'|tr -d \" ,\"`\n#    echo $used_vmemory\n#    ;;\n#  \n#  used_conn)\n#    used_conn=`echo \"db.serverStatus().connections\"|mongo admin|grep current|awk -F':' '{print $2}'|tr -d ' ,'`\n#    echo $used_conn\n#    ;;\n#  \n#  available_conn)\n#    available=`echo \"db.serverStatus().connections\"|mongo admin|grep available|awk -F':' '{print $2}'|tr -d ' ,'`\n#    echo $availabe\n#  ;;\n  \n  insert)\n    insert=`mongostat -n1 | tail -n 1|awk '{print $1}'|tr -d ' *,'`\n    echo $insert\n    ;;\n  \n  query)\n    query=`mongostat -n1 | tail -n 1|awk '{print $2}'|tr -d ' *,'`\n    echo $query\n    ;;\n  \n  update)\n    update=`mongostat -n1 | tail -n 1|awk '{print $3}'|tr -d ' *,'`\n    echo $update\n    ;;\n  \n  delete)\n    delete=`mongostat -n1 | tail -n 1|awk '{print $4}'|tr -d ' *,'`\n    echo $delete\n    ;;\n  getmore)\n    getmore=`mongostat -n1 | tail -n 1|awk '{print $5}'|tr -d ' *,'`  \n    echo $getmore\n    ;;\n  command)\n    command=`mongostat -n1 | tail -n 1|awk '{print $6}'|awk -F'|' '{print $1}'|tr -d ' *,'`\n    echo $command\n    ;;\n  vsize)\n    vsize=`mongostat -n1 | tail -n 1|awk '{print $10}'|tr -d ' *,G'`\n    echo $vsize\n    ;;\n  res)\n    res=`mongostat -n1 | tail -n 1|awk '{print $11}'|tr -d ' *,G'`\n    echo $res\n    ;;\n  qr)\n    qr=`mongostat -n1 | tail -n 1|awk '{print $12}'|awk -F'|' '{print $1}'|tr -d ' *,'`\n    echo $qr\n    ;;\n  qw)\n    qw=`mongostat -n1 | tail -n 1|awk '{print $12}'|awk -F'|' '{print $2}'|tr -d ' *,'`\n    echo $qw\n    ;;\n  ar)\n    ar=`mongostat -n1 | tail -n 1|awk '{print $13}'|awk -F'|' '{print $1}'|tr -d ' *,'`\n    echo $ar\n    ;;\n  aw)\n    aw=`mongostat -n1 | tail -n 1|awk '{print $13}'|awk -F'|' '{print $2}'|tr -d ' *,'`\n    echo $aw\n    ;;\n  conn)\n    conn=`mongostat -n1 | tail -n 1|awk '{print $16}'|tr -d ' *,'`\n    echo $conn\n    ;;\n\n  *)\n    echo \"please input  insert|query|update|delete\"\n    ;;\nesac\n```\n\n\n### conf配置文件\n\n```shell\ncat zabbix_mongo.conf\nUserParameter=mongo[*],/opt/script/zabbix/mongo.sh $1\n```","tags":["zabbix","mongo"],"categories":["zabbix"]},{"title":"zabbix监控memcache","url":"//2016/12/19/zabbix-monitor-memcache/","content":"### script\n\n```shell\ncat memcached.sh\n\n#!/bin/bash\nport=$1\nmem_conn=\"/bin/nc 127.0.0.1 $port\"\n\ncase $2 in \n  conn)\n    conn=`echo -e \"stats\\nquit\"|$mem_conn|grep curr_connections | awk '{print $3}' `\n    echo $conn\n    ;;\n  bytes)\n    bytes=`echo -e \"stats\\nquit\"|$mem_conn|grep bytes|awk '{print $3}'`\n    echo `echo $bytes |tr -d \" \"`\n    ;;\n  cmd_get)\n    cmd_get=`echo -e \"stats\\nquit\"|$mem_conn|grep cmd_get| awk '{print $3}' `\n    echo $cmd_get\n    ;;\n  cmd_set)\n    cmd_set=`echo -e \"stats\\nquit\"|$mem_conn|grep cmd_set| awk '{print $3}' `\n    echo $cmd_set\n    ;;\n  get_hits)\n    get_hits=`echo -e \"stats\\nquit\"|$mem_conn|grep get_hits| awk '{print $3}' `\n    echo $get_hits\n    ;;\n  read_qps_sec)\n    count1=`echo -e \"stats\\nquit\"|$mem_conn|grep cmd_get| awk '{print $3}'|tr -d '\\r' `\n    sleep 1\n    count2=`echo -e \"stats\\nquit\"|$mem_conn|grep cmd_get| awk '{print $3}'|tr -d '\\r' `\n    count=` expr $count2 - $count1`\n    echo $count\n    ;;\n  write_qps_sec)\n    count1=`echo -e \"stats\\nquit\"|$mem_conn|grep cmd_set| awk '{print $3}' `\n    sleep 1\n    count2=`echo -e \"stats\\nquit\"|$mem_conn|grep cmd_set| awk '{print $3}' `\n    count=`echo $count2 $count1|awk '{printf($1-$2)}'`\n    echo $count\n    ;;\n  \n  hit_target)\n    cmd_get=`echo -e \"stats\\nquit\"|$mem_conn|grep cmd_get| awk '{print $3}' `\n    get_hits=`echo -e \"stats\\nquit\"|$mem_conn|grep get_hits| awk '{print $3}' `\n    hit_target=`echo $get_hits $cmd_get|awk '{printf($1*100/$2)}'`\n    echo $hit_target\n    ;;\n  *)\n    echo \"please input conn|bytes|cmd_get|cmd_set|get_hits|read_qps_sec|write_qps_sec|hit_target\"\n    ;;\nesac\n```\n\n### 配置文件放到zabbix的conf.d/目录下，\n\n```shell\ncat zabbix_memcache.conf\nUserParameter=memcache[*],/opt/script/zabbix/memcached.sh $1 $2\n```","tags":["zabbix","monitor","memcache"],"categories":["zabbix"]},{"title":"nginx websocket proxying","url":"//2016/12/14/nginx-websocket-proxying/","content":"```\nserver {\n        ...\n\n        location /chat/ {\n            proxy_pass http://backend;\n            proxy_http_version 1.1;\n            proxy_set_header Upgrade $http_upgrade;\n            proxy_set_header Connection $connection_upgrade;\n        }\n    }\n\n``` \n官方关于websocket proxying的文档：http://nginx.org/en/docs/http/websocket.html","tags":["nginx","websocket"],"categories":["nginx"]},{"title":"zabbix 监控ssl证书过期时间","url":"//2016/12/05/zabbix-monitor-ssl-cert/","content":"脚本是参考网上修改过的版本，注意要指定servername参数，以免抓取到其它站点的证书。\n\n```shell\nmingxu.xie@t-slq-ops-1:/opt/script/zabbix# cat check_ssl_cert.sh \n#/bin/bash\nhost=1\nport=2\nend_date=openssl s_client -host $host -port $port -servername $host -showcerts &lt;/dev/null 2>/dev/null|sed -n '/BEGIN CERTIFICATE/,/END CERT/p' |openssl x509 -text 2>/dev/null |sed -n 's/ *Not After : *//p'\nif [ -n \"end_date\" ];then\n    end_date_seconds=`date '+%s' --date \"end_date\"now_seconds=date '+%s'`\n    echo \"(end_date_seconds-now_seconds)/24/3600\" | bc\nfi\n```","categories":["zabbix"]},{"title":"zabbix触发器函数","url":"//2016/12/05/zabbix-trigger-function/","content":"\n\n\n不再一一列出，参考官方文档\n\n[https://www.zabbix.com/documentation/4.0/manual/appendix/triggers/functions](https://www.zabbix.com/documentation/4.0/manual/appendix/triggers/functions)\n\n","tags":["zabbix"],"categories":["zabbix"]},{"title":"Zabbix自定义Item Not Supported,页面出现不支持解决\n","url":"//2016/11/23/fix-zabbix-item-not-supported/","content":"\n使用Zabbix的时候往往会自定义Item。\n但是经常会遇到自定义的`Item Not Supported`了。\nZabbix Agent默认的超时时间是`3`秒。往往我们自定义的Item由于各种原因返回时间会比较长。建议修改一个适合自己实际的值。\n\n\n```shell\nvim /etc/zabbix/zabbix_agent.conf\n#Range: 1-30\nTimeout=8\n\n###修改完毕后重启zabbix-agent\n/etc/init.d/zabbix-agent restart\n```","tags":["zabbix"],"categories":["zabbix","issue"]},{"title":"django重置管理员密码","url":"//2016/09/29/django-reset-admin-password/","content":"忘记Django密码，使用如下操作即可找回\n```\n> cd /you/project/path  # 进入你的项目目录\n> python manage.py shell # 进入django shell\n\n> from django.contrib.auth.models import User\n> user =User.objects.get(username='admin')\n> user.set_password('123456')\n> user.save()\n```\n再使用123456密码登录即可","tags":["webserver","python","django"],"categories":["python"]},{"title":"zabbix-server.conf文件详解","url":"//2016/09/01/zabbix-server-configure/","content":"\n```markdown\nAlertScriptsPath 默认值：/usr/local/share/zabbix/alertscripts 说明：告警脚本目录\n\nAllowRoot 默认值：0 说明：是否允许使用root启动，0:不允许，1:允许，默认情况下她会使用zabbix用户来启动zabbix进程，不推荐使用root\n\nCacheSize 取值范围： 128K-8G 默认值：8M 说明：配置缓存，用于存储host，item，trigger数据，2.2.3版本之前最大支持2G，目前最大支持8G，一般用不了多少的。\n\nCacheUpdateFrequency 取值范围：1-3600 默认值：60 说明：多少秒更新一次配置缓存\n\nDBHost 默认值：localhost 说明：数据库主机地址\n\nDBName 默认值：无 必填：是\n\nDBPassword： 默认值：空 说明：数据库密码\n\nDBPort 取值范围：1024-65535 默认值:3306 说明：SQLite作为DB，这个选项请忽略，如果使用socket链接，也请忽略。\n\nDBSchema 说明：Schema名称. 用于 IBM DB2 、 PostgreSQL.\n\nDBSocket 默认值：/tmp/mysql.sock 说明：mysql sock文件路径\n\nDebugLevel 取值范围：0-5 默认值：3 说明: 指定debug级别 0 - 基本信息 1 - critical信息 2 - error信息 3 - warnings信息 4 - 调试日志，日志内容很多，慎重使用 5 - 用于调试web和vmware监控\n\nExternalScripts 默认值： /usr/local/share/zabbix/externalscripts 说明： 外部脚本目录\n\nFping6Location 默认值：/usr/sbin/fping6 说明：fping6路径，不懂fping的人可以百度一下，如果zabbix非root启动，请给fping6 SUID\n\nFpingLocation 默认值：/usr/sbin/fping 说明:和上面的一样\n\nHistoryCacheSize 取值范围：128K-2G 默认值：8M 说明： 历史记录缓存大小，用于存储历史记录\n\nHistoryTextCacheSize 取值范围：128K-2G 默认值：16M 说明：文本类型历史记录的缓存大小，存储character, text 、log历史记录.\n\nHousekeepingFrequency 取值范围：0-24 默认值：1 说明：housekeep执行频率，默认每小时回去删除一些过期数据。如果server重启，那么30分钟之后才执行一次，接下来，每隔一小时在执行一次。\n\nInclude 说明：include配置文件，可以使用正则表达式，例如：/usr/local/zabbix-2.4.4/conf/ttlsa.com/*.conf\n\nJavaGateway 说明：Zabbix Java gateway的主机名，需要启动Java pollers\n\nJavaGatewayPort 取值范围：1024-32767 默认值：10052 Zabbix Java gateway监听端口\n\nListenIP 默认值：0.0.0.0 说明：监听地址，留空则会在所有的地址上监听，可以监听多个IP地址，ip之间使用逗号分隔，例如：127.0.0.1,10.10.0.2\n\nListenPort 取值范围：1024-32767 默认值：10051 说明：监听端口\n\nLoadModule 说明：加载模块，格式: LoadModule=，文件必须在指定的LoadModulePath目录下，如果需要加载多个模块，那么写多个即可。\n\nLoadModulePath 模块目录，参考上面\n\nLogFile 日志文件，例如：/data/logs/zabbix/zabbix-server.log\n\nLogFileSize 取值范围：0-1024 默认值：1 0表示禁用日志自动rotation，如果日志达到了限制，并且rotation失败，老日志文件将会被清空掉，重新生成一个新日志。\n\nLogSlowQueries 取值范围：0-3600000 默认值：0 多慢的数据库查询将会被记录，单位：毫秒，0表示不记录慢查询。只有在DebugLevel=3时，这个配置才有效。\n\nMaxHousekeeperDelete 取值范围： 0-1000000 默认值：5000 housekeeping一次删除的数据不能大于MaxHousekeeperDelete\n\nPidFile 默认值：/tmp/zabbix_server.pid PID文件\n\nProxyConfigFrequency 取值范围：1-604800 默认值：3600 proxy被动模式下，server多少秒同步配置文件至proxy。\n\nProxyDataFrequency 取值范围：1-3600 默认值:1 被动模式下，zabbix server间隔多少秒向proxy请求历史数据\n\nSenderFrequency 取值范围：5-3600 默认值：30 间隔多少秒，再尝试发送为发送的报警\n\nSNMPTrapperFile 默认值：/tmp/zabbix_traps.tmp SNMP trap发送到server的数据临时存放文件。\n\nSourceIP 出口IP地址\n\nSSHKeyLocation SSH公钥私钥路径\n\nSSLCertLocation SSL证书目录，用于web监控\n\nSSLKeyLocation SSL认证私钥路径、用于web监控\n\nSSLCALocation SSL认证,CA路径，如果为空，将会使用系统默认的CA\n\nStartDBSyncers 取值范围：1-100 默认值：4 预先foke DB Syncers的数量，1.8.5以前最大值为64\n\nStartDiscoverers 取值范围：0-250 默认值：1 pre-forked discoverers的数量，1.8.5版本以前最大可为255\n\nStartHTTPPollers 取值范围：0-1000 默认值：1 pre-forked HTTP pollers的数量，1.8.5以前最大255\n\nStartIPMIPollers 取值范围：0-1000 默认值：0 pre-forked IPMI pollers的数量，1.8.5之前，最大为255\n\nTimeout 取值范围：1-30 默认值：3 agent，snmp，external check的超时时间，单位为秒\n\nTmpDir 默认值：/tmp\n\nTrapperTimeout 取值范围：1-300 默认值：300 处理trapper数据的超时时间\n\nTrendCacheSize 取值范围：128K-2G 默认值：4M 历史数据缓存大小\n\nUnavailableDelay 取值范围：1-3600 默认值：60 间隔多少秒再次检测主机是否可用\n\nUnreachableDelay 取值范围：1-3600 默认值：15 间隔多少秒再次检测主机是否可达。\n\nUnreachablePeriod 取值范围：1-3600 默认值：45 检测到主机不可用，多久将它置为不可达\n\nUser 默认值：zabbix 启动zabbix server的用户，在配置禁止root启动，并且当前shell用户是root得情况下有效。如果当前用户是xiemx，那么zabbix server的运行用户是xiemx\n\nValueCacheSize 取值范围：0,128K-64G 默认值：8M 0表示禁用，history value缓存大小，当缓存超标了，将会每隔5分钟往server日志里面记录。养成看日志的好习惯。\n```","categories":["zabbix"]},{"title":"keepalived不支持IPVS","url":"//2016/08/24/keepalived-not-support-IPVS/","content":"编译keepalived是遇到keepalived无法支持ip_vs，情况如下：\n  \nkeepalived默认编译时是在/usr/src/linux下找内核源代码。默认安装的源码在`/usr/src/kernels/$(uname -r)/`目录下（如果没有这个目录可以安装下kernel-devel包)。\n  \n解决办法：\n  ```\nln -s /usr/src/kernels/`uname -r`/  /usr/src/linux\n或者编译时指定 --with-kernel-dir=/usr/src/kernels/`uname -r`/\n然后重新编译keepalived，现在官方的最新版已经很少出现这种情况，暂时不知道是不是版本问题。\n```\n","tags":["keepalived","cluster"],"categories":["keepalived"]},{"title":"keepalived配置多实例","url":"//2016/08/24/keepalived-multi-server/","content":"\n  \n安装keepalived\n\n1、 下载 wget http://www.keepalived.org/software/keepalived-1.2.23.tar.gz\n  \n2、 解包 tar xf keepalived-1.2.23.tar.gz\n  \n3、 切换目录 cd keepalived-1.2.23\n  \n4、 配置 ./configure –prefix=/opt/keepalived 因为keepalived 运行在ipvs 之上，因此这两\n  \n个软件一定要安装在一个系统里面。如果configure 操作能正常进行，运行完毕后将有如下的汇总输出：\n```\nKeepalived configuration\n------------------------\nKeepalived version : 1.2.23\nCompiler : gcc\nCompiler flags : -g -O2\nExtra Lib : -lpopt -lssl -lcrypto\nUse IPVS Framework : Yes\nIPVS sync daemon support : Yes\nUse VRRP Framework : Yes\nUse LinkWatch : No\nUse Debug flags : No\n```\n5、 编译和安装 \nmake&&make install\n6、 环境配置\n```\ncd /opt/keepalived/\nmkdir /etc/keepalived/\ncp sbin/keepalived /usr/sbin/\ncp etc/keepalived/keepalived.conf /etc/keepalived/\ncp etc/sysconfig/keepalived /etc/sysconfig/\ncp etc/rc.d/init.d/keepalived /etc/init.d/\n```\n7、启动keepalived查看是否正常\n```\n[root@cluster-node1 keepalived]# /etc/init.d/keepalived start\nStarting keepalived (via systemctl):  [  OK  ]\n[root@cluster-node1 keepalived]# ps -ef | grep keepalived\nroot       4662      1  0 01:22 ?        00:00:00 keepalived -D\nroot       4664   4662  0 01:22 ?        00:00:00 keepalived -D\nroot       4665   4662  0 01:22 ?        00:00:00 keepalived -D\nroot      16435   2705  0 01:37 pts/0    00:00:00 grep --color=auto keepalived\n```\n8、修改配置文件\n```\n[root@cluster-node1 keepalived]# cat /etc/keepalived/keepalived.conf \n! Configuration File for keepalived\n\nglobal_defs {\n   notification_email {\n     acassen@firewall.loc\n     failover@firewall.loc\n     sysadmin@firewall.loc\n   }\n   notification_email_from Alexandre.Cassen@firewall.loc\n   smtp_server 192.168.200.1\n   smtp_connect_timeout 30\n   router_id node1\n   vrrp_skip_check_adv_addr\n   vrrp_strict\n   vrrp_garp_interval 0\n   vrrp_gna_interval 0\n}\n\nvrrp_instance VI_1 {\n    state MASTER\n    interface eno16777736 \n    virtual_router_id 51\n    priority 100\n    advert_int 1\n    authentication {\n        auth_type PASS\n        auth_pass 1111\n    }\n    virtual_ipaddress {\n        10.0.0.1/24\n    }\n}\nvrrp_instance VI_2 {\n    state BACKUP\n    interface eno16777736 \n    virtual_router_id 52\n    priority 50\n    advert_int 1\n    authentication {\n        auth_type PASS\n        auth_pass 1111\n    }\n    virtual_ipaddress {\n        10.0.1.1/24\n    }\n}\n```\n```\n[root@cluster-node2 keepalived]# cat /etc/keepalived/keepalived.conf \n! Configuration File for keepalived\n\nglobal_defs {\n   notification_email {\n     acassen@firewall.loc\n     failover@firewall.loc\n     sysadmin@firewall.loc\n   }\n   notification_email_from Alexandre.Cassen@firewall.loc\n   smtp_server 192.168.200.1\n   smtp_connect_timeout 30\n   router_id node2\n   vrrp_skip_check_adv_addr\n   vrrp_strict\n   vrrp_garp_interval 0\n   vrrp_gna_interval 0\n}\n\nvrrp_instance VI_1 {\n    state BACKUP\n    interface eno16777736\n    virtual_router_id 51\n    priority 50\n    advert_int 1\n    authentication {\n        auth_type PASS\n        auth_pass 1111\n    }\n    virtual_ipaddress {\n        10.0.0.1/24\n    }\n}\nvrrp_instance VI_2 {\n    state MASTER\n    interface eno16777736\n    virtual_router_id 52\n    priority 100\n    advert_int 1\n    authentication {\n        auth_type PASS\n        auth_pass 1111\n    }\n    virtual_ipaddress {\n        10.0.1.1/24\n    }\n}\n```\n9、重启keepalived，查看vip是否正常绑定\n```\n[root@cluster-node1 keepalived]# ip add | egrep  \"0.1|1.1\"\n    inet 127.0.0.1/8 scope host lo\n    inet6 ::1/128 scope host \n    inet 10.0.0.1/24 scope global eno16777736\n[root@cluster-node2 keepalived]# ip add | egrep  \"0.1|1.1\"\n    inet 127.0.0.1/8 scope host lo\n    inet6 ::1/128 scope host \n    inet 10.0.1.1/24 scope global eno16777736\n```\n","tags":["keepalived","cluster"],"categories":["keepalived"]},{"title":"Tomcat修改网站根目录","url":"//2016/08/18/tomcat-change-webroot/","content":"1.tomcat原来的默认根目录是`http://localhost:8080`，如果想修改访问的根目录，可以这样：\n\n找到tomcat的server.xml（在conf目录下），找到:\n```xml\n<Host name=\"localhost\" appBase=\"webapps\"\n       unpackWARs=\"true\" autoDeploy=\"true\"\n       xmlValidation=\"false\" xmlNamespaceAware=\"false\"></Host>\n```\n在</Host>前插入:\n```xml\n<Context path=\"\" docBase=\"/home/tomcat/\" debug=\"0\"/>\n```\n其中/home/tomcat/就是我想设置的网站根目录，然后重启tomcat。再次访问`http://localhost:8080`时，就是直接访问`/home/tomcat/`目录下的文件了。\n\n2.tomcat的web.xml（在conf目录下），在该文件中找到\n```xml\n    <welcome-file-list>\n        <welcome-file>index.html</welcome-file>\n        <welcome-file>index.htm</welcome-file>\n        <welcome-file>index.jsp</welcome-file>\n    </welcome-file-list>\n```\n这是tomcat默认的3个文件，当你输入指定路径后，tomcat会自动查找这3个页面。如果你想让tomcat自动找到自己的页面，比如main.jsp。可以修改上面信息为：\n```xml\n    <welcome-file-list>\n        <welcome-file>main.jsp</welcome-file>\n        <welcome-file>index.html</welcome-file>\n        <welcome-file>index.htm</welcome-file>\n        <welcome-file>index.jsp</welcome-file>\n    </welcome-file-list>\n```","tags":["webserver","http","tomcat"],"categories":["tomcat"]},{"title":"apachetop – 展示web服务器实时统计数据","url":"//2016/08/15/apachetop/","content":"\napachetop – 展示web服务器实时统计数据\n\nApachetop展示Apache web服务器上关于http请求的实时表。\n\n它显示统计（stats）, 点击（hits）, 请求（requests）, 请求细节（request details），且能够获得当前你的web服务器正在运行程序的概貌，这一点很赞。\n\n如果你使用的是Nginx，也有一些相似的工具，但似乎没有apachetop那么详细。\n\n安装该命令并尝试运行：\n```shell\n$ sudo apt-get install apachetop \n```\n截图如下：\n\n![img](/images/img_57b1811567e55.png)","tags":["apache","command"],"categories":["apache"]},{"title":"watch重复执行某个命令","url":"//2016/08/11/shell-command-watch/","content":"当需要重复执行一个命令时，可使用`watch`\n\n```shell\n➜  ~ watch\n\nUsage:\n watch [options] command\n\nOptions:\n  -b, --beep             beep if command has a non-zero exit\n  -c, --color            interpret ANSI color and style sequences\n  -d, --differences[=<permanent>]\n                         highlight changes between updates\n  -e, --errexit          exit if command has a non-zero exit\n  -g, --chgexit          exit when output from command changes\n  -n, --interval <secs>  seconds to wait between updates\n  -p, --precise          attempt run command in precise intervals\n  -t, --no-title         turn off header\n  -x, --exec             pass command to exec instead of \"sh -c\"\n\n -h, --help     display this help and exit\n -v, --version  output version information and exit\n\nFor more details see watch(1).\n\n###每秒执行一次ls命令\nwatch -n 1 ls\n```\n\n","tags":["command","shell"],"categories":["command"]},{"title":"arp_ignore和arp_announce参数说明","url":"//2016/08/11/linux-disable-arp/","content":"lvsDR模式中需要对real server进行arp抑制，我们通常会在sysctl.conf文件中修改如下内核参数：\n```\nnet.ipv4.conf.lo.arp_ignore = 1\nnet.ipv4.conf.lo.arp_announce = 2\nnet.ipv4.conf.all.arp_ignore = 1\nnet.ipv4.conf.all.arp_announce = 2\narp_ignore:定义对目标地址为本地IP的ARP询问不同的应答模式0 \n\n0 - (默认值): 回应任何网络接口上对任何本地IP地址的arp查询请求 \n1 - 只回答目标IP地址是来访网络接口本地地址的ARP查询请求 \n2 -只回答目标IP地址是来访网络接口本地地址的ARP查询请求,且来访IP必须在该网络接口的子网段内 \n3 - 不回应该网络界面的arp请求，而只对设置的唯一和连接地址做出回应 \n4-7 - 保留未使用 \n8 -不回应所有（本地地址）的arp查询\n\narp_announce:对网络接口上，本地IP地址的发出的，ARP回应，作出相应级别的限制: 确定不同程度的限制,宣布对来自本地源IP地址发出Arp请求的接口 \n\n0 - (默认) 在任意网络接口（eth0,eth1，lo）上的任何本地地址 \n1 -尽量避免不在该网络接口子网段的本地地址做出arp回应. 当发起ARP请求的源IP地址是被设置应该经由路由达到此网络接口的时候很有用.此时会检查来访IP是否为所有接口上的子网段内ip之一.如果改来访IP不属于各个网络接口上的子网段内,那么将采用级别2的方式来进行处理. \n2 - 对查询目标使用最适当的本地地址.在此模式下将忽略这个IP数据包的源地址并尝试选择与能与该地址通信的本地地址.首要是选择所有的网络接口的子网中外出访问子网中包含该目标IP地址的本地地址. 如果没有合适的地址被发现,将选择当前的发送网络接口或其他的有可能接受到该ARP回应的网络接口来进行发送.\n```\n","tags":["linux","arp"],"categories":["linux"]},{"title":"zabbix Less than 25% free in the configuration cache解决","url":"//2016/08/09/zabbix-less-than-25-free-in-the-configuration-cache/","content":"\n在zabbix server默认配置下，出现告警\n\n```markdown\nLess than 25% free in the configuration cache\n```\n可增加zabbix配置缓存解决\n\n```shell\nvim zabbix_server.conf\n##将缓存从8M提升到16M，可以调到最高8G\nCacheSize=16M\n\n##重启zabbix server\nservice zabbix_server restart\n```","tags":["zabbix","issue"],"categories":["zabbix"]},{"title":"mysql锁表解决","url":"//2016/08/02/mysql-lock-table/","content":"1、查询是否锁表\n```sql\nshow OPEN TABLES where In_use > 0;\n```\n2、查询进程\n```\nshow processlist 查询到相对应的ID\n```\n3、杀死进程\n```\nkill id\n```\nother:\n```\n查看正在锁的事务\nSELECT * FROMINFORMATION_SCHEMA.INNODB_LOCKS; \n\n查看等待锁的事务\nSELECT * FROMINFORMATION_SCHEMA.INNODB_LOCK_WAITS;\n```","tags":["debug","mysql","database"],"categories":["mysql"]},{"title":"域名状态含义","url":"//2016/07/27/domain-status-code/","content":"新注册的域名，可能出现以下状态：\n\n|域名状态|含义|\n|:---|:-|\n|addPeriod|注册局设置域名新注册期（域名新注册 5 天内会出现的状态，不影响域名使用，5 天后自动解除）。|\n|·ok|普通状态（可正常使用。没有需要立即进行的操作，也没有设置任何保护措施。有其他状态时，OK 状态不显示，但并不代表不正常。|\n\n出于对域名注册信息的保护，域名在进行某些安全锁定后，会出现以下状态：\n\n|域名状态|含义|\n|:---|:-|\n|·clientDeleteProhibited|注册商设置禁止删除（保护域名的一种状态，域名不能被删除）。|\n|serverDeleteProhibited|注册局设置禁止删除（保护域名的一种状态，域名不能被删除）。|\n|·clientUpdateProhibited|注册商设置禁止更新（域名信息，包括注册人/管理联系人/技术联系人/付费联系人/DNS 等不能被修改，但可设置或修改解析记录）。|\n|·serverUpdateProhibited|注册局设置禁止更新（域名信息，包括注册人/管理联系人/技术联系人/付费联系人/DNS 等不能被修改，但可设置或修改解析记录）。|\n|·clientTransferProhibited|注册商设置禁止转移（保护域名的一种状态，域名不能转移注册商）。|\n|·serverTransferProhibited|注册局设置禁止转移（保护域名的一种状态，域名不能转移注册商。有的域名新注册及转移注册商 60 天内会被注册局设置成该状态，60 天后自动解除；有的则为域名涉及仲裁或诉讼案被注册局设置，仲裁或诉讼案结束会被解除）。|\n\n其他禁止解析、禁止续费的状态：\n\n|域名状态|含义|\n|:---|:-|\n|·pendingVerification|注册信息审核期（该域名注册后未进行实名审核，需尽早在域名付费后 5 天内提交资料审核，5 天后仍未实名审核的，将进入 ServerHold 状态）。|\n|·clientHold|注册商设置暂停解析（处于该状态域名解析暂停，需联系注册商解除该状态）。|\n|·serverHold|注册局设置暂停解析（处于该状态域名解析暂停，.cn 国内中英文域名注册成功后未通过实名审核时多出现该种状态，需在域名有效期内完成实名审核后解除）。|\n|·inactive|非激活状态（注册时未填写域名 DNS，不能进行解析，需在注册商处设置域名 DNS）。|\n|·clientRenewProhibited/serverRenewProhibited|注册商或注册局设置禁止续费（域名不能续费，处于该状态通常表示该域名处于仲裁或法院争议期，需联系注册商确认原因）。|\n|·pendingTransfer|注册局设置转移过程中（域名正在转移注册商过程中）。|\n|·redemptionPeriod|注册局设置赎回期（域名处于赎回期，可联系注册商高价赎回）。|\n|·pendingDelete|注册局设置待删除/赎回期（对于国际域名，该状态表示域名已过赎回期等待被删除，删除后开放重新注册；对于国内域名，该状态表示域名处于赎回期，可联系注册商高价赎回）。|\n","tags":["domain","statusCode"],"categories":["domain"]},{"title":"uwsgi 报错 invalid option --“x” getopt_long() error","url":"//2016/07/26/uwsgi-invalid-option-x-getopt_long-error/","content":"### 报错：\n\n```markdown\nuwsgi: invalid option — ‘x’\ngetopt_long() error \n```\n\n### 解决\n\n这个问题是因为编译uwsgi的时候少了libxml2库导致的，应该先安装库再编译，否则会少了xml的支持。\n\n1. uninstall uwsgi\n\n2. install  libxml*  \n\n3. install uwsgi\n\n","tags":["python","uwsgi","issue"],"categories":["uwsgi","python"]},{"title":"nginx+uwsgi部署django项目","url":"//2016/07/26/nginx-uwsgi-deploy-django/","content":"### 安装基本软件\n```shell\nsudo apt-get install python-dev nginx\npip install uwsgi\n```\n### 配置uwsgi和django的集成\n```shell\nvim test.py 创建test.py,添加如下代码\ndef application(env, start_response):\n    start_response('200 OK', [('Content-Type','text/html')])\n    return \"Hello World\"\n\n然后执行shell命令：\nuwsgi –http :8001 –wsgi-file test.py\n\n访问网页：\ncurl http://127.0.0.1:8001/\nHello World\n```\n#编写django_wsgi.py文件，将其放在与文件manage.py同一个目录下：\n```python\n### vim django_wsgi.py 添加如下代码：\n#!/usr/bin/env python\n# coding: utf-8\nimport os\nimport sys\n# 将系统的编码设置为UTF8\nreload(sys)\nsys.setdefaultencoding('utf8')\n\nos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"yoursite.settings\")\n\nfrom django.core.handlers.wsgi import WSGIHandler\napplication = WSGIHandler()\n```\n连接django和uwsgi，实现简单的WEB服务器。\n我们假设你的Django项目的地址是`/home/work/src/sites/testdjango1/testdjango/mysite`，\n\n然后，就可以执行以下命令：\n`uwsgi –http :8000 –chdir /home/work/src/sites/testdjango1/testdjango/mysite –module django_wsgi`\n这样，你就可以在浏览器中访问你的Django程序了。所有的请求都是经过uwsgi传递给Django程序的。\n\n### 集成django,uwsgi和nginx部署：\n* 在django项目根目录创建启动uwsgi的xml文件：\n```\n<uwsgi>\n    <socket>:8077</socket>\n    <chdir>/home/work/src/sites/testdjango1/testdjango/mysite</chdir>\n    <module>django_wsgi</module>\n    <processes>4</processes> <!-- 进程数 --> \n    <daemonize>uwsgi.log</daemonize>\n</uwsgi>\n```\n* 配置Nginx服务器：\n```\n#备份nginx配置文件：\nsudo cp /etc/nginx/sites-available/default /etc/nginx/sites-available/default.bak\nvim /etc/nginx/sites-available/default 修改如下：\n\nserver {\n\n        listen   80;\n        server_name localhost;\n        access_log /var/log/nginx/access.log;\n        error_log /var/log/nginx/error.log;\n\n        #charset koi8-r;\n\n        #access_log  logs/host.access.log  main;\n\n        location / {\n         include        uwsgi_params;\n         uwsgi_pass     127.0.0.1:8077;\n        }\n\n        #error_page  404              /404.html;\n\n        # redirect server error pages to the static page /50x.html\n        #\n        error_page   500 502 503 504  /50x.html;\n        location = /50x.html {\n            root   html;\n        }\n        #method first\n        location ~ ^/static/ {\n            root  /home/hz/PycharmProjects/myscrapy/check_ip/;\n            expires 24h;\n            access_log off;\n        }\n        #method second\n        #location ~* ^.+\\.(css|js|json|ttf|woff|map|woff2)${\n        #    root /home/hz/PycharmProjects/myscrapy/check_ip/;\n        #    access_log off;\n        #    expires 24h;\n        #}      \n}\n```\n如果不能访问日志文件，修改相关文件的权限即可\n* 验证测试各步骤结果\n```shell\n重启Nginx服务器，以使Nginx的配置生效。\nnginx -s  reload\n重启后检查Nginx日志是否有异常。\n\n启动uWSGI服务器\n\ncd /home/work/src/sites/testdjango1/testdjango/mysite\n\nuwsgi -x djangochina_socket.xml\n```\n* django搜集静态文件\n```shell\nsettings.py文件中指定STATIC_ROOT路径\npython manage.py collectstatic\n\n修改nginx配置文件指定目录到STATIC_ROOT\n```","tags":["python","django","nginx","uwsgi"],"categories":["python","uwsgi"]},{"title":"linux自带的限制策略","url":"//2016/07/11/linux-limit-tools/","content":"```\n[root@iZ11eqvuvnqZ ~]# cat /etc/security/limits.conf \n# /etc/security/limits.conf\n#\n#This file sets the resource limits for the users logged in via PAM.\n#It does not affect resource limits of the system services.\n#\n#Also note that configuration files in /etc/security/limits.d directory,\n#which are read in alphabetical order, override the settings in this\n#file in case the domain is the same or more specific.\n#That means for example that setting a limit for wildcard domain here\n#can be overriden with a wildcard setting in a config file in the\n#subdirectory, but a user specific setting here can be overriden only\n#with a user specific setting in the subdirectory.\n#\n#Each line describes a limit for a user in the form:\n#\n#domain>       type>  item>  value>\n#\n#Where:\n#domain> can be:\n#        - a user name\n#        - a group name, with @group syntax\n#        - the wildcard *, for default entry\n#        - the wildcard %, can be also used with %group syntax,\n#                 for maxlogin limit\n#\n#type> can have the two values:\n#        - \"soft\" for enforcing the soft limits\n#        - \"hard\" for enforcing hard limits\n#\n#item> can be one of the following:\n#        - core - limits the core file size (KB)\n#        - data - max data size (KB)\n#        - fsize - maximum filesize (KB)\n#        - memlock - max locked-in-memory address space (KB)\n#        - nofile - max number of open files\n#        - rss - max resident set size (KB)\n#        - stack - max stack size (KB)\n#        - cpu - max CPU time (MIN)\n#        - nproc - max number of processes\n#        - as - address space limit (KB)\n#        - maxlogins - max number of logins for this user\n#        - maxsyslogins - max number of logins on the system\n#        - priority - the priority to run user process with\n#        - locks - max number of file locks the user can hold\n#        - sigpending - max number of pending signals\n#        - msgqueue - max memory used by POSIX message queues (bytes)\n#        - nice - max nice priority allowed to raise to values: [-20, 19]\n#        - rtprio - max realtime priority\n#\n#domain>      &lt;type>  &lt;item>         &lt;value>\n#\n\n#*               soft    core            0\n#*               hard    rss             10000\n#@student        hard    nproc           20\n#@faculty        soft    nproc           20\n#@faculty        hard    nproc           50\n#ftp             hard    nproc           0\n#@student        -       maxlogins       4\n\n# End of file\n* soft nofile 65535\n* hard nofile 65535\n\n\n``` ","tags":["command","linux"],"categories":["linux"]},{"title":"限制进程的CPU利用率","url":"//2016/07/06/cpulimit/","content":"\n1. cpulimit安装\n\n```shell\napt-get install cpulimit\nyum install cpulimit\n### 请先安装epel源，在执行yum命令.\n```\n\n2. cpulimit实例\n\n```\n### 根据进程ID限值\ncpulimit -p 1234 -l 40\n进程ID为1234的程序只能使用40%的cpu\n\n### 根据进程路径限值\ncpulimit -e nginx -l 50\nnginx只能使用50%的cpu\n```\n\n3. 注意事项\n这边要留意一点，-l后面默认值是百分比，而且在双核情况下要减半。例如nginx的例子，在双核cpu情况下他可以利用25%的cpu，在4核的情况下，只能使用12.5%的cpu.root用户可以限值所有的进程，普通用户只能限值自己程序.\n\n项目地址：http://cpulimit.sourceforge.net/","tags":["command","linux","cpulimit"],"categories":["linux"]},{"title":"PHP_CURL 使用代理访问服务器","url":"//2016/07/06/php_curl/","content":"```php\n#使用代码时请加上php文件的括号\nfunction curl_string ($url,$user_agent,$proxy){\n\n       $ch = curl_init();\n       curl_setopt ($ch, CURLOPT_PROXY, $proxy);\n       curl_setopt ($ch, CURLOPT_URL, $url);\n       curl_setopt ($ch, CURLOPT_USERAGENT, $user_agent);\n       curl_setopt ($ch, CURLOPT_HEADER, 1);\n       curl_setopt ($ch, CURLOPT_RETURNTRANSFER, 1);\n       curl_setopt ($ch, CURLOPT_FOLLOWLOCATION, 1);\n       curl_setopt ($ch, CURLOPT_TIMEOUT, 120);\n       $result = curl_exec ($ch);\n       curl_close($ch);\n       return $result;\n\n}\n\n$url_page = \"http://www.google.com\";\n$user_agent = \"Mozilla/4.0\";\n$proxy = \"http://192.11.222.124:8000\";\n$string = curl_string($url_page,$user_agent,$proxy);\necho $string;\n\n```","tags":["php"],"categories":["php"]},{"title":"zabbix微信报警脚本","url":"//2016/06/29/zabbix-alert-to-wechat-script/","content":"\n```python\n#!/usr/bin/python\n\n__author__ = 'xiemx'\n\nimport sys\nimport json,requests\nimport os\nimport logging\n\nclass Weixin(object):\n\n    def get_token(self):\n        CorpID = '-------4fa4'\n        Secret = 'Aew6oxx-----------FaTClkjXlmw_zH'\n        token_url = 'https://qyapi.weixin.qq.com/cgi-bin/gettoken?corpid={}&corpsecret={}'.format(CorpID, Secret)\n        response = requests.get(token_url, verify=False).content\n        p = json.loads(response)\n        token = p['access_token']\n        return token\n\n    def send_msg(self, user_id, msg):\n        token = self.get_token()\n        url = 'https://qyapi.weixin.qq.com/cgi-bin/message/send?access_token={}'.format(token)\n        send_content={\n            \"touser\": user_id,\n            \"msgtype\": \"text\",\n            \"agentid\": \"2\",\n            \"text\": {\n               \"content\": msg\n            },\n            \"safe\":\"0\"\n            }\n        \n        p = requests.post(url, verify=False, data=json.dumps(send_content))\n        print p.content\n        logging.debug(\"weixin send success\")\n\nif __name__ == \"__main__\":\n    user_id = sys.argv[1]\n    msg = sys.argv[2] + '\\n' + sys.argv[3]\n    weixin = Weixin()\n    weixin.send_msg(user_id, msg)\n```","categories":["zabbix"]},{"title":"zabbix监控指定端口","url":"//2016/06/29/zabbix-monitor-port/","content":"\n1.登陆zabbix主界面\n\n选择：配置-模板\n\n选择模板组，这里我选择的是Template App Agentless，原因该自带模板组内包含各种常用服务的模板\n\n单击该模板组 项目\n\n![img](/images/img_577330bdc322e.png)点击右上角的 创建项目\n\n![img](/images/img_577330e1e5d06.png)这里添加的是mysql服务，按照如图配置\n\n![img](/images/img_577330f8871b5.png)单击下方 存档 保存。\n\n如图点击 触发器\n\n![img](/images/img_5773310c679a1.png)如图，点击右上方 创建触发器\n\n![img](/images/img_5773313591d94.png)按照如图配置\n\n![img](/images/img_57733153a13cb.png)点击 存档 保存，完毕。\n\n以上是将自定义模板服务添加到Template App Agentless模板组\n\n最后将Template App Agentless模板组添加到你需要监控的客户端主机内","categories":["zabbix"]},{"title":"shell 变量特殊操作","url":"//2016/06/29/shell-advance-variable/","content":"\n### 替换运算符\n```\n  ${var:-word}    如果var存在且非null，返回它的值；否则返回word\n  ${var:=word}    如果var存在且非null，返回它的值；否则将word赋值给var，并返回var的值\n  ${var:?word}    如果var存在且非null，返回它的值；否则显示var:word\n  ${var:+word}    如果var存在且非null，返回word；否则返回null\n```\n\n### 模式匹配运算符\n```\n  ${var#pattern}    匹配前缀（最小匹配），并返回余下内容\n  ${var##pattern}   匹配前缀（最大匹配），并返回余下内容\n  ${var%pattern}    匹配结尾（最小匹配），并返回余下内容\n  ${var%%pattern}   匹配结尾（最大匹配），并返回余下内容\n```\n  注：pattern为正则表达式匹配\n","tags":["linux","shell"],"categories":["shell","linux"]},{"title":"shell 特殊变量","url":"//2016/06/29/shell-special-env-variable/","content":"特殊变量\n``` \n$#    表示变量的个数，常用于循环\n$@    当前命令行所有参数\n$*    当前命令行所有参数，将命令行所有参数当一个单独参数获取\n$?    表示上一个命令退出的状态\n$$    表示当前进程编号\n$0    表示当前程序名称\n$!    表示最近一个后台命令的进程编号\n$IFS    表示内部的字段分隔符\n\n$?的参考值\n0    成功退出\n>0    退出失败\n1-125    命令退出失败，失败返回的相关值由程序定义（譬如，程序内退出只执行 exit 2，则返回为2）\n126    命令找到了，但无法执行\n127    命令找不到\n>128    命令因受到信号而死亡\n```","tags":["linux","shell"],"categories":["shell","linux"]},{"title":"rbash创建只读用户","url":"//2016/06/02/rbash-add-readonly-user/","content":"网上看到一篇用rbash来创建受限用户的文章，来源忘记了，mark在这里\n### 受限bash\n如果 bash 以 rbash 为程序名启动或者命令行参数有 -r 选项，则启动的这个 shell 会在某些功能上受限制．具体表现为如下操作都不能做：\n```\n通过 cd 来改变工作目录\n设置或取消环境变量： SHELL， PATH， ENV， BASH_ENV\n命令名中不能包含目录分隔符 ‘/’\n包含有 ‘/’ 的文件名作为内置命令 ‘.’ 的参数\nhash 内置命令有 -p 选项时的文件名参数包含 '/'\n在启动时通过 shell 环境导入函数定义\n在启动时通过 shell 环境解析 SHELLOPTS 的值\n使用 >，>|， <>， >&， &>， >> 等重定向操作符\n使用 exec 内置命令\n通过 enable 内置命令的 -f 和 -d 选项增加或删除内置命令\n使用 enable 内置命令来禁用或启用 shell 内置命令\n执行 command 内置命令时加上 -p 选项\n通过 set +r 或 set +o restricted 关闭受限模式\n```\n### 步骤\n```shell\nln -s /bin/bash  /bin/rbash\nuseradd -s /bin/rbash rttlsa\npasswd rttlsa\nmkdir /home/rttlsa/bin\nchown root. /home/rttlsa/.bash_profile \nchmod 755 /home/rttlsa/.bash_profile\nvi /home/rttlsa/.bash_profile \n.bash_profile\n\n# Get the aliases and functions\nif [ -f ~/.bashrc ]; then\n  . ~/.bashrc\nfi\n\n# User specific environment and startup programs\nPATH=$HOME/bin\nexport PATH\nln -s /bin/cat  /home/rttlsa/bin/cat  将允许执行的命令链接到$HOME/bin目录\n```\n如此即可创建只允许查看日志的只读用户。","tags":["command","linux","rbash","bash"],"categories":["linux"]},{"title":"错误Could not run curl-config: [Errno 2] No such file or directory解决","url":"//2016/05/27/cloud-not-run-curl-config/","content":"```shell\nroot@06f95000953c:~/phantomjs# pip install pycurl\nDownloading/unpacking pycurl\n  Downloading pycurl-7.43.0.tar.gz (182kB): 182kB downloaded\n  Running setup.py (path:/tmp/pip-build-XGZuU8/pycurl/setup.py) egg_info for package pycurl\n    Traceback (most recent call last):\n      File \"\", line 17, in \n      File \"/tmp/pip-build-XGZuU8/pycurl/setup.py\", line 823, in \n        ext = get_extension(sys.argv, split_extension_source=split_extension_source)\n      File \"/tmp/pip-build-XGZuU8/pycurl/setup.py\", line 497, in get_extension\n        ext_config = ExtensionConfiguration(argv)\n      File \"/tmp/pip-build-XGZuU8/pycurl/setup.py\", line 71, in __init__\n        self.configure()\n      File \"/tmp/pip-build-XGZuU8/pycurl/setup.py\", line 107, in configure_unix\n        raise ConfigurationError(msg)\n    __main__.ConfigurationError: Could not run curl-config: [Errno 2] No such file or directory\n    Complete output from command python setup.py egg_info:\n    Traceback (most recent call last):\n\n  File \"\", line 17, in \n\n  File \"/tmp/pip-build-XGZuU8/pycurl/setup.py\", line 823, in \n\n    ext = get_extension(sys.argv, split_extension_source=split_extension_source)\n\n  File \"/tmp/pip-build-XGZuU8/pycurl/setup.py\", line 497, in get_extension\n\n    ext_config = ExtensionConfiguration(argv)\n\n  File \"/tmp/pip-build-XGZuU8/pycurl/setup.py\", line 71, in __init__\n\n    self.configure()\n\n  File \"/tmp/pip-build-XGZuU8/pycurl/setup.py\", line 107, in configure_unix\n\n    raise ConfigurationError(msg)\n\n__main__.ConfigurationError: Could not run curl-config: [Errno 2] No such file or directory\n\n----------------------------------------\nCleaning up...\nCommand python setup.py egg_info failed with error code 1 in /tmp/pip-build-XGZuU8/pycurl\nStoring debug log for failure in /root/.pip/pip.log\n\nroot@06f95000953c:~/phantomjs# apt-get install libcurl4-openssl-dev\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nSuggested packages:\n  libcurl4-doc libcurl3-dbg libidn11-dev libkrb5-dev libldap2-dev librtmp-dev\nThe following NEW packages will be installed:\n  libcurl4-openssl-dev\n0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\nNeed to get 253 kB of archives.\nAfter this operation, 1262 kB of additional disk space will be used.\nGet:1 http://mirrors.163.com/ubuntu/ vivid-security/main libcurl4-openssl-dev amd64 7.38.0-3ubuntu2.2 [253 kB]\nFetched 253 kB in 5s (47.1 kB/s)            \ndebconf: delaying package configuration, since apt-utils is not installed\nSelecting previously unselected package libcurl4-openssl-dev:amd64.\n(Reading database ... 25480 files and directories currently installed.)\nPreparing to unpack .../libcurl4-openssl-dev_7.38.0-3ubuntu2.2_amd64.deb ...\nUnpacking libcurl4-openssl-dev:amd64 (7.38.0-3ubuntu2.2) ...\nSetting up libcurl4-openssl-dev:amd64 (7.38.0-3ubuntu2.2) ...\nroot@06f95000953c:~/phantomjs# \nroot@06f95000953c:~/phantomjs# \nroot@06f95000953c:~/phantomjs# curl\ncurl         curl-config  \nroot@06f95000953c:~/phantomjs# \n```\n\n\n安装libcurl4-openssl-dev此开发包即可\n```\nsudo apt-get install libcurl4-openssl-dev\n```","tags":["debug","linux"],"categories":["linux"]},{"title":"dedecms织梦验证码无法正常显示","url":"//2016/05/26/dedecms-yanzhenma/","content":"方案一：赋予sessions读、写、可执行的权限\n\n修改根目录下/data/sessions/的sess_***文件修改权限为777（命令：chmod 777 filename）。\n\n方案二：将vdimgck.php替换法\n\n替换前请将当前的vdimgck.php备份。找回相同版本的DEDE安装包，找到/include/vdimgck.php 文件，并用其替换当前站点的vdimgck.php文件。\n\n方案三：去掉登陆验证码代码\n  \n如果上面的两种解决办法都解决不了，那就直接去掉验证码功能。是修改data\\safe\\inc_safe_config.php 配置文件。\n\n方法：$safe_gdopen = ’1,2,3,5,6′; 这个就是系统哪些地方开启验证码。与[验证码安全设置]界面是一对一的关系。\n\n所以，如果当我们管理后台想关闭验证码(如果验证码无法正确输入，不支持GB库)的时候，只需要打开data\\safe\\inc_safe_config.php 将$safe_gdopen = ’1,2,3,5,6′; 中的6删除即可。","tags":["debug","php","dedecms"],"categories":["php"]},{"title":"redis主从复制","url":"//2016/05/23/redis-slave-replica/","content":"1. redis主从复制特点:\n\n* master可以拥有多个slave\n* 多个slave可以连接同一个master外，还可以连接到其他slave\n* 主从复制不会阻塞master，在同步数据时，master可以继续处理client请求\n* 提高系统的伸缩性\n* 可以在master禁用数据持久化，注释掉master配置文件中的所有save配置，只需在slave上配置数据持久化\n\n2. redis主从复制过程:\n\n当配置好slave后，slave与master建立连接，然后发送sync命令。无论是第一次连接还是重新连接，master都会启动一个后台进程，将数据库快照保存到文件中，同时master主进程会开始收集新的写命令并缓存。后台进程完成写文件后，master就发送文件给slave，slave将文件保存到硬盘上，再加载到内存中，接着master就会把缓存的命令转发给slave，后续master将收到的写命令发送给slave。如果master同时收到多个slave发来的同步连接命令，master只会启动一个进程来写数据库镜像，然后发送给所有的slave。\n\n3. redis主从配置:\n```conf\n### master\ndaemonize yes\npidfile /var/run/redis.pid\nport 6379\ntimeout 300\nloglevel verbose\nlogfile /usr/local/xiemx-redis/var/log/redis.log\ndatabases 16\nsave 900 1\nsave 300 10\nsave 60 10000\nrdbcompression yes\ndbfilename dump.rdb\ndir /usr/local/xiemx-redis/var/data\nrequirepass redis\nappendonly no\nappendfsync everysec\nno-appendfsync-on-rewrite no\nslowlog-log-slower-than 10000\nslowlog-max-len 1024\nvm-enabled no\nvm-swap-file /tmp/redis.swap\nvm-max-memory 0\nvm-page-size 32\nvm-pages 134217728\nvm-max-threads 4\nhash-max-zipmap-entries 512\nhash-max-zipmap-value 64\nlist-max-ziplist-entries 512\nlist-max-ziplist-value 64\nset-max-intset-entries 512\nactiverehashing yes\n```\n```conf\n### slave\ndaemonize yes\npidfile /var/run/redis.pid\nport 6379\ntimeout 300\nloglevel verbose\nlogfile /usr/local/xiemx-redis/var/log/redis.log\ndatabases 16\nsave 900 1\nsave 300 10\nsave 60 10000\nrdbcompression yes\ndbfilename dump.rdb\ndir /usr/local/xiemx-redis/var/data\nappendonly no\nappendfsync everysec\nno-appendfsync-on-rewrite no\nslowlog-log-slower-than 10000\nslowlog-max-len 1024\nvm-enabled no\nvm-swap-file /tmp/redis.swap\nvm-max-memory 0\nvm-page-size 32\nvm-pages 134217728\nvm-max-threads 4\nhash-max-zipmap-entries 512\nhash-max-zipmap-value 64\nlist-max-ziplist-entries 512\nlist-max-ziplist-value 64\nset-max-intset-entries 512\nactiverehashing yes\nslaveof 192.168.1.189 6379\nmasterauth redis\n```\n4. redis复制测试\n\n```log\n查看master端日志:\n[8930] 31 Jul 19:16:09 - Accepted 192.168.1.136:54774\n[8930] 31 Jul 19:16:09 * Slave ask for synchronization\n[8930] 31 Jul 19:16:09 * Starting BGSAVE for SYNC\n[8930] 31 Jul 19:16:09 * Background saving started by pid 10782\n[10782] 31 Jul 19:16:09 * DB saved on disk\n[8930] 31 Jul 19:16:09 * Background saving terminated with success\n[8930] 31 Jul 19:16:09 * Synchronization with slave succeeded\n[8930] 31 Jul 19:16:14 - DB 0: 1 keys (0 volatile) in 4 slots HT.\n[8930] 31 Jul 19:16:14 - 1 clients connected (1 slaves), 807320 bytes in use\n\n查看slave端日志:\n[24398] 01 Aug 10:16:10 * Connecting to MASTER...\n[24398] 01 Aug 10:16:10 * MASTER  SLAVE sync started: SYNC sent\n[24398] 01 Aug 10:16:10 * MASTER  SLAVE sync: receiving 25 bytes from master\n[24398] 01 Aug 10:16:10 * MASTER  SLAVE sync: Loading DB in memory\n[24398] 01 Aug 10:16:10 * MASTER  SLAVE sync: Finished with success\n[24398] 01 Aug 10:16:15 - DB 0: 1 keys (0 volatile) in 4 slots HT.\n[24398] 01 Aug 10:16:15 - 1 clients connected (0 slaves), 798960 bytes in use\n```\n```\nmaster端操作:\nredis 127.0.0.1:6379> set k_m master\nOK\n\nslave端操作:\nredis 127.0.0.1:6379> get k_m\n\"master\"\n```\n","tags":["redis"],"categories":["redis"]},{"title":"Linux内核调优部分参数说明","url":"//2016/05/23/linux-kernal-paramenter/","content":"#接收套接字缓冲区大小的默认值(以字节为单位)。\nnet.core.rmem_default = 262144\n\n#接收套接字缓冲区大小的最大值(以字节为单位)。\n\nnet.core.rmem_max = 16777216\n\n#发送套接字缓冲区大小的默认值(以字节为单位)。\n\nnet.core.wmem_default = 262144\n\n#发送套接字缓冲区大小的最大值(以字节为单位)。\n\nnet.core.wmem_max = 16777216\n\n#用来限制监听(LISTEN)队列最大数据包的数量，超过这个数量就会导致链接超时或者触发重传机制。\n\nnet.core.somaxconn = 262144\n\n#当网卡接收数据包的速度大于内核处理的速度时，会有一个队列保存这些数据包。这个参数表示该队列的最大值。\n\nnet.core.netdev_max_backlog = 262144\n\n#表示系统中最多有多少TCP套接字不被关联到任何一个用户文件句柄上。如果超过这里设置的数字，连接就会复位并输出警告信息。这个限制仅仅是为了防止简单的DoS攻击。此值不能太小。\n\nnet.ipv4.tcp_max_orphans = 262144\n\n#表示那些尚未收到客户端确认信息的连接（SYN消息）队列的长度，默认为1024，加大队列长度为262144，可以容纳更多等待连接的网络连接数。\n\nnet.ipv4.tcp_max_syn_backlog = 262144\n\n#表示系统同时保持TIME_WAIT套接字的最大数量。如果超过此数，TIME_WAIT套接字会被立刻清除并且打印警告信息。之所以要设定这个限制，纯粹为了抵御那些简单的DoS攻击，不过，过多的TIME_WAIT套接字也会消耗服务器资源，甚至死机。\n\nnet.ipv4.tcp_max_tw_buckets = 10000\n\n#表示允许系统打开的端口范围。\n\nnet.ipv4.ip_local_port_range = 1024 65500\n\n#以下两参数可解决生产场景中大量连接的服务器中TIME_WAIT过多问题。\n\n#表示开启TCP连接中TIME_WAIT套接字的快速回收，默认为0，表示关闭。\n\nnet.ipv4.tcp_tw_recycle = 1\n\n#表示允许重用TIME_WAIT状态的套接字用于新的TCP连接,默认为0，表示关闭。\n\nnet.ipv4.tcp_tw_reuse = 1\n\n#当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭。\n\nnet.ipv4.tcp_syncookies = 1\n\n#表示系统允许SYN连接的重试次数。为了打开对端的连接，内核需要发送一个SYN并附带一个回应前面一个SYN的ACK包。也就是所谓三次握手中的第二次握手。这个设置决定了内核放弃连接之前发送SYN+ACK包的数量。\n\nnet.ipv4.tcp_synack_retries = 1\n\n#表示在内核放弃建立连接之前发送SYN包的数量。\n\nnet.ipv4.tcp_syn_retries = 1\n\n#减少处于FIN-WAIT-2连接状态的时间，使系统可以处理更多的连接。\n\nnet.ipv4.tcp_fin_timeout = 30\n\n#这个参数表示当keepalive启用时，TCP发送keepalive消息的频度。默认是2小时，若将其设置得小一些，可以更快地清理无效的连接。\n\nnet.ipv4.tcp_keepalive_time = 600\n\n#探测消息未获得响应时，重发该消息的间隔时间（秒）。系统默认75秒。\n\nnet.ipv4.tcp_keepalive_intvl = 30\n\n#在认定连接失效之前，发送多少个TCP的keepalive探测包。系统默认值是9。这个值乘以tcp_keepalive_intvl之后决定了，一个连接发送了keepalive探测包之后可以有多少时间没有回应。\n\nnet.ipv4.tcp_keepalive_probes = 3\n\n#确定TCP栈应该如何反映内存使用，每个值的单位都是内存页（通常是4KB）。第一个值是内存使用的下限；第二个值是内存压力模式开始对缓冲区使用应用压力的上限；第三个值是内存使用的上限。在这个层次上可以将报文丢弃，从而减少对内存的使用。示例中第一个值为7864324/1024/1024=3G，第二个值为10485764/1024/1024=4G，第三个值为1572864*4/1024/1024=6G。\n\nnet.ipv4.tcp_mem = 786432 1048576 1572864\n\n#此参数限制并发未完成的异步请求数目，应该设置避免I/O子系统故障。\n\nfs.aio-max-nr = 1048576\n\n#该参数决定了系统中所允许的文件句柄最大数目，文件句柄设置代表linux系统中可以打开的文件的数量。\n\nfs.file-max = 6815744\n\n#表示尽量使用内存，减少使用磁盘swap交换分区，内存速度明显高于磁盘一个数量级。\n\nvm.swappiness = 0","tags":["linux","kernal"],"categories":["linux"]},{"title":"邮件发送脚本","url":"//2016/04/28/python-send-mail/","content":"python调用外部smtp服务器发送邮件 \n```python\n#!/usr/bin/env python\n\nfrom email.MIMEText import MIMEText\nfrom email.MIMEMultipart import MIMEMultipart\nfrom email.MIMEBase import MIMEBase\nfrom email import Utils,Encoders\nimport mimetypes,sys\nimport smtplib\n\ndef send_mail(user,passwd,host,context,sub,to_user):\n    self_user = user\n    self_pass = passwd\n    self_smtp = host\n    self_sub  = sub\n    self_context = context\n    self_touser = to_user\n\n    msg = MIMEMultipart()\n    msg[\"To\"] = self_touser\n    msg[\"From\"] = \"xxxx@unxn.com\"\n    msg[\"Subject\"] = self_sub\n    msg[\"Data\"] = Utils.formatdate(localtime = 1)\n    \n    body = MIMEText(self_context,_subtype = \"plain\")\n    \n    msg.attach(body)\n    send = smtplib.SMTP(self_smtp)\n    send.login(self_user,self_pass)\n    send.sendmail(self_user,self_touser,msg.as_string())\n    send.close()\n\nif __name__=='__main__':\n    host= \"smtp.qq.com\"\n    user= \"xxxxxxxx@qq.com\"\n    passwd= \"xxxxxxxx\"\n    to_user = \"xxxxxxxxx@163.com\"\n    sub = \"Log anaylse result!\"\n\n    f = open(\"log\",\"r\")\n    context = f.read()\n    f.close()\n    \n    send_mail(user,passwd,host,context,sub,to_user)\n```","tags":["python","mail"],"categories":["python"]},{"title":"python socket编程","url":"//2016/04/28/python-code-socket/","content":"需要在多台服务器上运行日志分析脚本，分析完成后每台机器直接发送邮件会出现大量邮件同时过来，现在想将多台机器的分析结果收集起来，通过网络发送到服务端，在服务端收集所有的日志，在统一发送一封邮件，实现的socket代码如下。\n\n### 接收端（server）\n```python\n#!/usr/bin/env python\nimport socket\ndef socket_server(host,port):\n    self_host = host\n    self_port = port\n\n    s = socket.socket(socket.AF_INET,socket.SOCK_STREAM)\n    s.bind((self_host,self_port))\n    s.listen(4)\n\n    f = open(\"log\", \"a\")\n    while True:\n        conn,addr=s.accept()\n        data = conn.recv(1024)\n        print 'data:', data\n        f.write(data)\n    f.close()\n    s.close()\n\nif __name__ == \"__main__\":\n    host = '127.0.0.1'\n    port = 65530\n\n    socket_server(host,port)\n```\n### 发送端（client）\n```python\n#!/usr/bin/env python\nimport socket\n\ndef socket_client(host,port,content):\n    self_host = host\n    self_port = port\n    self_content = content\n\n    s = socket.socket(socket.AF_INET,socket.SOCK_STREAM)\n    s.connect((self_host,self_port))\n    s.sendall(content)\n    s.close()\n\nif __name__ == \"__main__\":\n    host = \"127.0.0.1\"\n    port = 65530\n\n    f = open(\"sendfile\",\"r\")\n    content = f.read()\n    f.close()\n\n    socket_client(host, port, content)\n```","tags":["python","socket"],"categories":["python"]},{"title":"window系统上部署Zabbix_agent","url":"//2016/04/13/deploy-zabbix_agent-to-windows/","content":"\n1.获取windows的agent客户端，解压文件至指定位置\n\n\t![img](/images/img_570da13048078.png)\n\n2.修改conf文件中的\n\n```shell\nServer=server端的IP地址\n\nServerActive=server端的IP地址\n\nHostName=服务器在监控端上的监控名，可见名称可以不写默认为主机名称\n```\n\n![img](/images/img_570da1465a8dc.png)\n\n3.命令行模式下安装Zabbix_agent监控程序\n\n```shell\n# 进入解压后的文件夹下\n\ncd  C:\\zabbix_agents\\bin\\win64\n\n#安装程序\n\nzabbix_agentd.exe –c c:\\zabbix\\zabbix_agentd.conf -i\n\n#启动程序\n\nzabbix_agentd.exe –c c:\\zabbix\\zabbix_agentd.conf -s\n\n```\n\n![img](/images/img_570da15f9ead1.png)\n\n\n```markdown\n参数含义：\n-c    指定配置文件所在位置\n-i     安装客户端\n-s    启动客户端\n-x    停止客户端\n-d    卸载客户端\n```\n\n\n4.监控端添加agent服务器注意主机名为conf文件中定义的名称，否则会导致某些监控项目异常\n\n![img](/images/img_570da173ef401.png)\n\n ","tags":["windows","zabbix"],"categories":["zabbix"]},{"title":"Tomcat虚拟主机别名设置","url":"//2016/04/13/tomcat-server-name-alias/","content":"一个虚拟空间需要绑定多个域名时可以通过alias标签来设置别名，详见如下配置文件部分截图\n```xml\n <Engine name=\"Catalina\" defaultHost=\"localhost\">\n      <!--For clustering, please take a look at documentation at:\n          /docs/cluster-howto.html  (simple how to)\n          /docs/config/cluster.html (reference documentation)\n       -->\n      <!--\n      <Cluster className=\"org.apache.catalina.ha.tcp.SimpleTcpCluster\"/>\n      -->\n      <!-- Use the LockOutRealm to prevent attempts to guess user passwords\n           via a brute-force attack -->\n      <Realm className=\"org.apache.catalina.realm.LockOutRealm\">\n        <!-- This Realm uses the UserDatabase configured in the global JNDI\n             resources under the key \"UserDatabase\".  Any edits\n             that are performed against this UserDatabase are immediately\n             available for use by the Realm.  -->\n        <Realm className=\"org.apache.catalina.realm.UserDatabaseRealm\"\n               resourceName=\"UserDatabase\"/>\n      </Realm>\n        <Host name=\"www.xiemx.com\" debug=\"0\" appBase=\"webapps\" unpackWARs=\"true\" autoDeploy=\"true\" xmlValidation=\"false\" xmlNamespaceAware=\"false\">\n                <Alias>xiemx.com</Alias>\n                <Alias>xx.xiemx.com</Alias>\n                <Alias>yy.xiemx.com</Alias> \n        </Host>\n  </Engine>\n  ```","tags":["webserver","http","tomcat"],"categories":["tomcat"]},{"title":"tomcat出现PermGen Space问题","url":"//2016/03/29/tomcat-permgen-space/","content":"### 现象\ntomcat服务器运行一段时间，总是会自动报异常：`java.lang.OutOfmemoryError:PermGen Space`的错误，导致项目无法正常运行。\n\n出现这个错误是由于内存泄漏。PermGen Space指的是内存的永久保存区，该块内存主要是被JVM存放class和mete信息的，当class被加载loader的时候就会被存储到该内存区中，与存放类的实例的heap区不同，java中的垃圾回收器GC不会在主程序运行期对PermGen space进行清理，所以当我们的应用中有很多的class时，很可能就会出现PermGen space的错误。\n\n### 解决方法\n手动设置MaxPermSize的大小\n\n修改 TOMCAT_HOME/bin/catalina.bat(Linux上为catalina.sh)文件，在echo \"using CATALINA_BASE：$CATALINA_BASE\"上面加入这一行内容：\n```shell\nset JAVA_OPTS=%JAVA_OPTS% -server -XX:PermSize=128m -XX:MaxPermSize=512m\n```","tags":["webserver","http","tomcat"],"categories":["tomcat"]},{"title":"python中is和==的区别","url":"//2016/03/29/python-is/","content":"Python中的对象包含三要素：`id`、`type`、`value`\n  * id用来唯一标识一个对象\n  * type标识对象的类型\n  * value是对象的值。\n  \n`is` 通过id来判断对象是否相等\n  \n`==` 通过value来判断值是否相等\n\n例：\n```python\n>>> a = {1:2}\n>>> b = a.copy()\n>>> a is b\nFalse\n>>> a == b\nTrue\n>>>\n```","tags":["python"],"categories":["python"]},{"title":"Zentaopms部署","url":"//2016/03/25/zentaopms-deployment/","content":"\n\n\n \n\n1. 获取软件\n\n   ```shell\n   wget http://sourceforge.net/projects/zentao/files/8.1.3/ZenTaoPMS.8.1.3.zip/download\n   ```\n\n2. 将软件解压放到web服务器的目录下\n\n    ![img](/images/img_56f4aa5d219ba-20190917143633356.png)\n\n3. 创建数据库，授权用户\n\n   ```sql\n   create database zentao;grant all zentao.* to zentao@localhost identified by 'zentap';\n   ```\n\n   \n\n   ![img](/images/img_56f4aa6c69a26-20190917143633077.png)\n\n4. Web访问页面开启安装进程\n\n   访问`http://localhost/zentaopms/www/install.php`，同dz等程序安装相同注意配置时使用授权的账户密码和数据库\n\n   \n\n5. 安装完成后需要破解zentaopms方可正常登录\n\n   通过下面的地址下载loader-wizard：http://www.ioncube.com/loader-wizard/loader-wizard.zip 下载之后，将其解压缩，到web服务器的DocumentRoot下。\n\n   \n\n6. 使用浏览器访问loader-wizard.php文件。该文件会检测当前环境，给出提示解决方法依照步骤处理\n  ![img](/images/img_56f4aa815cb3c-20190917143633401.png)\n\n\n7. 重新web服务启动之后，再次访问loader.php，如果安装成功，系统会提示。\n\n   ![img](/images/img_56f4aa8c14ae0-20190917145027287.png)\n\n   看到这个界面，就表示解密软件已经安装成功了。\n\n8. 再次访问zentaopms的首页测试是否可以正常登录\n  ![img](/images/img_56f4aa989af5e-20190917143633212.png)\n\n \n\n","tags":["zentaopms","deployment"],"categories":["zentaopms","deployment"]},{"title":"zabbix监控特定进程","url":"//2016/03/25/zabbix-monitor-process/","content":"\n1. 在特定机器或模板上创建新的监控项，点击Key 后面的Select 按钮，选择如下两项，一项是用来监控特定进程的数量，另一项是用来监控进程使用内存的大小。\n\n![img](/images/img_56f4a9789a5c5.png)\n\n2. 以下是对squid进程的监控配置，key中的参数说明，第一个参数是进程名字，没必要填写，填了反而会使监控不太准确（仅个人测试），第二个参数是运行进程的用户名，第三个为进程的状态 ，包括：*all*(default),*run*,*sleep*,*zomb*，第四个参数用来指定进程名中包含的字符，对进程进行过滤。\n\n![img](/images/img_56f4a9875257e.png)\n\n3. 配置好监控项后，添加触发器，如下触发器表示最后两次的值都是0，说明没有squid进程在运行，则出发报警。\n\n![img](/images/img_56f4a99aa9435.png)\n\n4. 当我们kill掉进程tomcat会如下报警\n\n![img](/images/img_56f4a9ab888d0.png)","tags":["zabbix"],"categories":["zabbix"]},{"title":"python tab自动补全模块","url":"//2016/03/04/python-tab/","content":"```python\n#vi tab.py\n\n#!/usr/bin/env python \n# python startup file \nimport sys\nimport readline\nimport rlcompleter\nimport atexit\nimport os\n# tab completion \nreadline.parse_and_bind('tab: complete')\n# history file \nhistfile = os.path.join(os.environ['HOME'], '.pythonhistory')\ntry:\n    readline.read_history_file(histfile)\nexcept IOError:\n    pass\natexit.register(readline.write_history_file, histfile)\ndel os, histfile, readline, rlcompleter\n```","tags":["python"],"categories":["python"]},{"title":"Mysql-proxy代理程序","url":"//2016/02/18/build-mysql-proxy/","content":"1、安装Lua、glib、pkg-config、libevent、GCC、make等工具不同的环境下不同\n\n2、安装mysql-proxy\n```shell\ntar xzf mysql-proxy-0.8.1.tar.gz\ncd mysql-proxy-0.8.1\n./configure  --prefix=/usr/local/mysql-proxy[root@node1 bin]# /usr/local/mysql-proxy/bin/mysql-proxy     --help-all\nUsage:\nmysql-proxy [OPTION...] - MySQL Proxy\nHelp Options:\n-h, --help                                              Show help options\n--help-all                                              Show all help options\n--help-proxy                                            Show options for the proxy-module\n\nproxy-module\n  -P, --proxy-address=<host:port>                         listening address:port of the proxy-server (default: :4040)\n  -r, --proxy-read-only-backend-addresses=<host:port>     address:port of the remote slave-server (default: not set)\n  -b, --proxy-backend-addresses=<host:port>               address:port of the remote backend-servers (default: 127.0.0.1:3306)\n--proxy-skip-profiling                                  disables profiling of queries (default: enabled)\n--proxy-fix-bug-25371                                   fix bug #25371 (mysqld > 5.1.12) for older libmysql versions\n-s, --proxy-lua-script=<file>                           filename of the lua script (default: not set)\n--no-proxy                                              don't start the proxy-module (default: enabled)\n--proxy-pool-no-change-user                             don't use CHANGE_USER to reset the connection coming from the pool (default: enabled)\n--proxy-connect-timeout                                 connect timeout in seconds (default: 2.0 seconds)\n--proxy-read-timeout                                    read timeout in seconds (default: 8 hours)\n--proxy-write-timeout                                   write timeout in seconds (default: 8 hours)\n\nApplication Options:\n-V, --version                                           Show version\n--defaults-file=<file>                                  configuration file\n--verbose-shutdown                                      Always log the exit code when shutting down\n--daemon                                                Start in daemon-mode\n--user=<user>                                           Run mysql-proxy as user\n--basedir=<absolute path>                               Base directory to prepend to relative paths in the config\n--pid-file=<file>                                       PID file in case we are started as daemon\n--plugin-dir=<path>                                     path to the plugins\n--plugins=<name>                                        plugins to load\n--log-level=(error|warning|info|message|debug)          log all messages of level ... or higher\n--log-file=<file>                                       log all messages in a file\n--log-use-syslog                                        log all messages to syslog\n--log-backtrace-on-crash                                try to invoke debugger on crash\n--keepalive                                             try to restart the proxy if it crashed\n--max-open-files                                        maximum number of open files (ulimit -n)\n--event-threads                                         number of event-handling threads (default: 1)\n--lua-path=<...>                                        set the LUA_PATH\n--lua-cpath=<...>                                       set the LUA_CPATH\n```","tags":["mysql","mysql-proxy"],"categories":["mysql"]},{"title":"Mysql数据库代理amoeba","url":"//2016/02/18/mysql-proxy-amoeba/","content":"基本环境：\n```\nA（172.25.16.10）：客户端    \nB（172.25.16.11）：主数据库       \nC（172.25.16.12）：从数据库     \nD（172.25.16.13）：从数据库\nJ（172.25.16.19）：代理数据库          \n```\n\n基于amoeba的读写分离\n\n1. 在server j 安装JDK\n\n2. 新建目录/usr/local/amoeba\n```shell\nmkdir /usr/local/amoeba\n```\n3. 把压缩包解压到该目录/usrl/local/amoeba\n\n4. 在conf目录下有amoeba的配置文件\n  需要修改的两个：`amoeba.xml` 和 `dbServers.xml`\n### amoeba.xml(3个地方要改):\n#### 监听的端口号：\n```\n<proxy>\n<!-- service class must implements com.meidusa.amoeba.service.Service -->\n  <service name=\"Amoeba for Mysql\" class=\"com.meidusa.amoeba.net.ServerableConnectionManager\">\n<!-- port -->\n<property name=\"port\">3306</property>\n```\n#### 客户端访问时用的用户名\n```\n<property name=\"authenticator\">\n\n  <bean class=\"com.meidusa.amoeba.mysql.server.MysqlClientAuthenticator\">\n  <property name=\"user\">amoeba</property>\n  <property name=\"password\">amoeba</property>\n  <property name=\"filter\">\n    <bean class=\"com.meidusa.amoeba.server.IPAccessController\">\n      <property name=\"ipFile\">${amoeba.home}/conf/access_list.conf</property>\n    </bean>\n  </property>\n  </bean>\n</property>\n```\n#### read/write pool\n```\n<property name=\"sqlFunctionFile\">${amoeba.home}/conf/functionMap.xml</property>\n<property name=\"LRUMapSize\">1500</property>\n     <property name=\"defaultPool\">serverb#默认访问的数据库#</property>\n            <property name=\"writePool\">serverb#指定可写池#</property>\n            <property name=\"readPool\">readgroup1#指定只读池#</property>\n<property name=\"needParse\">true</property>\n```\n\n### dbServers.xml文件：\n\n#### 修改端口号，用户名，密码。\n```\n<dbServer name=\"abstractServer\" abstractive=\"true\">\n<factoryConfig class=\"com.meidusa.amoeba.mysql.net.MysqlServerConnectionFactory\">\n  <property name=\"manager\">${defaultManager}</property>\n  <property name=\"sendBufferSize\">64</property>\n  <property name=\"receiveBufferSize\">128</property>\n  <!-- mysql port -->\n  <property name=\"port\">3306</property>\n  <!-- mysql schema -->\n  <property name=\"schema\">test</property>\n  <!-- mysql user -->\n  <property name=\"user\">dbproxy</property>\n  <!--  mysql password  -->\n  <property name=\"password\">xiemx</property>\n</factoryConfig>\n```\n \n\n#### 指定可以代理的数据库\n```\n<dbServer name=\"serverb\"  parent=\"abstractServer\">\n  <factoryConfig>\n    <!-- mysql ip -->\n    <property name=\"ipAddress\">172.25.16.11</property>\n  </factoryConfig>\n</dbServer>\n\n<dbServer name=\"serverc\"  parent=\"abstractServer\">\n  <factoryConfig>\n    <!-- mysql ip -->\n    <property name=\"ipAddress\">172.25.16.12</property>\n  </factoryConfig>\n</dbServer>\n\n<dbServer name=\"serverd\"  parent=\"abstractServer\">\n  <factoryConfig>\n    <!-- mysql ip -->\n    <property name=\"ipAddress\">172.25.16.13</property>\n  </factoryConfig>\n</dbServer>\n```\n \n\n#### 指定只读组\n```\n<dbServer name=\"readgroup1\" virtual=\"true\">\n  <poolConfig class=\"com.meidusa.amoeba.server.MultipleServerPool\">\n    <!-- Load balancing strategy: 1=ROUNDROBIN , 2=WEIGHTBASED , 3=HA-->\n    <property name=\"loadbalance\">1</property>\n    <!-- Separated by commas,such as: server1,server2,server1 -->\n    <property name=\"poolNames\">serverc,serverd</property>\n  </poolConfig>\n</dbServer>\n```\n \n\n5. 启动脚本在/bin下\n启动时会报错，需要修改文件即可：`DEFAULT_OPTS=\"-server -Xms256m -Xmx256m -Xss228k\"`(最后一个参数改为228即可)\n```shell\n./amoeba start\n```\n6. 在BCD上授权允许J访问数据库\n```sql\ngrant all on db1.* to dbproxy@'172.25.16.19' identified by 'xiemx';\n```","tags":["mysql","mysql-proxy","amoeba"],"categories":["mysql"]},{"title":"UDEV规则","url":"//2016/02/18/udev/","content":"udev根据系统中的硬件设备的状态动态更新设备，通过对内核产生的设备名增加别名的方式来达到不管设备连接的顺序而维持一个统一的设备名的目的。udev可以根据设备的其他信息如总线（bus），生产商（vendor）等不同来区分不同的设备，并产生设备文件。udev是硬件平台无关的，属于user space的进程，它脱离驱动层的关联而建立在操作系统之上，遵循Linux Standard Base （LSB）设备命名方法，但也可以自定义命名\n\n### 工作流程图\n\n![image-20191018151450374](/images/image-20191018151450374.png)\n\n### 配置文件、rule参数\n\n```conf\n[mingxu.xie@cn-aux-cc udev]$ ll -R\n.:\ntotal 8\ndrwxr-xr-x 2 root root 4096 Jun  6 02:58 rules.d\n-rw-r--r-- 1 root root  218 Mar 19  2014 udev.conf\n\n./rules.d:\ntotal 32\n-rw-r--r-- 1 root root  640 Jul 31  2016 51-ec2-hvm-devices.rules\n-rw-r--r-- 1 root root  641 Jul 31  2016 52-ec2-vcpu.rules\n-rw-r--r-- 1 root root  740 Jul 31  2016 53-ec2-network-interfaces.rules\n-rw-r--r-- 1 root root  680 Jul 31  2016 60-cdrom_id.rules\n-rw-r--r-- 1 root root  326 Apr 26  2016 60-raw.rules\n-rw-r--r-- 1 root root 1343 Jul 31  2016 70-ec2-nvme-devices.rules\n-rw-r--r-- 1 root root 1424 Jul 31  2016 75-persistent-net-generator.rules\n-rwxr-xr-x 1 root root  343 Aug 21  2016 80-docker.rules\n\n# 主配置 /etc/udev/udev.conf \nudev_root=\"/dev/\"\nudev_rules=\"/etc/udev/rules.d/\"\n\n#rule规则的文件命名第一段为执行顺序，同rc脚本\n[mingxu.xie@cn-aux-cc udev]$ cat rules.d/53-ec2-network-interfaces.rules\n# Copyright (C) 2012 Amazon.com, Inc. or its affiliates.\n# All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\").\n# You may not use this file except in compliance with the License.\n# A copy of the License is located at\n#\n#    http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS\n# OF ANY KIND, either express or implied. See the License for the\n# specific language governing permissions and limitations under the\n# License.\n\nACTION==\"add\", SUBSYSTEM==\"net\", KERNEL==\"eth*\", IMPORT{program}=\"/bin/sleep 1\"\nSUBSYSTEM==\"net\", KERNEL==\"eth*\", RUN+=\"/etc/sysconfig/network-scripts/ec2net.hotplug\"\n```\n```conf\n#监听一个docker run -it --rm nginx bash容器启动的设备变化情况，如下\n[mingxu.xie@cn-aux-cc udev]$ udevadm monitor\nmonitor will print the received events for:\nUDEV - the event which udev sends out after rule processing\nKERNEL - the kernel uevent\n\nKERNEL[240932.229041] add      /devices/virtual/bdi/253:8 (bdi)\nKERNEL[240932.229579] add      /devices/virtual/block/dm-8 (block)\nUDEV  [240932.229612] add      /devices/virtual/bdi/253:8 (bdi)\nUDEV  [240932.229641] add      /devices/virtual/block/dm-8 (block)\nKERNEL[240932.248388] change   /devices/virtual/block/dm-8 (block)\nUDEV  [240932.272904] change   /devices/virtual/block/dm-8 (block)\nKERNEL[240932.425971] remove   /devices/virtual/block/dm-8 (block)\nUDEV  [240932.428073] remove   /devices/virtual/block/dm-8 (block)\nKERNEL[240932.456141] remove   /devices/virtual/bdi/253:8 (bdi)\nKERNEL[240932.456261] remove   /devices/virtual/block/dm-8 (block)\nUDEV  [240932.456380] remove   /devices/virtual/bdi/253:8 (bdi)\nUDEV  [240932.456419] remove   /devices/virtual/block/dm-8 (block)\nKERNEL[240932.469307] add      /devices/virtual/bdi/253:8 (bdi)\nKERNEL[240932.469384] add      /devices/virtual/block/dm-8 (block)\nUDEV  [240932.469657] add      /devices/virtual/bdi/253:8 (bdi)\nUDEV  [240932.469691] add      /devices/virtual/block/dm-8 (block)\nKERNEL[240932.488228] change   /devices/virtual/block/dm-8 (block)\nUDEV  [240932.504203] change   /devices/virtual/block/dm-8 (block)\nKERNEL[240932.636785] remove   /devices/virtual/block/dm-8 (block)\nUDEV  [240932.638755] remove   /devices/virtual/block/dm-8 (block)\nKERNEL[240932.676137] remove   /devices/virtual/bdi/253:8 (bdi)\nKERNEL[240932.676245] remove   /devices/virtual/block/dm-8 (block)\nUDEV  [240932.676367] remove   /devices/virtual/bdi/253:8 (bdi)\nUDEV  [240932.676420] remove   /devices/virtual/block/dm-8 (block)\nKERNEL[240932.683098] add      /devices/virtual/bdi/253:8 (bdi)\nKERNEL[240932.683204] add      /devices/virtual/block/dm-8 (block)\nUDEV  [240932.683384] add      /devices/virtual/bdi/253:8 (bdi)\nUDEV  [240932.683582] add      /devices/virtual/block/dm-8 (block)\nKERNEL[240932.696227] change   /devices/virtual/block/dm-8 (block)\nUDEV  [240932.712517] change   /devices/virtual/block/dm-8 (block)\nKERNEL[240932.793677] add      /devices/virtual/net/veth4d1712b (net)\nKERNEL[240932.793699] add      /devices/virtual/net/veth4d1712b/queues/rx-0 (queues)\nKERNEL[240932.793727] add      /devices/virtual/net/veth4d1712b/queues/tx-0 (queues)\nKERNEL[240932.793908] add      /devices/virtual/net/veth17f6e3e (net)\nKERNEL[240932.793930] add      /devices/virtual/net/veth17f6e3e/queues/rx-0 (queues)\nKERNEL[240932.793942] add      /devices/virtual/net/veth17f6e3e/queues/tx-0 (queues)\nUDEV  [240932.827598] add      /devices/virtual/net/veth4d1712b (net)\nUDEV  [240932.827843] add      /devices/virtual/net/veth4d1712b/queues/rx-0 (queues)\nUDEV  [240932.828121] add      /devices/virtual/net/veth4d1712b/queues/tx-0 (queues)\nKERNEL[240932.849171] add      /kernel/slab/:A-0000200/cgroup/vm_area_struct(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nKERNEL[240932.849196] add      /kernel/slab/:A-0000064/cgroup/anon_vma_chain(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nUDEV  [240932.849211] add      /kernel/slab/:A-0000200/cgroup/vm_area_struct(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nKERNEL[240932.849225] add      /kernel/slab/anon_vma/cgroup/anon_vma(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nKERNEL[240932.849238] add      /kernel/slab/:aA-0000192/cgroup/dentry(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nKERNEL[240932.849252] add      /kernel/slab/proc_inode_cache/cgroup/proc_inode_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nUDEV  [240932.849269] add      /kernel/slab/:A-0000064/cgroup/anon_vma_chain(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nKERNEL[240932.849436] add      /kernel/slab/:0000064/cgroup/kmalloc-64(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nKERNEL[240932.849455] add      /kernel/slab/shmem_inode_cache/cgroup/shmem_inode_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nUDEV  [240932.849709] add      /kernel/slab/proc_inode_cache/cgroup/proc_inode_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nUDEV  [240932.850033] add      /kernel/slab/anon_vma/cgroup/anon_vma(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nUDEV  [240932.850374] add      /kernel/slab/shmem_inode_cache/cgroup/shmem_inode_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nUDEV  [240932.850527] add      /kernel/slab/:aA-0000192/cgroup/dentry(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nUDEV  [240932.852368] add      /kernel/slab/:0000064/cgroup/kmalloc-64(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nKERNEL[240932.854849] add      /kernel/slab/:0000192/cgroup/kmalloc-192(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nKERNEL[240932.854871] add      /kernel/slab/:0001024/cgroup/kmalloc-1024(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nKERNEL[240932.855103] add      /kernel/slab/radix_tree_node/cgroup/radix_tree_node(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nUDEV  [240932.855120] add      /kernel/slab/:0000192/cgroup/kmalloc-192(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nKERNEL[240932.855133] add      /kernel/slab/:A-0000192/cgroup/cred_jar(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nKERNEL[240932.855145] add      /kernel/slab/:A-0002048/cgroup/mm_struct(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nKERNEL[240932.855158] add      /kernel/slab/mqueue_inode_cache/cgroup/mqueue_inode_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nKERNEL[240932.855170] add      /kernel/slab/sock_inode_cache/cgroup/sock_inode_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nKERNEL[240932.855359] add      /kernel/slab/:A-0009664/cgroup/task_struct(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nUDEV  [240932.855395] add      /kernel/slab/:A-0002048/cgroup/mm_struct(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nKERNEL[240932.855410] add      /kernel/slab/:A-0000704/cgroup/files_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nUDEV  [240932.855445] add      /kernel/slab/mqueue_inode_cache/cgroup/mqueue_inode_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nKERNEL[240932.855461] add      /kernel/slab/sighand_cache/cgroup/sighand_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nUDEV  [240932.855474] add      /kernel/slab/:0001024/cgroup/kmalloc-1024(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nKERNEL[240932.855487] add      /kernel/slab/:A-0001024/cgroup/signal_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nKERNEL[240932.855499] add      /kernel/slab/:A-0000128/cgroup/pid(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nUDEV  [240932.855512] add      /kernel/slab/radix_tree_node/cgroup/radix_tree_node(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nUDEV  [240932.856010] add      /kernel/slab/sock_inode_cache/cgroup/sock_inode_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nUDEV  [240932.856029] add      /kernel/slab/:A-0009664/cgroup/task_struct(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nUDEV  [240932.856045] add      /kernel/slab/sighand_cache/cgroup/sighand_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nKERNEL[240932.856077] add      /kernel/slab/:0000256/cgroup/kmalloc-256(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nUDEV  [240932.856092] add      /kernel/slab/:A-0000704/cgroup/files_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nKERNEL[240932.856104] add      /kernel/slab/:0000512/cgroup/kmalloc-512(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nUDEV  [240932.856188] add      /kernel/slab/:A-0000128/cgroup/pid(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nUDEV  [240932.857080] add      /kernel/slab/:A-0000192/cgroup/cred_jar(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nUDEV  [240932.857105] add      /kernel/slab/:A-0001024/cgroup/signal_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nUDEV  [240932.857126] add      /kernel/slab/:0000256/cgroup/kmalloc-256(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nUDEV  [240932.857140] add      /kernel/slab/:0000512/cgroup/kmalloc-512(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nKERNEL[240932.862157] add      /kernel/slab/xfs_inode/cgroup/xfs_inode(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nUDEV  [240932.862348] add      /kernel/slab/xfs_inode/cgroup/xfs_inode(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nKERNEL[240932.863008] add      /kernel/slab/inode_cache/cgroup/inode_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nUDEV  [240932.863179] add      /kernel/slab/inode_cache/cgroup/inode_cache(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nUDEV  [240932.932564] add      /devices/virtual/net/veth17f6e3e (net)\nUDEV  [240932.932779] add      /devices/virtual/net/veth17f6e3e/queues/rx-0 (queues)\nUDEV  [240932.932880] add      /devices/virtual/net/veth17f6e3e/queues/tx-0 (queues)\nKERNEL[240932.960183] remove   /devices/virtual/net/veth4d1712b (net)\nUDEV  [240932.982851] remove   /devices/virtual/net/veth4d1712b (net)\nKERNEL[240933.089679] add      /kernel/slab/:0000008/cgroup/kmalloc-8(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nUDEV  [240933.089933] add      /kernel/slab/:0000008/cgroup/kmalloc-8(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nKERNEL[240933.150833] add      /kernel/slab/:0002048/cgroup/kmalloc-2048(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nKERNEL[240933.150863] add      /kernel/slab/:0000096/cgroup/kmalloc-96(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nUDEV  [240933.151052] add      /kernel/slab/:0002048/cgroup/kmalloc-2048(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\nUDEV  [240933.151130] add      /kernel/slab/:0000096/cgroup/kmalloc-96(990:53b4e439f50242461e4c1246718e12d769d3bc4752c0f0ed5bc4f317bb4feb3d) (cgroup)\n\n```","tags":["linux","udev"],"categories":["linux"]},{"title":"三种共享存储比较","url":"//2016/02/18/Three-shared-storage-comparisons/","content":"\n### 共享存储（Share Storage）类型\n\n  1. NAS(Network Attached Storage)网络附加存储\n  2. SAN (Storage Area Network)储存区域网络\n  3. iSAN (internet Storage Area Network )以太网存储区域网络,基于以太网的san\n\n#### NAS(Network Attached Storage)\n  1. 基于tcp/ip网络\n  2. 以文件为单位进行操作（文件锁）\n\n#### SAN (Storage Area Network)\n  1. 基于硬盘驱动协议（sisc）传输的是磁道/扇区信息\n  2. 基于扇区/block锁\n\n#### iSAN (internet Storage Area Network )\n  1. 基于tcp/ip网络 , 通过以太网数据包传递scsi协议数据\n  2. 基于扇区/block锁","categories":["storage"]},{"title":"ISCSI存储网络构建","url":"//2016/02/18/iscsi-store/","content":"iscsi的结构和san的结构\n\n[![san](/images/san.png)](http://www.xiemx.com/wp-content/uploads/2016/02/san.png)\n\n#### iSCSI 通信端\n* 发起 I/O 请求的启动设备(Initiator)\n* 响应请求并执行实际 I/O 操作的目标设备(Target)\n\n#### iSCSI工作过程\n\n* target端导出共享设备\n* initiator端发现设备\n* initiator端导入设备\n* initiator端分区、格式化、挂接设备\n\n#### iSCSI实现套件\n服务端（target）：scsi-target-utils软件包\n客户端（initiator）：iscsi-initiator-utils软件包\n\n### 部署过程：\n```\nyum insitall scsi-target-utils\nyum insitall iscsi-initiator-utils\n```\n服务器端导出共享设备： 配置文件如下\n```\n[root@rhel6 ~]# grep -v '#\\|^$' /etc/tgt/targets.conf\ndefault-driver iscsi\n<target iqn.2015-10.com.example-f30:vdb1-1g>\n  backing-store /dev/vdb1\n</target>\n```\n 启动tgtd服务\n```shell\n[root@rhel6 ~]# /etc/init.d/tgtd start\nStarting SCSI target daemon:                [ OK ]\n\n ```\n\n查看客户端本地有无/dev/sda设备\n```shell\n[root@rhel6 ~]# ll /dev/sda\nls: cannot access /dev/sda: No such file or directory\n```\n搜索target端的共享设备\n```shell\n[root@rhel6 ~]# iscsiadm -m discovery -t st -p 172.25.30.10\n172.25.30.10:3260,1 iqn.2015-10.com.example-f30:vdb1-1g\n```\n 导入target端的共享设备\n```shell\n[root@rhel6 ~]# iscsiadm -m node -l\nLogging in to [iface: default, target: iqn.2015-10.com.example-f30:vdb1-1g, portal: 172.25.30.10,3260] (multiple)\nLogin to [iface: default, target: iqn.2015-10.com.example-f30:vdb1-1g, portal: 172.25.30.10,3260] successful.\n```\n 查看本地/dev/sda设备\n```shell\n[root@rhel6 ~]# ll /dev/sda\nbrw-rw----. 1 root disk 8, 0 Jan 14 14:51 /dev/sda\n```\n卸载导入的设备\n```shell\n[root@rhel6 nodes]# ll /dev/sd*\nbrw-rw----. 1 root disk 8, 0 Jan 14 14:53 /dev/sda\nbrw-rw----. 1 root disk 8, 16 Jan 14 15:35 /dev/sdb\nbrw-rw----. 1 root disk 8, 17 Jan 14 15:35 /dev/sdb1\n\n[root@rhel6 nodes]# iscsiadm -m node -T iqn.2016-01-14.com.example.node4-f24:vdb1-1G -u\nLogging out of session [sid: 2, target: iqn.2016-01-14.com.example.node4-f24:vdb1-1G, portal: 172.25.24.13,3260]\nLogout of [sid: 2, target: iqn.2016-01-14.com.example.node4-f24:vdb1-1G, portal: 172.25.24.13,3260] successful.\n\n[root@rhel6 nodes]# ll /dev/sd*\nbrw-rw----. 1 root disk 8, 0 Jan 14 14:53 /dev/sda\n```\n### 配置iSCSI的acl和验证\n\n在target主配置文件中添加如下两行\n```\n[root@rhel6 ~]# grep -v '#\\|^$' /etc/tgt/targets.conf\ndefault-driver iscsi\n<target iqn.2015-10.com.example-f30:vdb1-1g>\n  backing-store /dev/vdb1\n</target>\n\n<target iqn.2015-10.com.example-f30:vdb2-2g>\n  backing-store /dev/vdb2\n  initiator-address 172.25.30.0/24\n  incominguser xiemx uplooking\n</target>\n```\n重新加载配置文件\n```\n/etc/init.d/tgtd   reload\n/etc/init.d/tgtd   force-reload\n```\n \n\n查看配置导出的设备信息\n```shell\n[root@rhel6 ~]# tgtadm --lld iscsi --mode target --op show\nTarget 1: iqn.2015-10.com.example-f30:vdb1-1g\n  System information:\n​    Driver: iscsi\n​    State: ready\n  I_T nexus information:\n  LUN information:\n​    LUN: 0\n​      Type: controller\n​      SCSI ID: IET   00010000\n​      SCSI SN: beaf10\n​      Size: 0 MB, Block size: 1\n​      Online: Yes\n​      Removable media: No\n​      Prevent removal: No\n​      Readonly: No\n​      Backing store type: null\n​      Backing store path: None\n​      Backing store flags:\n​    LUN: 1\n​      Type: disk\n​      SCSI ID: IET   00010001\n​      SCSI SN: beaf11\n​      Size: 1074 MB, Block size: 512\n​      Online: Yes\n​      Removable media: No\n​      Prevent removal: No\n​      Readonly: No\n​      Backing store type: rdwr\n​      Backing store path: /dev/vdb1\n​      Backing store flags:\n  Account information:\n  ACL information:\n​    ALL\n Target 2: iqn.2015-10.com.example-f30:vdb2-2g\n  System information:\n​     Driver: iscsi\n​     State: ready\n  I_T nexus information:\n  LUN information:\n​    LUN: 0\n​      Type: controller\n​      SCSI ID: IET   00020000\n​      SCSI SN: beaf20\n​      Size: 0 MB, Block size: 1\n​      Online: Yes\n​      Removable media: No\n​      Prevent removal: No\n​      Readonly: No\n​      Backing store type: null\n​      Backing store path: None\n​      Backing store flags:\n​    LUN: 1\n​      Type: disk\n​      SCSI ID: IET   00020001\n​      SCSI SN: beaf21\n​      Size: 2148 MB, Block size: 512\n​      Online: Yes\n​      Removable media: No\n​      Prevent removal: No\n​      Readonly: No\n​      Backing store type: rdwr\n​      Backing store path: /dev/vdb2\n​      Backing store flags:\n  Account information:\n​     xiemx\n  ACL information:\n​    172.25.30.0/24\n\n```\n\ninitiator端开启CHAP验证并配置用户密码\n\n```shell\n[root@rhel6 ~]grep -v '^#\\|^$' /etc/iscsi/iscsid.conf\niscsid.startup = /etc/rc.d/init.d/iscsid force-start\nnode.startup = automatic\nnode.leading_login = No\nnode.session.auth.authmethod = CHAP\nnode.session.auth.username = xiemx\nnode.session.auth.password = uplooking\nnode.session.timeo.replacement_timeout = 120\nnode.conn[0].timeo.login_timeout = 15\nnode.conn[0].timeo.logout_timeout = 15\nnode.conn[0].timeo.noop_out_interval = 5\nnode.conn[0].timeo.noop_out_timeout = 5\nnode.session.err_timeo.abort_timeout = 15\nnode.session.err_timeo.lu_reset_timeout = 30\nnode.session.err_timeo.tgt_reset_timeout = 30\nnode.session.initial_login_retry_max = 8\nnode.session.cmds_max = 128\nnode.session.queue_depth = 32\nnode.session.xmit_thread_priority = -20\nnode.session.iscsi.InitialR2T = No\nnode.session.iscsi.ImmediateData = Yes\nnode.session.iscsi.FirstBurstLength = 262144\nnode.session.iscsi.MaxBurstLength = 16776192\nnode.conn[0].iscsi.MaxRecvDataSegmentLength = 262144\nnode.conn[0].iscsi.MaxXmitDataSegmentLength = 0\ndiscovery.sendtargets.iscsi.MaxRecvDataSegmentLength = 32768\nnode.conn[0].iscsi.HeaderDigest = None\nnode.session.nr_sessions = 1\nnode.session.iscsi.FastAbort = Yes\n\n```\n\ninitiator重新发现并导入共享设备\n\n```shell\n[root@rhel6 nodes]# iscsiadm -m discovery -t st -p 172.25.30.10\n172.25.30.10:3260,1 iqn.2015-10.com.example-f30:vdb1-1g\n172.25.30.10:3260,1 iqn.2015-10.com.example-f30:vdb2-2g\n\n[root@rhel6 nodes]# iscsiadm -m node -T iqn.2015-10.com.example-f30:vdb1-1g -l\nLogging in to [iface: default, target: iqn.2015-10.com.example-f30:vdb1-1g, portal: 172.25.30.10,3260] (multiple)\nLogin to [iface: default, target: iqn.2015-10.com.example-f30:vdb1-1g, portal: 172.25.30.10,3260] successful.\n\n[root@rhel6 nodes]# iscsiadm -m node -T iqn.2015-10.com.example-f30:vdb2-2g -l\nLogging in to [iface: default, target: iqn.2015-10.com.example-f30:vdb2-2g, portal: 172.25.30.10,3260] (multiple)\nLogin to [iface: default, target: iqn.2015-10.com.example-f30:vdb2-2g, portal: 172.25.30.10,3260] successful.\n\n[root@rhel6 nodes]# ll /dev/sd*\nbrw-rw----. 1 root disk 8, 16 Jan 14 16:04 /dev/sdb\nbrw-rw----. 1 root disk 8, 32 Jan 14 16:04 /dev/sdc\n```","tags":["linux","iscsi","store"],"categories":["linux"]},{"title":"LVS-DR集群构建","url":"//2016/02/18/lvs-dr/","content":"设置端口标记的规则\n```shell\n#!/bin/bash\nVIP=$1\nIPTABLES=/sbin/iptables\n$IPTABLES -t mangle -A PREROUTING -p tcp -d $VIP --dport 80 -j MARK --set-mark 100\n$IPTABLES -t mangle -A PREROUTING -p tcp -d $VIP --dport 443 -j MARK --set-mark 100\n```\nARP防火墙设置\n```shell\n#!/bin/bash\nVIP=192.168.0.100\nRIP=192.168.0.x\nDGW=172.25.0.254\nDGWMAC=52:54:00:00:00:fe\narptables -F\narptables -A IN -d $VIP -j DROP\narptables -A OUT -s $VIP -j mangle --mangle-ip-s $RIP\n/sbin/ifconfig eth0:1 $VIP broadcast $VIP netmask 255.255.255.0 up\narp -s $DGW  $DGWMAC\n/sbin/route add default gw $DGW\n```\n 检测脚本\n```shell\n#!/bin/bash\n/usr/bin/links -dump 1 $1 >/dev/null 2>&1\nif [ 0 -eq $? ] ; then\n    echo ok\nelse\n    echo fail\nfi\n```\n主配置文件\n```conf\n[root@lvs-f30 ~]# cat /etc/sysconfig/ha/lvs.cf\nserial_no = 25\nprimary = 172.25.30.14\nprimary_private = 192.168.122.246\nservice = lvs\nbackup_active = 1\nbackup = 172.25.30.15\nbackup_private = 192.168.122.247\nheartbeat = 1\nheartbeat_port = 539\nkeepalive = 6\ndeadtime = 18\nnetwork = direct\ndebug_level = NONE\nmonitor_links = 0\nsyncdaemon = 0\nsyncd_iface = eth0\n\nvirtual http {\n     active = 1\n     address = 172.25.30.100 eth0:1\n     vip_nmask = 255.255.255.0\n     fwmark = 100\n     port = 80\n     send = \"GET / HTTP/1.0\\r\\n\\r\\n\"\n     expect = \"ok\"\n     use_regex = 0\n     send_program = \"/bin/testlink %h\"\n     load_monitor = none\n     scheduler = wlc\n     protocol = tcp\n     timeout = 6\n     reentry = 15\n     quiesce_server = 0\n\n     server node1 {\n         address = 192.168.122.224\n         active = 1\n         port = 80\n         weight = 1\n     }\n\n     server node2 {\n         address = 192.168.122.245\n         active = 1\n         port = 80\n         weight = 1\n     }\n}\n\nvirtual https {\n     active = 1\n     address = 172.25.30.100 eth0:1\n     vip_nmask = 255.255.255.0\n     fwmark = 100\n     port = 443\n     send = \"GET / HTTP/1.0\\r\\n\\r\\n\"\n     expect = \"HTTP\"\n     use_regex = 0\n     load_monitor = none\n     scheduler = wlc\n     protocol = tcp\n     timeout = 6\n     reentry = 15\n     quiesce_server = 0\n\n     server node1 {\n         address = 192.168.122.224\n         active = 1\n         port = 443\n         weight = 1\n     }\n\n     server node2 {\n         address = 192.168.122.245\n         active = 1\n         port = 443\n         weight = 1\n     }\n}\n```","tags":["cluster","lvs"],"categories":["lvs"]},{"title":"LVS-NAT集群构建","url":"//2016/02/18/lvs-nat-cluster/","content":"\n由于lvs基于内核实现的负载均衡技术，因此主要是在内核层面配置，软件层面需要配置的东西很少。我们是通过软件生成配置文件，再讲配置文件刷到内核中。\n\n```shell\n#安装组包\nyum groupinstall \"Load Balancer\"\n\n#启动图形化服务\n/etc/init.d/piranha-gui start\n\n#创建piranha用户密码\npiranha-passwd\n\n#通过图形化创建的配置文件存放在\n/etc/sysconfig/ha/lvs.cf\n\n#将配置刷到内核模块中ip_vs和ip_vs*\n/etc/init.d/pulse start\n```\n```shell\n#测试脚本/bin/testlink\n[root@lvs-f30 ~]# cat /bin/testlink\n#!/bin/bash\n/usr/bin/links -dump 1 $1 >/dev/null 2>&1\nif [ 0 -eq $? ] ; then\necho ok\nelse\necho fail\nfi\n```\n```conf\n[root@lvs-f30 ~]# cat /etc/sysconfig/ha/lvs.cf\nserial_no = 30\nprimary = 172.25.30.14\nprimary_private = 192.168.122.246\nservice = lvs\nbackup_active = 1\nbackup = 172.25.30.15\nbackup_private = 192.168.122.247\nheartbeat = 1\nheartbeat_port = 539\nkeepalive = 6\ndeadtime = 18\nnetwork = nat\nnat_router = 192.168.122.254 eth2\nnat_nmask = 255.255.255.255\ndebug_level = NONE\nmonitor_links = 0\nsyncdaemon = 0\nsyncd_iface = eth2\n\nvirtual http {\n\n    active = 1\n    address = 172.25.30.100 eth0:1\n    vip_nmask = 255.255.255.0\n    port = 80\n    send = \"GET / HTTP/1.0\\r\\n\\r\\n\"\n    expect = \"ok\"\n    use_regex = 0\n    send_program = \"/bin/testlink %h\"\n    load_monitor = none\n    scheduler = wlc\n    protocol = tcp\n    timeout = 6\n    reentry = 15\n    quiesce_server = 0\n\n    server node1 {\n\n        address = 192.168.122.224\n        active = 1\n        port = 80\n        weight = 1\n\n    }\n\n    server node2 {\n\n        address = 192.168.122.245\n        active = 1\n        port = 80\n        weight = 1\n\n    ​}\n\n}\n```\n[![lvsnat](/images/lvsnat.png)](lvsnat.png)","tags":["cluster","lvs"],"categories":["lvs"]},{"title":"LVS的三种路由技术","url":"//2016/02/18/lvs-three-mode/","content":"\n目前LVS的三种路由方式\n\n\n  1. Virtual Server via Network Address Translation（VS-NAT） \n  2. Virtual Server via Direct Routing（VS-DR） \n  3. Virtual Server via IP Tunneling（VS-TUN）\n\n","tags":["cluster","lvs"],"categories":["lvs"]},{"title":"rhcs搭建HA集群","url":"//2016/02/01/rhcs-ha-cluster/","content":"YUM配置HA集群(图形化)\n\nRHEL和Centos的光盘中自带有红帽的conga套件。将安装源指向光盘即可yum来安装，也可以将luci和ricci拷贝出来直接通过\"yum localinstall luci ricci\"来安装。其中luci是web界面的图形化配置工具，ricci为同步配置文件的工具。ricci运行需要用到ricci账户权限，安装ricci时设定下ricci密码。\n\n安装设置步骤\n\n1.所有节点进行环境初始化\n\n* 时间日期（date）\n* elinux\n* 防火墙（iptables）\n* 主机名（/etc/hosts和/etc/sysconfig/network）\n* 网卡网络(stop NetworkManager)\n* yum源\n\n2.在所有节点上安装ricci，在其中一台节点上安装luci，并配置ricci密码。\n```shell\n[root@localhost ~]# yum install luci ricci -y\n[root@localhost ~]# yum install ricci -y\n[root@localhost ~]# echo 123123123 | passwd --stdin ricci\n```\n3.启动luci进入web配置界面，可使用系统root账户登录luci\n```shell\n[root@localhost ~]# /etc/init.d/luci start\nStart luci...                       [ OK ]\nPoint your web browser to https://localhost.localdomain:8084 (or equivalent) to access luci\n\n[![ha](/images/ha.png)](http://www.xiemx.com/wp-content/uploads/2016/01/ha.png)\n\n[![hainstall](/images/hainstall.png)](http://www.xiemx.com/wp-content/uploads/2016/02/hainstall.png)\n\n在双机集群的基础上添加一个节点进去实现多机集群，这里使用非图形化安装\n安装 High Availability 组包\n\n[root@node3 ~]# yum groupinstall \"High Availability\"\n设置ricci用户密码:\n[root@node1 ~]# echo uplooking | passwd --stdin ricci\nChanging password for user ricci.\npasswd: all authentication tokens updated successfully.\n```\n修改配置文件/etc/cluster/cluster.conf并同步到所有结点上\n```xml\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n\n<cluster config_version=\"12\" name=\"f30Cluster\"> \n  <clusternodes> \n    <clusternode name=\"node1.xiemx.com\" nodeid=\"1\"> \n      <fence> \n        <method name=\"FenceMethod\"> \n          <device domain=\"node1\" name=\"Fence1\"/> \n        </method> \n      </fence> \n    </clusternode>  \n    <clusternode name=\"node2.xiemx.com\" nodeid=\"2\"> \n      <fence> \n        <method name=\"FenceMethod\"> \n          <device domain=\"node2\" name=\"Fence2\"/> \n        </method> \n      </fence> \n    </clusternode>  \n    <clusternode name=\"node3.xiemx.com\" nodeid=\"3\"> \n      <fence> \n        <method name=\"FenceMethod\"> \n          <device domain=\"node3\" name=\"Fence3\"/> \n        </method> \n      </fence> \n    </clusternode> \n  </clusternodes>  \n  <cman expected_votes=\"1\"/>  \n  <fencedevices> \n    <fencedevice agent=\"fence_xvm\" name=\"Fence1\"/>  \n    <fencedevice agent=\"fence_xvm\" name=\"Fence2\"/>  \n    <fencedevice agent=\"fence_xvm\" name=\"Fence3\"/> \n  </fencedevices>  \n  <rm> \n    <failoverdomains> \n      <failoverdomain name=\"Domain1\" restricted=\"1\"> \n        <failoverdomainnode name=\"node1.xiemx.com\"/>  \n        <failoverdomainnode name=\"node2.xiemx.com\"/>  \n        <failoverdomainnode name=\"node3.xiemx.com\"/> \n      </failoverdomain> \n    </failoverdomains>  \n    <resources> \n      <ip address=\"172.25.30.100/24\" sleeptime=\"10\"/>  \n      <script file=\"/etc/init.d/httpd\" name=\"httpd\"/> \n    </resources>  \n    <service domain=\"Domain1\" name=\"httpd\" recovery=\"restart\"> \n      <ip ref=\"172.25.30.100/24\"/>  \n      <script ref=\"httpd\"/> \n    </service> \n  </rm> \n</cluster>\n\n```\n同步配置:\n要保证所有节点的ricci服务已启动，且ricci账户都配置密码\n```shell\n[root@node1-f30 ~]# cman_tool version -r\nYou have not authenticated to the ricci daemon on node1-f30.example.com\nPassword:\nYou have not authenticated to the ricci daemon on node3-f30.example.com\nPassword:\nYou have not authenticated to the ricci daemon on node2-f30.example.com\nPassword:\n```\n启动cman，rgmanager，modcluster服务\n```shell\n[root@node3-f30 ~]# /etc/init.d/cman start\n\nStarting cluster:\nChecking if cluster has been disabled at boot...    [ OK ]\nChecking Network Manager...               [ OK ]\nGlobal setup...                     [ OK ]\nLoading kernel modules...                [ OK ]\nMounting configfs...                  [ OK ]\nStarting cman...                    [ OK ]\nWaiting for quorum...                  [ OK ]\nStarting fenced...                   [ OK ]\nStarting dlm_controld...                [ OK ]\nTuning DLM kernel config...               [ OK ]\nStarting gfs_controld...                [ OK ]\nUnfencing self...                    [ OK ]\nJoining fence domain...                 [ OK ]\n\n[root@node3-f30 ~]# /etc/init.d/rgmanager start\nStarting Cluster Service Manager:             [ OK ]\n\n[root@node3-f30 ~]# /etc/init.d/modclusterd start\nStarting Cluster Module - cluster monitor: Setting verbosity level to LogBasic\n[ OK ]\n```\n切换资源到node3节点上去运行，测试节点是否正常:\n```shell\n[root@node1-f30 ~]# clusvcadm -r httpd -m node3-f30.example.com\nTrying to relocate service:httpd to node3-f30.example.com...Success\nservice:httpd is now running on node3-f30.example.com\n\n同步fence_xvm.key:\n\n[root@node1-f30 ~]# scp /etc/cluster/fence_xvm.key 172.25.30.12:/etc/cluster/fence_xvm.key\nThe authenticity of host '172.25.30.12 (172.25.30.12)' can't be established.\nRSA key fingerprint is cf:7c:26:aa:4f:41:7b:21:5e:09:ce:8a:15:2c:97:32.\nAre you sure you want to continue connecting (yes/no)? yes\nWarning: Permanently added '172.25.30.12' (RSA) to the list of known hosts.\nroot@172.25.30.12's password:\nfence_xvm.key                100% 512   0.5KB/s  00:00\n```\nQdisk仲裁盘添加：\n\n在新的机器上共享一个磁盘出来加入到集群中,共享磁盘需要用到scsi-target-utils工具包，可yum安装获得\n```shell\n[root@rhel6 ~]# yum install scsi*\n\n将配置文件中的如下3行配置取消注释修改需要添加磁盘\n\n[root@rhel6 ~]# vi /etc/tgt/targets.conf\n<target iqn.2008-09.com.example:server.target1>\nbacking-store /dev/vdb1\n</target>\n\n集群的节点中安装包 iscsi-initiator-utils\n\n[root@node1-f30 ~]# yum install scsi*\n\n添加仲裁盘到本地,格式化仲裁盘\n\niscsiadm -m discovery -t st -p 172.25.30.13\niscsiadm -m node -l\nmkqdisk -c /dev/sda -l myqdisk\n\n[root@node3 ~]# iscsiadm -m discovery -t st -p 172.25.30.13\nStarting iscsid:                      [ OK ]\n172.25.30.13:3260,1 iqn.2008-09.com.example:server.target1\n\n[root@node3 ~]# iscsiadm -m node -l\nLogging in to [iface: default, target: iqn.2008-09.com.example:server.target1, portal: 172.25.30.13,3260] (multiple)\nLogin to [iface: default, target: iqn.2008-09.com.example:server.target1, portal: 172.25.30.13,3260] successful.\n\n[root@node3 ~]# mkqdisk -c /dev/sda -l myqdisk\nmkqdisk v3.0.12.1\nWriting new quorum disk label 'myqdisk' to /dev/sda.\nWARNING: About to destroy all data on /dev/sda; proceed [N/y] ? y\nWarning: Initializing previously initialized partition\nInitializing status block for node 1...\nInitializing status block for node 2...\n```\n在luci图形界面中添加仲裁到集群\n\n[![ha2](/images/ha2.png)](http://www.xiemx.com/wp-content/uploads/2016/01/ha2.png)\n也可以通过修改配置文件然后通过cman_tool version -r来同步到集群中的每个节点上\n\n配置文件:\n```xml\n<quorumd label=\"myqdisk\" min_score=\"1\">\n<heuristic interval=\"5\" program=\"ping 172.25.30.254 -c 1\" tko=\"2\"/>\n</quorumd>\n```\n同步配置文件时要注意修改版本号","tags":["cluster","rhcs"],"categories":["cluster"]},{"title":"mysql master-slave","url":"//2016/01/31/mysql-master-slave/","content":"Mysql复制（replication）是一个异步的复制，从一个Mysql 实例（Master）复制到另一个Mysql 实例（Slave）。实现整个主从复制，需要由Master服务器上的IO进程，和Slave服务器上的Sql进程和IO进程共从完成。要实现主从复制，首先必须打开Master端的binary log（bin-log）功能，因为整个 MySQL 复制过程实际上就是Slave从Master端获取相应的二进制日志，然后再在自己slave端完全顺序的执行日志中所记录的各种操作。 （二进制日志几乎记录了除select以外的所有针对数据库的sql操作语句）主从同步也称之为AB复制。\n\n基本过程：\n\n* Slave端的IO进程连接上Master，向Master请求指定日志文件的指定position后的日志内容；\n* Master接收到来自Slave的IO进程的请求后，负责复制的IO进程根据Slave的请求信息，读取相应日志内容，返回给Slave的IO进程。并将本次请求读取的bin-log文件名及位置一起返回给Slave端。\n* Slave的IO进程接收到信息后，将接收到的日志内容依次添加到Slave端的relay-log文件的最末端，并将读取到的Master端的bin-log的文件名和位置记录到master-info文件中，以便在下一次读取的时候能够清楚的告诉Master“我需要从某个bin-log的哪个位置开始往后的日志内容，请发给我”。\n* Slave的Sql进程检测到relay-log中新增加了内容后，会解析relay-log的内容成为在Master端真实执行时候的那些可执行的内容，并在自身执行。\n\n###数据库主从同步：\n```\n主：servera 172.25.30.10     \n从：serverb 172.25.30.11\n```\n\n数据库的主从同步（AB复制），要保证数据的一致性，首先将主上的数据库备份出来，在从机上恢复一下保证基础的数据相同。从上述过程中我们可以得知slave是通过获取master的二进制日志来重演来达到数据同步的，因此master需要开启二进制日志，slave来获取二进制日志时使用什么账户，账号具有什么权限需要在master端定义，salve需要知道谁是master，用什么账户、同步哪个二进制文件、从二进制文件的哪个position开始同步。另外还需要设置下主从的server id。\n\n由上推导出操作步骤\n\n1. master/slave先设置server-id，master在开启二进制日志功能log-bin\n```\n[root@localhost mysql]# cat /etc/my.cnf\n[mysqld]\nserver-id=1            server-id数字任选，但不可重复\ndatadir=/var/lib/mysql\nlog-bin=/var/log/mysql/binlog          slave不需要开启此功能\nsocket=/var/lib/mysql/mysql.sock\nuser=mysql\nlog-slow-queries=/var/log/mysql/slowlog\nsymbolic-links=0\n[mysqld_safe]\nlog-error=/var/log/mysqld.log\npid-file=/var/run/mysqld/mysqld.pid\n```\n2. master创建同步用户，并授权\n```sql\nmysql>grant   replication  slave on *.*  to xiemx@‘172.25.30.%’ identified by '123123';flush privileges;\n```\n3. slave端指定master信息\n```\nmysql>change master to master_host='172.25.30.10',master_user='xiemx',master_password='123123',master_log_file=\"binlog.000003\"，master_log_pos=245;\n\nmysql>slave start；\n```\n4. 查看salve状态信息；\n```sql\nmysql> show slave status \\G;           \\G表示将行和列翻转过来显示\n*************************** 1. row ***************************\nSlave_IO_State: Waiting for master to send event\nMaster_Host: 172.25.30.11\nMaster_User: xiemx\nMaster_Port: 4331\nConnect_Retry: 60\nMaster_Log_File: binlog.000003\nRead_Master_Log_Pos: 245\nRelay_Log_File: mysqld-relay-bin.000001           中继日志\nRelay_Log_Pos: 245\nSlave_IO_Running: Yes               slave的io是否正常运行\nSlave_SQL_Running: Yes              slave的sql是否正常运行，需要注意两个都为yes也不一定正常。但有一个no肯定不正常。我们需要在测试下读写。\nReplicate_Do_DB:\nReplicate_Ignore_DB: mysql\nReplicate_Do_Table:\nReplicate_Ignore_Table:\nReplicate_Wild_Do_Table: photo.%\nReplicate_Wild_Ignore_Table: mysql.%\nLast_Errno: 0\nLast_Error:\nSkip_Counter: 0\nExec_Master_Log_Pos: 13456620\nRelay_Log_Space: 36764898503\nUntil_Condition: None\nUntil_Log_File:\nUntil_Log_Pos: 0\nMaster_SSL_Allowed: No\nMaster_SSL_CA_File:\nMaster_SSL_CA_Path:\nMaster_SSL_Cert:\nMaster_SSL_Cipher:\nMaster_SSL_Key:\nSeconds_Behind_Master: 249904\n××××××××××××××××××××××××××××××××××××××××××××××××××××××××××\n```\n5. 测试主从是否同步\n\n由于slave是从master获取二进制日志来同步数据的，因此在slave上不应该有任何非查询类的sql语句执行，如果slave执行了sql语句操作了某个数据，当master操作这个数据时，salve通过二进制日志重演去操作时发生报错，会导致主从断开。如果主从断开我们就需要重新change master来建立主从。因此切记在单向主从同步的环境中，不可操作slave去执行非查询类操作。\n\n我们在master上创建一个xiemx数据库，查看下slave是否会自动创建出来。\n```\nmysql> create database  xiemx；show databases;\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| mysql              |\n| test               |\n| xiemx              |\n+--------------------+\n4 rows in set (0.00 sec)\n```\n如果如果在slave上show databases；结果和上面相同的则说明主从同步设置成功。","tags":["mysql"],"categories":["mysql"]},{"title":"mysql master-master","url":"//2016/01/31/mysql-master-master/","content":"\n原理同单向主从相同，都是salve获取master的二进制日志来重演数据，在互为主从的架构中，servera既是master又是slave，通用的serverb也是master和slave，servera回去同步serverb的数据，serverb也会去同步servera的操作。已达到数据的一致性。\n\n由原理可以得知：\n\n* servera需要开启二进制日志授权serverb来同步，servera需要change master指向serverb\n\n* serverb需要开启二进制日志授权servera来同步，serverb需要change master指向servera，另外serverb的数据是通过master来同步到中继日志中来重演生成的，因此需要在开启将中继日志写入二进制日志功能`log_slave_update=1`\n\n需要注意的是，在设置双向主从时，我们是先设置一条单向主从(servera-serverb)，在设置另一条单向主从(serverb-servera)。我们在设置完第一条单向主从(servera-serverb)成功时，slave(serverb)上是不能执行任何非查询语句的，因此第二条单向主从(serverb-servera)在serverb上设置servera的同步账户时grant授权语句需要通过(servera-serverb)这条单向主从的master(servera)来传递给salve(serverb)。\n\n#### 操作步骤\n\n1. 设置server-id，开启二进制日志，serverb需要开启中继日志写入二进制日志功能\n```shell\n[root@localhost mysql]# cat /etc/my.cnf\n[mysqld]\nserver-id=1            master/slave数字不可重复\ndatadir=/var/lib/mysql\nlog-bin=/var/log/mysql/binlog\nsocket=/var/lib/mysql/mysql.sock\nlog_slave_update=1              serverb上添加此项\nlog-slow-queries=/var/log/mysql/slowlog\nsymbolic-links=0\n[mysqld_safe]\nlog-error=/var/log/mysqld.log\npid-file=/var/run/mysqld/mysqld.pid\n```\n2. servera创建同步用户，并授权\n```sql\nmysql>grant   replication  slave on *.*  to xiemx1@‘172.25.30.%’ identified by '123123';flush privileges;\n```\n3. serverb指定master信息\n```\nmysql>change master to master_host='172.25.30.10',master_user='xiemx1',master_password='123123',master_log_file=\"binlog.000003\"，master_log_pos=245;\n\nmysql>slave start；\n\nmysql>show slave status\\G;查看io和sql是否yes，在测试主从是否生效，在保障主从生效的情况下执行下列操作。\n```\n4. servera上授权第二条单向的同步账户\n```\nmysql>grant   replication  slave on *.*  to xiemx2@‘172.25.30.%’ identified by '123123';flush privileges;\n```\n5. servera指定master信息\n```\nmysql>change master to master_host='172.25.30.11',master_user='xiemx2',master_password='123123',master_log_file=\"binlog.000001\"，master_log_pos=245;\n\nmysql>slave start；\n\nmysql>show slave status\\G;查看io和sql是否为yes\n```\n6. 在servera上创建一个库看serverb是否会自动生成，在serverb上创建库看servera是否会自动生成。\n如果都没有问题的话说明设置成功。\n`show master status\\G;` 可以查看当前的二进制日志文件和position号","tags":["mysql"],"categories":["mysql"]},{"title":"dns view功能","url":"//2016/01/31/dns-view/","content":"假设现在有一个网站服务器存放在一个双线或者多线机房内，这个服务器要提供一个web给全国的用户访问。现在用的用户使用的是电信的网，有的是联通的宽带，有的可能是长城铁通之类的isp，如果让这些用户都去访问我的某一个ip，会导致一部分用户存在跨isp延迟较高的情况，要解决这样的一个情况我们可以在多线机房内申请多个isp的ip绑定到机器上，然后修改dns的解析，让dns根据请求的来源分配给不同的ip。通俗的说就是电信用户走电信线路，联通用户走联通线路。这就要用到dns的一个view功能。\n\n具体配置如下：\n```\nview \"电信-view\"{\nmatch-clients { 服务器ip1 };#匹配这个ip/列表，符合则使用下面的服务器进行解析，否则跳出\nzone \"abc.com\" {\ntype master;\nfile \"dx.abc.com.zone\";该zone内服务器ipadd要与match-clients所属isp对应\n};\ninclude \"/etc/named.rfc1912.zones\";#需要包含该行。\n}\n\nview \"网通-view\"{\nmatch-clients { 服务器ip2 };#匹配这个ip/列表，符合则使用下面的服务器进行解析，否则跳出\nzone \"abc.com\" {\ntype master;\nfile \"wt.abc.com.zone\";该zone内服务器ipadd要与match-clients所属isp对应\n};\ninclude \"/etc/named.rfc1912.zones\";#需要包含该行。\n}\n\nview \"other-view\"{\nmatch-clients { any };#匹配这个ip，符合则使用下面的服务器进行解析，否则跳出\nzone \"abc.com\" {\ntype master;\nfile \"other.abc.com.zone\";\n};\ninclude \"/etc/named.rfc1912.zones\";#需要包含该行。\n}\n```\n这里只写出view部分的设置，其他部分设置可以参考dns服务器搭建内容。","tags":["dns","view"],"categories":["dns"]},{"title":"四种事务隔离级别","url":"//2016/01/31/transaction-isolation-level/","content":"数据库系统提供了四种事务隔离级别：\n  \nA.Serializable（串行化）：一个事务在执行过程中完全看不到其他事务对数据库所做的更新（事务执行的时候不允许别的事务并发执行。事务串行化执行，事务只能一个接着一个地执行，而不能并发执行。\n  \nB.Repeatable Read（可重复读）：一个事务在执行过程中可以看到其他事务已经提交的新插入的记录，但是不能看到其他其他事务对已有记录的更新。\n  \nC.Read Commited（读已提交数据）：一个事务在执行过程中可以看到其他事务已经提交的新插入的记录，而且能看到其他事务已经提交的对已有记录的更新。\n  \nD.Read Uncommitted（读未提交数据）：一个事务在执行过程中可以看到其他事务没有提交的新插入的记录，而且能看到其他事务没有提交的对已有记录的更新。\n","tags":["mysql"],"categories":["mysql"]},{"title":"mysql binlog恢复数据","url":"//2016/01/30/mysql-binlog-recovery-data/","content":"```shell\n[root@localhost mysql]# cat /etc/my.cnf\n[mysqld]\ndatadir=/var/lib/mysql\nlog-bin=/var/log/mysql/binlog   开启二进制日志\nsocket=/var/lib/mysql/mysql.sock\nuser=mysql\nlog-slow-queries=/var/log/mysql/slowlog\nsymbolic-links=0\n[mysqld_safe]\nlog-error=/var/log/mysqld.log\npid-file=/var/run/mysqld/mysqld.pid\n\n[root@localhost mysql]# cd /var/log/mysql\n[root@localhost mysql]# ls\nbinlog.000001  binlog.000002  binlog.000003  binlog.index             二进制生成的文件\n```\n查看数据库的日志相关设置：\n```sql\nmysql> SHOW  GLOBAL VARIABLES LIKE '%log%';\n+-----------------------------------------+----------------------------+\n| Variable_name                           | Value                      |\n+-----------------------------------------+----------------------------+\n| back_log                                | 50                         |\n| binlog_cache_size                       | 32768                      |\n| binlog_direct_non_transactional_updates | OFF                        |\n| binlog_format                           | STATEMENT                  |\n| expire_logs_days                        | 0                          |\n| general_log                             | OFF                        |\n| general_log_file                        | /var/run/mysqld/mysqld.log |\n| innodb_flush_log_at_trx_commit          | 1                          |\n| innodb_locks_unsafe_for_binlog          | OFF                        |\n| innodb_log_buffer_size                  | 1048576                    |\n| innodb_log_file_size                    | 5242880                    |\n| innodb_log_files_in_group               | 2                          |\n| innodb_log_group_home_dir               | ./                         |\n| innodb_mirrored_log_groups              | 1                          |\n| log                                     | OFF                        |\n| log_bin                                 | ON                         |    查看二进制日志是否开启\n| log_bin_trust_function_creators         | OFF                        |\n| log_bin_trust_routine_creators          | OFF                        |\n| log_error                               | /var/log/mysqld.log        |   错误日志\n| log_output                              | FILE                       |\n| log_queries_not_using_indexes           | OFF                        |\n| log_slave_updates                       | OFF                        |\n| log_slow_queries                        | ON                         |\n| log_warnings                            | 1                          |\n| max_binlog_cache_size                   | 18446744073709547520       |\n| max_binlog_size                         | 1073741824                 |\n| max_relay_log_size                      | 0                          |\n| relay_log                               |                            |   中继日志\n| relay_log_index                         |                            |\n| relay_log_info_file                     | relay-log.info             |\n| relay_log_purge                         | ON                         |\n| relay_log_space_limit                   | 0                          |\n| slow_query_log                          | ON                         |   慢查询日志\n| slow_query_log_file                     | /var/log/mysql/slowlog     |\n| sql_log_bin                             | ON                         |\n| sql_log_off                             | OFF                        |\n| sql_log_update                          | ON                         |\n| sync_binlog                             | 0                          |\n+-----------------------------------------+----------------------------+\n38 rows in set (0.00 sec)\n\n```\n\n查看二进制日志文件：二进制文件不是文本文档无法使用cat等于都文本的命令查看，需使用mysqlbinlog命令查看。\n```sql\n[root@localhost mysql]# mysqlbinlog binlog.000003\n/*!40019 SET @@session.max_insert_delayed_threads=0*/;\n/*!50003 SET @OLD_COMPLETION_TYPE=@@COMPLETION_TYPE,COMPLETION_TYPE=0*/;\nDELIMITER /*!*/;\n# at 4\n#160110 16:41:49 server id 1  end_log_pos 106     Start: binlog v 4, server v 5.1.73-log created 160110 16:41:49 at startup\n# Warning: this binlog is either in use or was not closed properly.\nROLLBACK/*!*/;\nBINLOG '\nTRmSVg8BAAAAZgAAAGoAAAABAAQANS4xLjczLWxvZwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAABNGZJWEzgNAAgAEgAEBAQEEgAAUwAEGggAAAAICAgC\n'/*!*/;\n# at 106                 position号标记\n#160110 17:26:15 server id 1  end_log_pos 191     Query    thread_id=3    exec_time=0    error_code=0    时间标记160110表示2016-01-10\nSET TIMESTAMP=1452417975/*!*/;\nSET @@session.pseudo_thread_id=3/*!*/;\nSET @@session.foreign_key_checks=1, @@session.sql_auto_is_null=1, @@session.unique_checks=1, @@session.autocommit=1/*!*/;\nSET @@session.sql_mode=0/*!*/;\nSET @@session.auto_increment_increment=1, @@session.auto_increment_offset=1/*!*/;\n/*!\\C latin1 *//*!*/;\nSET @@session.character_set_client=8,@@session.collation_connection=8,@@session.collation_server=8/*!*/;\nSET @@session.lc_time_names=0/*!*/;\nSET @@session.collation_database=DEFAULT/*!*/;\ncreate database xiemx     对数据库的操作记录\n/*!*/;\nDELIMITER ;\n# End of log file\nROLLBACK /* added by mysqlbinlog */;\n/*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/;\n```\n\n二进制恢复命令：\n```\n[root @server1 mysql ] #mysqlbinlog --start-date=\"2015-06-18 9:55:00\" --stop-date=\"2015-06-18 10:05:00\" /var/log/mysql/binlog.000006 | mysql -uroot -p\n\n从二进制日志binlog.000006的2015-06-18 9:55:00这个时间到2015-06-18 10:05:00之间的操作\n\n[root @server1 mysql ] #mysqlbinlog --stop-position=\"368312\" /var/log/mysql/binlog.000006 | mysql -u root -p  从二进制日志binlog.000006的开头恢复到368312这个position标记结束\n\n[root @server1 mysql ] #mysqlbinlog --start-position=\"368315\" /var/log/mysql/bin.000006 | mysql -u root -p     从二进制日志binlog.000006的368315这个position标记恢复到日志结束\n```\n在使用二进制恢复数据库时可以使用position标记和时间标记，假设我们在数据库操作中误执行了错误的操作我们可以用position和时间标记来跳 过错误的操作不执行，恢复出数据库。二进制日志恢复数据库是基于某个完整备份的基础上。因此在备份数据库时我们可以使用flush log命令来刷新二进制文件，记录下文件名。这样下次数据库出现问题我们就可以用手上的二进制文件和备份来恢复数据。\n\n ","tags":["mysql","binlog"],"categories":["mysql"]},{"title":"Tomcat多实例运行","url":"//2016/01/30/multi-tomcat-server/","content":"一个程序默认在系统中都是维护一个进程（或一个主进程多个子进程），这样的结构在进程出错终止时会导致该进程下所有的网站都会打不开。但tomcat我们可以通过调整启动时指定的tomcat程序位置来启动多个tomcat程序，分属不同的进程这样在某个进程终止时也不会影响到其他进程，不会导致其它网站无法访问。但一个端口只能被一个进程监听，当我们启动多进程就不能同时监听8080端口，我们可以依次监听8081，8082等，如果要实现直接输入域名访问可以在前段增加nginx来做代理即可。\n\n```shell\n[root@serverc ~]# /etc/init.d/tomcat stop\n[root@serverc ~]# cd /home/tomcat/\n[root@serverc tomcat]# mkdir tomcat1 tomcat2\n[root@serverc tomcat]# cd apache-tomcat-8.0.24/\n[root@serverc apache-tomcat-8.0.24]# cp -rp logs/ temp/ tomcat1.com/ work/ webapps/ conf/ ../tomcat1/\n[root@serverc apache-tomcat-8.0.24]# cp -rp logs/ temp/ tomcat2.com/ work/ webapps/ conf/ ../tomcat2/\n[root@serverc apache-tomcat-8.0.24]# rm -rf LICENSE NOTICE RELEASE-NOTES RUNNING.txt conf logs tomcat1.com tomcat2.com webapps\n[root@serverc apache-tomcat-8.0.24]# ls\nbin lib temp work\n[root@serverc apache-tomcat-8.0.24]# cd ../tomcat1/\n[root@serverc tomcat1]# vim conf/server.xml\n<Host name=\"www.tomcat1.com\" appBase=\"tomcat1.com\"\nunpackWARs=\"true\" autoDeploy=\"true\">\n<Valve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs\"\nprefix=\"localhost_access_log\" suffix=\".txt\"\npattern=\"%h %l %u %t &quot;%r&quot; %s %b\" />\n</Host>\n\n[root@serverc tomcat1]# cd ../tomcat2/\n[root@serverc tomcat2]# vim conf/server.xml\n<Host name=\"www.tomcat2.com\" appBase=\"tomcat2.com\"\nunpackWARs=\"true\" autoDeploy=\"true\">\n<Valve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs\"\nprefix=\"localhost_access_log\" suffix=\".txt\"\npattern=\"%h %l %u %t &quot;%r&quot; %s %b\" />\n</Host>\n=============================================================================\n<Connector port=\"8081\" protocol=\"HTTP/1.1\"\nconnectionTimeout=\"20000\"\nredirectPort=\"8443\" />\n=============================================================================\n<Connector port=\"8010\" protocol=\"AJP/1.3\" redirectPort=\"8443\" />\n\n[root@serverc tomcat2]# cd /etc/init.d/\n[root@serverc init.d]# mv tomcat tomcat1\n[root@serverc init.d]# vim tomcat1\nexport CATALINA_HOME=\"/home/tomcat/apache-tomcat-8.0.24/\"\nexport CATALINA_BASE=\"/home/tomcat/tomcat1\"\n\n[root@serverc init.d]# cp tomcat1 tomcat2\n[root@serverc init.d]# vim tomcat2\nexport CATALINA_HOME=\"/home/tomcat/apache-tomcat-8.0.24/\"\nexport CATALINA_BASE=\"/home/tomcat/tomcat2\"\n\n[root@serverc init.d]# /etc/init.d/tomcat1 start\n[root@serverc init.d]# /etc/init.d/tomcat2 start\n[root@serverc init.d]# netstat -ltunp | grep 8080\ntcp6 0 0 :::8080 :::* LISTEN 5749/jsvc.exec\n[root@serverc init.d]# netstat -ltunp | grep 8081\ntcp6 0 0 :::8081 :::* LISTEN 5782/jsvc.exec\n```","tags":["webserver","http","tomcat"],"categories":["tomcat"]},{"title":"Tomcat创建虚拟主机","url":"//2016/01/30/tomcat-virtual-host/","content":"1.修改tomcat服务的主配置文件。该文件位于tomcat主程序下conf目录中，文件名称为server.xml\n\n```shell\n  [root@serverc ~]# cd /home/tomcat/apache-tomcat-8.0.24/conf/ \n  [root@serverc conf]# ls -l server.xml \n  -rw------- 1 tomcat root 6458 Jul  2 04:23 server.xml\n```\n\n2.编辑该文件，添加虚拟主机。host字段为tomcat服务的虚拟主机配置字段。配置两个虚拟主机，`www.tomcat1.com`和`www.tomcat2.com`，两台虚拟主机有自己的`appbase`（即网页文件根目录，下图中使用的是相对路径表示虚拟主机网页根目录，该路径是相对于tomcat服务主程序所在目录来说，该路径现不存在，后续再创建）和相关日志前缀。注`name`字段为虚拟主机名，`appBase`为根目录，`uppackwars`为自动解压是否开启，`autoDeploy`为自动部署是否开启\n\n```xml\n<Host name=\"www.tomcat1.com\"  appBase=\"tomcat1.com\"\nunpackWARs=\"true\" autoDeploy=\"true\">\n<Valve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs\"\nprefix=\"localhost_access_log\" suffix=\".txt\"\npattern=\"%h %l %u %t \"%r\" %s %b\" />\n</Host>\n<Host name=\"www.tomcat2.com\"  appBase=\"tomcat2.com\"\nunpackWARs=\"true\" autoDeploy=\"true\">\n<Valve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs\"\nprefix=\"localhost_access_log\" suffix=\".txt\"\npattern=\"%h %l %u %t \"%r\" %s %b\" />\n</Host>\n```\n\n3.进入tomcat服务主程序所在目录，创建上述步骤中虚拟主机所指定的appBase，分别进入每一个虚拟主机的appBase目录下创建ROOT目录，在ROOT目录写每一个虚拟主机对应的首页文件。\n```shell\n  [root@serverc conf]# cd /home/tomcat/apache-tomcat-8.0.24/ \n  [root@serverc apache-tomcat-8.0.24]# mkdir tomcat1.com/ \n  [root@serverc apache-tomcat-8.0.24]# cd tomcat1.com/ \n  [root@serverc tomcat1.com]# mkdir ROOT \n  [root@serverc tomcat1.com]# echo tomcat1 > ROOT/index.html \n  [root@serverc tomcat1.com]# cd ../ \n  [root@serverc apache-tomcat-8.0.24]# mkdir tomcat2.com \n  [root@serverc apache-tomcat-8.0.24]# cd tomcat2.com \n  [root@serverc tomcat2.com]# mkdir ROOT \n  [root@serverc tomcat2.com]# echo tomcat2 > ROOT/index.html\n```\n\n4.重启tomcat服务\n\n```shell\n  [root@serverc tomcat2.com]# /etc/init.d/tomcat stop\n  [root@serverc tomcat2.com]# /etc/init.d/tomcat start\n```\n\n5.在client端机器上测试\n\n```shell\n  [root@workstation ~]# echo 172.25.41.10 www.tomcat1.com >> /etc/hosts \n  [root@workstation ~]# echo 172.25.41.10 www.tomcat2.com >> /etc/hosts\n  [root@workstation ~]# curl www.tomcat1.com:8080\n  tomcat1\n  [root@workstation ~]# curl www.tomcat2.com:8080\n  tomcat2\n```\n ","tags":["webserver","http","tomcat"],"categories":["tomcat"]},{"title":"Tomcat搭建web服务器","url":"//2016/01/24/tomcat-server/","content":"tomcat是一款处理jsp页面的web套件，可以处理html页面和jsp页面。由于java语言的跨平台性，软件只要解压后即可运行，但运行java需要在系统中安装jdk的java虚拟机。tomcat的软件包可以去tomcat官网http://tomcat.apache.org获得，jdk的软件包需要去oracle官网下载`http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html`。\n\n另外在部署tomcat虚拟机时我们一般会创建tomcat用户和组并将tomcat程序放在家目录下，家目录的权限为700，该目录只有tomcat用户有完整权限，其他用户无任何权限，可增加安全性。另一种部署软件的方法同我们平时安装源码软件相同，将软件放在/usr/lib/local目录下。这里就将软件部署在home/tomcat/目录下。\n\n### tomcat目录结构\n\n```shell\n  [root@serverc tomcat]# tar xf apache-tomcat-8.0.24.tar.gz  -C /home/tomcat/\n  [root@serverc tomcat]# cd /home/tomcat/apache-tomcat-8.0.24/\n  [root@serverc apache-tomcat-8.0.24]# ls -l\n  drwxr-xr-x 2 root root  4096 Dec 11 14:22 bin  #存放命令\n  drwxr-xr-x 2 root root  4096 Jul  2 04:23 conf  #存放配置文件\n  drwxr-xr-x 2 root root  4096 Dec 11 14:22 lib   #存放库文件，java的库文件后缀为jar\n  -rw-r--r-- 1 root root 57011 Jul  2 04:23 LICENSE\n  drwxr-xr-x 2 root root     6 Jul  2 04:20 logs   #存放相关日志\n  -rw-r--r-- 1 root root  1444 Jul  2 04:23 NOTICE\n  -rw-r--r-- 1 root root  6741 Jul  2 04:23 RELEASE-NOTES\n  -rw-r--r-- 1 root root 16204 Jul  2 04:23 RUNNING.txt\n  drwxr-xr-x 2 root root    29 Dec 11 14:22 temp   #存放临时文件\n  drwxr-xr-x 7 root root    76 Jul  2 04:21 webapps  #默认网站网页文件根目录\n  drwxr-xr-x 2 root root     6 Jul  2 04:20 work   #工作目录\n```\n\n### 启动tomcat\n```shell\nbin目录下有控制tomcat服务的启动关闭脚本，在启动jdk时我们声明下jdk的安装位置:\nexport  JAVA\\_HOME=\"/usr/java/jdk1.7.0\\_79/\" \n声明tomcat程序中命令和库文件所在位置\nexport CATALINA\\_HOME=\"/home/tomcat/apache-tomcat-8.0.24/\"\n声明tomcat程序中配置文件、网站根目录等所在位置\nexport CATALINA\\_BASE=\"/home/tomcat/apache-tomcat-8.0.24/\"\n启动tomcat服务的脚本名称为startup.sh。执行该脚本可启动tomcat服务。该服务默认监听tcp/8080端口。关闭脚本为shutdown.sh。\n\n  [root@serverc ~]# cd /home/tomcat/apache-tomcat-8.0.24/bin/\n  [root@serverc bin]# ./startup.sh\n  [root@serverc bin]# ps -ef | grep java\n  [root@serverc bin]# netstat -ltunp | grep 8080\n  tcp6       0      0 :::8080                 :::*                    LISTEN      1102/java\n\n```\n测试tomcat服务器\n\n在浏览器中输入地址访问8080，http://192.168.17.10:8080 来测试tomcat服务器是否正常\n\n![tomcat](/images/tomcat-300x84.png)","tags":["webserver","http","tomcat"],"categories":["tomcat"]},{"title":"Apache配置文件参数详解","url":"//2016/01/17/apache-configure/","content":"```conf\n[root@localhost ~]# grep -v '^#\\|^$\\|#' /etc/httpd/conf/httpd.conf\nServerTokens OS   当服务器响应主机头（header）信息时显示Apache的版本和\n\n操作系统名称\nServerRoot \"/etc/httpd\"   设置服务器的根目录\nPidFile run/httpd.pid     PID存放位置\nTimeout 60                若60秒后没有收到或送出任何数据就切断该连接\nKeepAlive Off             是否开启保持链接状态\nMaxKeepAliveRequests 100  在使用保持连接功能时，设置客户一次请求连接能\n\n响应文件的最大上限\nKeepAliveTimeout 15       在使用保持连接功能时，两个相邻的连接的时间间\n\n隔超过15秒，就切断连接\n<IfModule prefork.c>      设置使用Prefork MPM运行方式的参数，此运行方式\n\n是Red hat默认的方式\nStartServers       8      设置服务器启动时运行的进程数\nMinSpareServers    5      最小空闲进程数\nMaxSpareServers   20      最大空闲进程数\nServerLimit      256      最大的进程数\nMaxClients       256      最大的请求并发MaxClients=ServerLimit*进程的线\n\n程数\nMaxRequestsPerChild  4000 限制每个子进程在结束处理请求之前能处理的连接\n\n请求为1000\n</IfModule>\n<IfModule worker.c>      设置使用Worker MPM运行方式的参数\nStartServers         4\nMaxClients         300\nMinSpareThreads     25\nMaxSpareThreads     75\nThreadsPerChild     25\nMaxRequestsPerChild  0\n</IfModule>\nListen 80    监听端口\nLoadModule auth_basic_module modules/mod_auth_basic.so\nLoadModule auth_digest_module modules/mod_auth_digest.so\nLoadModule authn_file_module modules/mod_authn_file.so\nLoadModule authn_alias_module modules/mod_authn_alias.so\nLoadModule authn_anon_module modules/mod_authn_anon.so\nLoadModule authn_dbm_module modules/mod_authn_dbm.so\nLoadModule authn_default_module modules/mod_authn_default.so\nLoadModule authz_host_module modules/mod_authz_host.so\nLoadModule authz_user_module modules/mod_authz_user.so\nLoadModule authz_owner_module modules/mod_authz_owner.so\nLoadModule authz_groupfile_module modules/mod_authz_groupfile.so\nLoadModule authz_dbm_module modules/mod_authz_dbm.so\nLoadModule authz_default_module modules/mod_authz_default.so\nLoadModule ldap_module modules/mod_ldap.so\nLoadModule authnz_ldap_module modules/mod_authnz_ldap.so\nLoadModule include_module modules/mod_include.so\nLoadModule log_config_module modules/mod_log_config.so\nLoadModule logio_module modules/mod_logio.so\nLoadModule env_module modules/mod_env.so\nLoadModule ext_filter_module modules/mod_ext_filter.so\nLoadModule mime_magic_module modules/mod_mime_magic.so\nLoadModule expires_module modules/mod_expires.so\nLoadModule deflate_module modules/mod_deflate.so\nLoadModule headers_module modules/mod_headers.so\nLoadModule usertrack_module modules/mod_usertrack.so\nLoadModule setenvif_module modules/mod_setenvif.so\nLoadModule mime_module modules/mod_mime.so\nLoadModule dav_module modules/mod_dav.so\nLoadModule status_module modules/mod_status.so\nLoadModule autoindex_module modules/mod_autoindex.so\nLoadModule info_module modules/mod_info.so\nLoadModule dav_fs_module modules/mod_dav_fs.so\nLoadModule vhost_alias_module modules/mod_vhost_alias.so\nLoadModule negotiation_module modules/mod_negotiation.so\nLoadModule dir_module modules/mod_dir.so\nLoadModule actions_module modules/mod_actions.so\nLoadModule speling_module modules/mod_speling.so\nLoadModule userdir_module modules/mod_userdir.so\nLoadModule alias_module modules/mod_alias.so\nLoadModule substitute_module modules/mod_substitute.so\nLoadModule rewrite_module modules/mod_rewrite.so\nLoadModule proxy_module modules/mod_proxy.so\nLoadModule proxy_balancer_module modules/mod_proxy_balancer.so\nLoadModule proxy_ftp_module modules/mod_proxy_ftp.so\nLoadModule proxy_http_module modules/mod_proxy_http.so\nLoadModule proxy_ajp_module modules/mod_proxy_ajp.so\nLoadModule proxy_connect_module modules/mod_proxy_connect.so\nLoadModule cache_module modules/mod_cache.so\nLoadModule suexec_module modules/mod_suexec.so\nLoadModule disk_cache_module modules/mod_disk_cache.so\nLoadModule cgi_module modules/mod_cgi.so\nLoadModule version_module modules/mod_version.so\n以上为动态加载的模块\nInclude conf.d/*.conf                   将/etc/httpd/conf.d目录下所有以conf结尾的配置文件包含进来\nUser apache              进程运行账户\nGroup apache           进程运行群组\nServerAdmin root@localhost             Apache服务器管理员的E_mail地址\nUseCanonicalName Off\nDocumentRoot \"/var/www/html\"         设置网站根目录\n<Directory />  设置apache服务器根的访问权限\nOptions FollowSymLinks             允许符号链接\nAllowOverride None               禁止读取.htaccess配置文件的内容\n</Directory>\n<Directory \"/var/www/html\"> 设置Apache根的访问权限\nOptions Indexes FollowSymLinks                      FollowSymLinks允许符号链接，Indexes无首页时罗列出目录下文件\nAllowOverride None\nOrder allow,deny            先执行Allow访问规则，在执行Deny访问规则\nAllow from all                 allow访问规则，允许所有链接\n</Directory>\n<IfModule mod_userdir.c>\nUserDir disabled\n</IfModule>\nDirectoryIndex index.html index.html.var           网站首页文件名称定义\nAccessFileName .htaccess            访问控制文件的名称\n<Files ~ \"^\\.ht\">         关于.ht开头文件的权限控制\nOrder allow,deny\nDeny from all\nSatisfy All            访问.ht开头文件需要满足两种访问控制都允许\n</Files>\nTypesConfig /etc/mime.types            MIME对应格式的配置文件的存放位置\nDefaultType text/plain                        默认的MIME文件类型为纯文本或HTML文件\n<IfModule mod_mime_magic.c>    当mod_mime_magic.c模块被加载时，指定magic信息码配置文件的存放位置\nMIMEMagicFile conf/magic\n</IfModule>\nHostnameLookups Off          关闭记录访问web客户的hostname功能，只记录IP\nErrorLog logs/error_log        错误日志存放位置\nLogLevel warn                        记录日志的等级\nLogFormat \"%h %l %u %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\" \\\"%{User-Agent}i\n\n\\\"\" combined\nLogFormat \"%h %l %u %t \\\"%r\\\" %>s %b\" common\nLogFormat \"%{Referer}i -> %U\" referer\nLogFormat \"%{User-agent}i\" agent\n\n以上为记录日志的四种格式\nCustomLog logs/access_log combined             访问日志的纪录格式为combined（混合型），并指定访问日志存放位置\nServerSignature On                             apache自己产生的页面中使用apache服务器版本的签名\nAlias /icons/ \"/var/www/icons/\"   定义别名/icons/\n<Directory \"/var/www/icons\">\nOptions Indexes MultiViews FollowSymLinks              MultiViews 使用内容协商来决定被发送的网页的性质\nAllowOverride None\nOrder allow,deny\nAllow from all\n</Directory>\n<IfModule mod_dav_fs.c>   DAV加锁数据库文件的存放位置\nDAVLockDB /var/lib/dav/lockdb\n</IfModule>\nScriptAlias /cgi-bin/ \"/var/www/cgi-bin/\"  设置CGI目录的访问别名\n<Directory \"/var/www/cgi-bin\">\nAllowOverride None\nOptions None\nOrder allow,deny\nAllow from all\n</Directory>\nIndexOptions FancyIndexing VersionSort NameWidth=* HTMLTable       未找到首页文件时生存目录列表的方式，FancyIndexing 对每种类型的文件前加上一个小图标以示区别，VersionSort 对同一个软件的多个版本进行排序，NameWidth=* 文件名字段自动适应当前目录下的最长文件名。生成小图标的时候使用AddIcon选项。\nCharset=UTF-8           字符编码\nAddIconByEncoding (CMP,/icons/compressed.gif) x-compress x-gzip\nAddIconByType (TXT,/icons/text.gif) text/*\nAddIconByType (IMG,/icons/image2.gif) image/*\nAddIconByType (SND,/icons/sound2.gif) audio/*\nAddIconByType (VID,/icons/movie.gif) video/*\nAddIcon /icons/binary.gif .bin .exe\nAddIcon /icons/binhex.gif .hqx\nAddIcon /icons/tar.gif .tar\nAddIcon /icons/world2.gif .wrl .wrl.gz .vrml .vrm .iv\nAddIcon /icons/compressed.gif .Z .z .tgz .gz .zip\nAddIcon /icons/a.gif .ps .ai .eps\nAddIcon /icons/layout.gif .html .shtml .htm .pdf\nAddIcon /icons/text.gif .txt\nAddIcon /icons/c.gif .c\nAddIcon /icons/p.gif .pl .py\nAddIcon /icons/f.gif .for\nAddIcon /icons/dvi.gif .dvi\nAddIcon /icons/uuencoded.gif .uu\nAddIcon /icons/script.gif .conf .sh .shar .csh .ksh .tcl\nAddIcon /icons/tex.gif .tex\nAddIcon /icons/bomb.gif /core\nAddIcon /icons/back.gif ..\nAddIcon /icons/hand.right.gif README\nAddIcon /icons/folder.gif ^^DIRECTORY^^\nAddIcon /icons/blank.gif ^^BLANKICON^^\nDefaultIcon /icons/unknown.gif      位置类型文件使用此图像作为图标\nReadmeName README.html          当服务器自动列出目录列表时，在所生成的页面之后显示readme.html的内容\nHeaderName HEADER.html            当服务器自动列出目录列表时，在所生成的页面之前显示header.html的内容\nAddLanguage ca .ca\nAddLanguage cs .cz .cs\nAddLanguage da .dk\nAddLanguage de .de\nAddLanguage el .el\nAddLanguage en .en\nAddLanguage eo .eo\nAddLanguage es .es\nAddLanguage et .et\nAddLanguage fr .fr\nAddLanguage he .he\nAddLanguage hr .hr\nAddLanguage it .it\nAddLanguage ja .ja\nAddLanguage ko .ko\nAddLanguage ltz .ltz\nAddLanguage nl .nl\nAddLanguage nn .nn\nAddLanguage no .no\nAddLanguage pl .po\nAddLanguage pt .pt\nAddLanguage pt-BR .pt-br\nAddLanguage ru .ru\nAddLanguage sv .sv\nAddLanguage zh-CN .zh-cn\nAddLanguage zh-TW .zh-tw\n以上为设置网页内容的语言种类\nLanguagePriority en ca cs da de el eo es et fr he hr it ja ko ltz nl nn\n\nno pl pt pt-BR ru sv zh-CN zh-TW 生效的先后顺序\nForceLanguagePriority Prefer Fallback\nPrefer 当有多种语言可以匹配时，使用LanguagePriority 列表的第一项\nFallback 当没有语言可以匹配时，使用LanguagePriority 列表的第一项\nAddDefaultCharset UTF-8   设置默认字符集\nAddType application/x-compress .Z\nAddType application/x-gzip .gz .tgz\nAddType application/x-x509-ca-cert .crt\nAddType application/x-pkcs7-crl    .crl\n以上为添加一些mime类型\nAddHandler type-map var 设置apcche对某些扩展名的处理方式\nAddType text/html .shtml\nAddOutputFilter INCLUDES .shtml  使用过滤器执行SSI\nAlias /error/ \"/var/www/error/\"  定义错误页面别名\n<IfModule mod_negotiation.c>\n<IfModule mod_include.c>\n<Directory \"/var/www/error\"> 错误页的权限定义\nAllowOverride None\nOptions IncludesNoExec\nAddOutputFilter Includes html\nAddHandler type-map var\nOrder allow,deny\nAllow from all\nLanguagePriority en es de fr\nForceLanguagePriority Prefer Fallback\n</Directory>\n</IfModule>\n</IfModule>\n以下为设置浏览器匹配\nBrowserMatch \"Mozilla/2\" nokeepalive\nBrowserMatch \"MSIE 4\\.0b2;\" nokeepalive downgrade-1.0 force-response-\n\n1.0\nBrowserMatch \"RealPlayer 4\\.0\" force-response-1.0\nBrowserMatch \"Java/1\\.0\" force-response-1.0\nBrowserMatch \"JDK/1\\.0\" force-response-1.0\nBrowserMatch \"Microsoft Data Access Internet Publishing Provider\"\n\nredirect-carefully\nBrowserMatch \"MS FrontPage\" redirect-carefully\nBrowserMatch \"^WebDrive\" redirect-carefully\nBrowserMatch \"^WebDAVFS/1.[0123]\" redirect-carefully\nBrowserMatch \"^gnome-vfs/1.0\" redirect-carefully\nBrowserMatch \"^XML Spy\" redirect-carefully\nBrowserMatch \"^Dreamweaver-WebDAV-SCM1\" redirect-carefully\n```","tags":["apache","webserver"],"categories":["apache"]},{"title":"Apache运行模式prefork和worker","url":"//2016/01/17/apache-running-mode/","content":"#### 两种模式\n* prefork模式\n多进程模式。将MaxClients设置为一个足够大的数值以处理潜在的请求高峰，同时又不能太大，以致需要使用的内存超出物理内存的大小。\n\n* worker模式\n多线程多进程模式。由于使用线程来处理请求，可以应对高并发的访问，而系统资源的开销小于基于进程的MPM。但是，它也使用了多进程，每个进程又有多个线程，以获得基于进程的MPM的稳定性。控制每个子进程允许建立的线程数的ThreadsPerChild指令，和控制允许建立的总线程数的MaxClients指令。\n\n#### prefork和worker模式的切换\n\n[root@localhost ~]# rpm -ql httpd | grep worker\n/usr/sbin/httpd.worker\n\n默认的工作模式为prefork当要切换模式时修改/usr/sbin/下的2个启动脚本名称即可\n\n1. 将当前的prefork模式启动文件改名\n```\nmv httpd httpd.prefork\n```\n2. 将worker模式的启动文件改名\n```\nmv httpd.worker httpd\n```\n3. 修改Apache配置文件\n```\nvim /etc/httpd/conf/httpd.conf\n找到里边的如下一段，可适当修改负载等参数：\n<IfModule prefork.c>\nStartServers 8\nMinSpareServers 5\nMaxSpareServers 20\nServerLimit 256\nMaxClients 256\nMaxRequestsPerChild 4000\n</IfModule>\n```\n处于稳定性和安全性考虑，不建议更换apache2的运行方式，使用系统默认prefork即可。另外很多php模块不能工作在worker模式下，例如redhat linux自带的php也不能支持线程安全。所以最好不要切换工作模式。\n\n#### prefork和worker模式的比较\nprefork模式使用多个子进程，每个子进程只有一个线程。每个进程在某个确定的时间只能维持一个连接。在大多数平台上，Prefork MPM在效率上要比Worker MPM要高，但是内存使用大得多。prefork的无线程设计在某些情况下将比worker更有优势：它可以使用那些没有处理好线程安全的第三方模块，并 且对于那些线程调试困难的平台而言，它也更容易调试一些。\n\nworker模式使用多个子进程，每个子进程有多个线程。每个线程在某个确定的时 间只能维持一个连接。通常来说，在一个高流量的HTTP服务器上，Worker MPM是个比较好的选择，因为Worker MPM的内存使用比Prefork MPM要低得多。但worker MPM也由不完善的地方，如果一个线程崩溃，整个进程就会连同其所有线程一起\"死掉\".由于线程共享内存空间，所以一个程序在运行时必须被系统识别为\"每 个线程都是安全的\"。\n\n总的来说，prefork方式速度要稍高于worker，然而它需要的cpu和memory资源也稍多于woker，但在高并发环境下woker优于prefork。\n\n#### prefork模式配置详解\n```\n<IfModule prefork.c>\nServerLimit 256\nStartServers 5\nMinSpareServers 8\nMaxSpareServers 10\nMaxClients 256\nMaxRequestsPerChild 4000\n</IfModule>\nServerLimit\n默认的MaxClient最大是256个线程,如果想设置更大的值，就的加上ServerLimit这个参数。20000是ServerLimit这个参数的最大值。如果需要更大，则必须编译apache,此前都是不需要重新编译Apache。\n生效前提：必须放在其他指令的前面\n\nStartServers\n指定服务器启动时建立的子进程数量，prefork默认为8。\n\nMinSpareServers\n指定空闲子进程的最小数量，默认为5。如果当前空闲子进程数少于MinSpareServers ，那么Apache将以最大每秒一个的速度产生新的子进程。此参数不要设的太大。\n\nMaxSpareServers\n设置空闲子进程的最大数量，默认为10。如果当前有超过MaxSpareServers数量的空闲子进程，那么父进程将杀死多余的子进程。此参数不要设的 太大。如果你将该指令的值设置为比MinSpareServers小，Apache将会自动将其修改成\"MinSpareServers+1\"。\n\nMaxClients\n限定同一时间客户端最大接入请求的数量(单个进程并发线程数)，默认为256。任何超过MaxClients限制的请求都将进入等候队列,一旦一个链接被释放，队列中的请求将得到服务。要增大这个值，你必须同时增大ServerLimit。\n\nMaxRequestsPerChild\n每个子进程在其生存期内允许伺服的最大请求数量，默认为10000.到达MaxRequestsPerChild的限制后，子进程将会结束。如果 MaxRequestsPerChild为\"0\"，子进程将永远不会结束。将MaxRequestsPerChild设置成非零值有两个好处：\n1.可以防止(偶然的)内存泄漏无限进行，从而耗尽内存。\n2.给进程一个有限寿命，从而有助于当服务器负载减轻的时候减少活动进程的数量。\n\nworker模式配置详解\n<IfModule worker.c>\nStartServers 4\nMaxClients 150\nMinSpareThreads 25\nMaxSpareThreads 75\nThreadsPerChild 25\nMaxRequestsPerChild 0\n</IfModule>\n\nStartServers\n服务器启动时建立的子进程数，默认值是\"3\"。\n\nMaxClients\n允许同时伺服的最大接入请求数量(最大线程数量)。任何超过MaxClients限制的请求都将进入等候队列。默认值 是\"400\",16(ServerLimit)乘以25(ThreadsPerChild)的结果。因此要增加MaxClients的时候，你必须同时增 加ServerLimit的值。\n\nMinSpareThreads\n最小空闲线程数,默认值是\"75\"。这个MPM将基于整个服务器监视空闲线程数。如果服务器中总的空闲线程数太少，子进程将产生新的空闲线程。\n\nMaxSpareThreads\n设置最大空闲线程数。默认值是\"250\"。这个MPM将基于整个服务器监视空闲线程数。如果服务器中总的空闲线程数太多，子进程将杀死多余的空闲线程。 MaxSpareThreads的取值范围是有限制的。Apache将按照如下限制自动修正你设置的值：worker要求其大于等于 MinSpareThreads加上ThreadsPerChild的和。\n\nThreadsPerChild\n每个子进程建立的常驻的执行线程数。默认值是25。子进程在启动时建立这些线程后就不再建立新的线程了。\n\nMaxRequestsPerChild\n设置每个子进程在其生存期内允许伺服的最大请求数量。到达MaxRequestsPerChild的限制后，子进程将会结束。如果MaxRequestsPerChild为\"0\"，子进程将永远不会结束。将MaxRequestsPerChild设置成非零值有两个好处：\n1.可以防止(偶然的)内存泄漏无限进行，从而耗尽内存。\n2.给进程一个有限寿命，从而有助于当服务器负载减轻的时候减少活动进程的数量。\n注意对于KeepAlive链接，只有第一个请求会被计数。事实上，它改变了每个子进程限制最大链接数量的行为。\n```","categories":["apache"]},{"title":"Apache搭建web服务器","url":"//2016/01/17/apache-webserver/","content":"linux下搭建web服务器的套件很多，基本使用的还是apache、nginx、tomcat。这三种服务器都有各自的优缺点，在不同的场景下应根据实际需求选用。linux下apache搭建web服务器配置起来较为简单，只需在系统中安装httpd软件包即可。但要实现复杂环境下的应用，则需要配置更多的功能。\n\n1. 安装软件包\n```shell\nyum install httpd -y\n\n[root@localhost ~]# rpm -ql httpd | grep 'httpd\\.conf'\n/etc/httpd/conf/httpd.conf    apache服务的主配置文件\n\n[root@localhost ~]# grep -v '^#\\|^$\\|#' /etc/httpd/conf/httpd.conf\nListen 80     监听端口\nInclude conf.d/*.conf    装载附加配置文件\nUser apache\nGroup apache  运行用户和组\nDocumentRoot \"/var/www/html\"    默认主目录\nDirectoryIndex index.html index.html.var  默认首页\n```\n\n2. 在主目录/var/www/html下创建一个测试页\n\n```shell\n[root@localhost ~]# echo \"test page\" > /var/www/html/index.html\n[root@localhost ~]# /etc/init.d/httpd start\nStarting httpd:                        [OK]\n```\n3. 使用浏览器测试访问\n\n[![apachetest](/images/apachetest-300x89.png)](/images/apachetest-300x89.png)","tags":["apache","webserver"],"categories":["apache"]},{"title":"Apache的访问控制","url":"//2016/01/17/apache-auth/","content":"Apache的访问控制两种，一是客户端限制，一是用户验证机制。\n\n* 客户端限制：\n```\n<Directory /some/dir>\norder allow,deny\ndeny from all\n</Directory >\n```\n \n\n这就是一个目录限制，他限制所有IP对这个目录的访问。\n\n* 用户验证机制：\n```\n<Directory /some/dir>\nAuthType Basic\nAuthName \"My Auth File\"\nAuthUserFile /some/file/path\nRequire valid-user\n</Directory >\n```\n这就是一个用户验证机制，他要求用户给出用户名和密码才能访问目录下的内容。","tags":["apache","webserver"],"categories":["apache"]},{"title":"PXE批量在线安装操作系统","url":"//2016/01/17/pxe-kickstart/","content":"pxe 通过网络方式安装部署\n\n* dhcp:动态管理协议\n* 网卡支持tftp(文件下载使用,不支持验证，安全系数低,只下载些基础文件）使用http下载ks.conf(应答文件)/rpm包\n\n### 流程\n```\n客户端向服务端申请下载 dhcp.client  服务端返回信息客户端下载，并分配给客户端ip地址\n客户端拿到ip地址后去tftp服务端下载pxelinux.0 并在客户端安装pxelinux.0(引导安装程序）\n客户端去下载配置文件 pxelinux.cfg/default 安装后出现安装标签 指引安装（例 install foution0)\n选择安装标签后 到服务端下载unlinuz(微型运行平台） initrd(基本的命令 程序） ks文件路径（自动安装使用）\n然后出现安装界面 开始交互式安装\n如果自动安装去找http服务器，下载应答ks.cfg,下载rpm包 自动安装后会再此运行ks.cfg脚本\n```\n \n\n### 原理图：\n```\nclients：dhcp client\n------------------------------> dhcp server\n<------------------------------\n分配地址池ip，告知tftp的地址\n请求pxelinux.0文件\n------------------------------> tftp server\n<------------------------------\npxelinux.0\n引导界面\n-------------------------------> tftp server\n<------------------------------\npxelinux.cfg/default\n安装界面\n-------------------------------> tftp server\n<-------------------------------\nvmlinuxz initrd ks文件路径\n\n-------------------------------> http server\n<------------------------------\nks.cfg rpm包\n<-------------------------------\n自动执行脚本\n安装完成\n```\n### 操作步骤\n\n依据以上原理图可以得知PXE过程需要用到的文件有pxelinux.o、default、ks.cfg、vmlinuxz、initrd.img。需要用到的协议tftp、dhcp、http。\n\npxelinux.0 来源syslinux软件包\nks.cfg 来源kickstart，也可以通过安装system-config-kickstart来图形化配置\nvmlinuz 来源于iso镜像文件\ndefault 需要手工配置\ninitrd.img 来源iso镜像文件\n\n1.安装软件\n```\nyum install httpd dhcp tftp-server -y\n```\n2.配置dhcp\n```\n[root@linux]# cat /etc/dhcpd.conf\nddns-update-style interim;\nallow booting; #定义能够PXE启动\nallow bootp; #定义支持bootp\nnext-server 192.168.0.1; #TFTP Server的IP地址\nfilename \"pxelinux.0\"; #bootstrap 文件(NBP)\n\ndefault-lease-time 1800;\nmax-lease-time 7200;\nping-check true;\noption domain-name-servers 192.168.0.1;\n\nsubnet 192.168.0.0 netmask 255.255.255.0\n{\nrange 192.168.0.128 192.168.0.220;\noption routers 192.168.0.1;\noption broadcast-address 192.168.0.255;\n}\n```\n3.启动tftp\n```\n[root@linux]# cat /etc/xinetd.d/tftp\nservice tftp\n{\nsocket_type = dgram\nprotocol = udp\nwait = yes\nuser = root\nserver = /usr/sbin/in.tftpd\nserver_args = -s /var/lib/tftpboot   tftp服务根目录\ndisable = no   是否关闭tftp服务\nper_source = 11\ncps = 100 2\nflags = IPv4\n}\n\n重启xinetd服务\n```\n4.获取pxelinux.0文件\n```\n[root@linux]# rpm -ql syslinux | grep \"pxelinux.0\"\n/usr/lib/syslinux/pxelinux.0\n[root@linux]# cp /usr/lib/syslinux/pxelinux.0 /var/lib/tftpboot/\n```\n5.创建/var/lib/tftp/pxelinux.cfg目录、创建default文件\n```\n将 boot.msg initrd.img splash.png vesamenu.c32 vmlinuz 复制到/var/lib/tftpboot/\n\n[root@linux]# cat /tftpboot/pxelinux.cfg/default\n\ndefault vesamenu.c32   显示图形化引导界面，也可以写成default linux文本化界面\ntimeout 60     等待操作时间\ndisplay boot.msg    显示一些引导信息\n\nmenu background splash.jpg   背景图片\nmenu title Welcome to pxe Setup!   界面标题\n\nlabel 1\nmenu label Boot from ^local drive   安装1选项标题\nmenu default    60s无操作默认启动此选项\nlocalboot 0xffff\n\nlabel 2\nmenu label Install linux\nipappend 2\nkernel vmlinuz\nappend initrd=initrd.img ks=http://172.25.16.9/ks.cfg\n```\n6.将iso文件展开到http目录下\n\n7.生成ks.cfg文件\n\n  `system-config-kickstart`安装此图形化工具，生成自动应答脚本。也可以拷贝已安装系统中自动生成的脚本。\n\n8.关闭防火墙，selinux。测试http、tftp等服务都正常启动且文件都已放置可正常访问下载。","tags":["linux","pxe","kickstart"],"categories":["linux"]},{"title":"DNS服务器搭建","url":"//2015/11/30/dns-server/","content":"DNS即域名系统，用于将IP地址和域名之间的翻译。linux中搭建dns服务器一般使用BIND软件来实现。我们在网络中常常会有将域名转化成IP，和将IP转化成域名这两种需求，这两种需求对应了dns解析中的正向和反向解析。\n\n* 正向解析：将域名解析成IP地址\n\n* 反向解析：将ip地址解析成域名\n\n\n`完全合格域名（FQDN）`：点结尾的域名，例如\"www.xiemx.com.\" 就是一个完全合格域名。在一般的网络应用中，我们可以省略完全合格域名最右侧的点，但DNS对这个点不能随便省略。因为这个点代表了DNS的根，有了这个点，完全合格域名就可以表达为一个绝对路径，例如\"www.xiemx.com.\"就可以表示为DNS根下的com子域下xiemx.com域中一个名为www的主机。如果DNS发现一个域名不是以点结尾的完全合格域名，就会把这个域名加上当前的区域名称作为后缀，让其满足完全合格域名的形式需求。\n\n查询方法\n\n1. 递归查询:\n一般客户机和服务器之间属递归查询，即当客户机向DNS服务器发出请求后,若DNS服务器本身不能解析,则会向另外的DNS服务器发出查询请求，得到结果后转交给客户机；\n2. 迭代查询(反复查询):\n一般DNS服务器之间属迭代查询，如：若DNS2不能响应DNS1的请求，则它会将DNS3的IP给DNS1，以便其再向DNS3发出请求；DNS3不能响应DNS1的请求，则它会将DNS4的IP给DNS1。如此反复。\n\n记录类型\n\n* A记录：域名到IP地址的映射。\n\n* CNAME记录：也叫别名记录，用于定义A记录的别名\n\n* MX记录：邮件交换记录（MX）邮件服务器发送邮件时定位邮件服务器，可以有多个MX记录，但存在优先级差别\n\n* NS记录：用于标识区域的DNS服务器，即是说负责此DNS区域的权威名称服务器，用哪一台DNS服务器来解析该区域。\n\n* PTR记录：是IP地址到DNS名称的映射，用于反向解析\n\n \n\n安装软件：\n```shell\nyum  -y  install  bind\n\n我们使用的是软件版本是9.8.2，不同的版本配置的语法会有区别。\n\n[root@localhost ~]# yum info bind\nLoaded plugins: product-id, refresh-packagekit, security, subscription-manager\nThis system is not registered to Red Hat Subscription Management. You can use subscription-manager to register.\nlocalyumdata | 3.9 kB 00:00 ...\nInstalled Packages\nName : bind\nArch : x86_64\nEpoch : 32\nVersion : 9.8.2\nRelease : 0.17.rc1.el6_4.6\n```\n配置文件：\n```\n/var/run/named/  ——进程pid文件\n/var/named/  ——数据文件目录\n/etc/named.conf ——主配置文件\n/etc/named.rfc1912.zones ——辅配置文件\n/etc/rc.d/init.d/named ——进程控制脚本文件\n\n[root@localhost ~]# cat /etc/named.conf\noptions {\n  listen-on port 53 { any; };——设置监听那些ipv4地址的53端口，默认为127.0.0.1。设置any允许所有，也可以；号间隔一个个罗列IP。\n  listen-on-v6 port 53 { any; };——ipv6\n  directory \"/var/named\";——数据目录\n  dump-file \"/var/named/data/cache_dump.db\";\n  statistics-file \"/var/named/data/named_stats.txt\";\n  memstatistics-file \"/var/named/data/named_mem_stats.txt\";\n  allow-query { any; };——允许哪些IP来查询，默认为localhost。any为允许所有\n  recursion yes;——是否允许递归\n\n  dnssec-enable yes; ——是否启用DNSSEC支持，DNS安全扩展（DNSSEC）提供了验证DNS数据由效性的系统\n  dnssec-validation yes;\n  dnssec-lookaside auto;\n\n  /* Path to ISC DLV key */\n  bindkeys-file \"/etc/named.iscdlv.key\";\n\n  managed-keys-directory \"/var/named/dynamic\";\n};\n\nlogging {\n  channel default_debug {\n    file \"data/named.run\";\n    severity dynamic;\n  };\n};\n\nzone \".\" IN {\n  type hint;\n  file \"named.ca\";\n};  ——定义根域服务器\n\ninclude \"/etc/named.rfc1912.zones\"; ——装载辅配置文件，我们一般将需要解析的域名配置写在这个文件中。\ninclude \"/etc/named.root.key\";\n```\n```shell\n[root@localhost ~]# cat /etc/named.rfc1912.zones\n\nzone \"xiemx.com\" IN {                    ——\" \"内写要解析的域名\n  type master;                                      ——设置类型为master主类型，还有其它的hint根类型、slave从类型。当2台dns做主辅同步时会设置辅DNS中的域类型为slave。\n  file \"xiemx.com.zone\";                    ——xiemx.com域的数据文件，默认在/var/named/中，缺省目录定义在主配置中directory行。\n  allow-update { none; };\n};                                                          —— xiemx.com域名的正向解析配置\n\nzone \"1.168.192.in-addr.arpa\" IN { ——\"\"内的IP反向写要解析的ip网段.in-addr-arpa,本条就是解析192.168.1这个网段\n  type master;\n  file \"192.168.1.zone\";\n  allow-update { none; };\n};                                                          ——192.168.1这个网段的反向解析配置\n\n 以上的named.rfc1912.zones内容只是节选了部分，如我们手动添加容易出现语法错误时，可以复制文件中原有的localhost区域信息，直接修改即可。\n```\n```shell\n[root@localhost ~]# cat /var/named/xiemx.com.zone\n$TTL    1D       ——TTL条目更新时间默认1D，当时间小时解析精度高，时间太大会导致更改解析记录后生效时间太长，TTL太小的话频繁更新也会影响域名访问速度。\n@          IN       SOA      @   rname.invalid. (\n0    ; serial   ——时间标识，一般都是写当前的日期，由于主辅同步时确定条目是不是新的\n1D    ; refresh——slave多长时间更新一次\n1H    ; retry——slave同步失败时间隔多长时间再次尝试\n1W    ; expire——当主辅不能同步时，1周之后认为其死亡，不再尝试同步\n3H )    ; minimum——条目缓存时间，一般为错误的记录保存此时间。当一个主机请求解析一条错误的条目时，3小时后才能再次请求查询\nNS              @\nA                 192.168.1.10\nwww    A                 192.168.1.100\nMX  5          mail                 ——MX记录后接5为优先级，mx记录有优先级属性需要定义\nmail     A                 192.168.1.101\nftp       CNAME      ftp1                \nftp1       A                192.168.1.103  \n$generate  150-200   test$        A           192.168.1.$      ——用变量定义一个连续范围的解析一一对应别名和IP 。例：test150.xiemx.com<——>192.168.1.150\n```\n```shell\n[root@localhost ~]# cat /var/named/192.168.1.zone\n$TTL    1D\n@          IN       SOA      @   rname.invalid. (\n0    ; serial\n1D    ; refresh\n1H    ; retry\n1W    ; expire\n3H )    ; minimum\nNS              xiemx.com.\n101    PTR    www.xiemx.com.\n102    PTR    mail.xiemx.com.\n103    PTR    ftp.xiemx.com.\n103    PTR    xxx.xiemx.com.\n\n以上的正反解只是测试时随便写的，一般正反解会相互对应。另外在写条目是要注意：\n\n1.mx记录、cname记录的值一般都会有对应A记录\n2.正反解中的@会继承配置文件中\" \"内的内容\n3.正反解时如果写的域名不是完全合格域名，dns会自动用继承的@来补全你的域名：如正解中的www，dns会自动补全为www.xiemx.com.。\n4.同一个IP可对应多个域名（PTR记录可以存在多条），一个域名只能有1条A记录（一个域名只能对应一个IP）\n```\n```shell\n测试：\n\n[root@localhost ~]# nslookup\n> server 192.168.17.6\nDefault server: 192.168.17.6\nAddress: 192.168.17.6#53\n\n> xiemx.com\nServer:        192.168.17.6\nAddress:    192.168.17.6#53\n\nName:    xiemx.com\nAddress: 127.0.0.1\n\n> www.xiemx.com\nServer:        192.168.17.6\nAddress:    192.168.17.6#53\n\nName:    www.xiemx.com\nAddress: 192.168.1.100\n> mail.xiemx.com\nServer:        192.168.17.6\nAddress:    192.168.17.6#53\n\nName:    mail.xiemx.com\nAddress: 192.168.1.101\n\n> ftp.xiemx.com\nServer:        192.168.17.6\nAddress:    192.168.17.6#53\n\nftp.xiemx.com    canonical name = ftp1.xiemx.com.\nName:    xxx.xiemx.com\nAddress: 192.168.1.103\n\n> test88.xiemx.com\nServer:        192.168.17.6\nAddress:    192.168.17.6#53\n\nName:    test88.xiemx.com\nAddress: 192.168.1.88\n\n> 192.168.1.181\nServer:        192.168.17.6\nAddress:    192.168.17.6#53\n\n181.1.168.192.in-addr.arpa    name = test181.xiemx.com.\n```","tags":["dns"],"categories":["dns"]},{"title":"NTP服务器搭建","url":"//2015/11/29/ntp-server/","content":"NTP即网络时间协议，用来同步计算机时间，提高时间的精确度。\n\n环境：`RHEL6.5`\n\n安装包：`ntp-4.2.6p5-1.el6.x86_64`\n\n安装方式：\n```\nyum install ntp\n```\nntp主配置文件中并无太多配置，有效的配置默认如下\n```shell\n[root@localhost html]# egrep  -v \"^$|^#\" /etc/ntp.conf\ndriftfile /var/lib/ntp/drift\nrestrict default kod nomodify notrap nopeer noquery——拒绝ipv4查询\nrestrict -6 default kod nomodify notrap nopeer noquery——拒绝ipv6查询\nrestrict 127.0.0.1——允许本机查询\nrestrict -6 ::1——允许本机查询\nserver 0.rhel.pool.ntp.org iburst——server指定我们去哪里同步\nserver 1.rhel.pool.ntp.org iburst\nserver 2.rhel.pool.ntp.org iburst\nserver 3.rhel.pool.ntp.org iburst\nincludefile /etc/ntp/crypto/pw\nkeys /etc/ntp/keys\n```\n我们作为服务器端需要授权其他用户过来同步时间，需要在配置行中添加\n```\nrestrict 192.168.10.10 mask 255.255.255.0 nomodify ——允许192.168.10.10/24这台机器过来同步\nrestrict 192.168.10.0 mask 255.255.255.0 nomodify ——允许192.168.10.0/24这个网段的机器过来同步\nserver 210.72.145.44 ——我们去这台机器上同步时间\n```\n以上设置设置好后我们即可通过ntpdate去同步时间\n```\nntpdate 210.72.145.44\n```\n客户端也可以通过ntpdate向我们请求数据，ntp服务器搭建好后客户端可以通过计划任务固定时间来向我们请求时间。ntp配置简单，但要注意防火墙的限制。\n\n#### 常用ntp服务器：\n```\n210.72.145.44 (国家授时中心服务器IP地址)\nntp.sjtu.edu.cn 202.120.2.101 (上海交通大学网络中心NTP服务器地址）\ns1b.time.edu.cn 清华大学\ns1c.time.edu.cn 北京大学\ns1d.time.edu.cn 东南大学\ns1e.time.edu.cn 清华大学\ns2d.time.edu.cn 西南地区网络中心\ns2e.time.edu.cn 西北地区网络中心\ns2f.time.edu.cn 东北地区网络中心\ns2g.time.edu.cn 华东南地区网络中心\n```\n ","tags":["linux","ntp"],"categories":["linux"]},{"title":"linux磁盘配额","url":"//2015/11/29/linux-disk-quota/","content":"再多用户的模式下，linux中常常需要对用户进行磁盘空间限制，例如虚拟主机需要限制用户的空间。linux大多数的发行版都采用quota来对磁盘配额来进行管理，quota是系统内核中的一个功能，要使用quota需要系统内核支持quota功能。目前使用的发行本中都是支持次功能的，如果内核不支持此功能那么就需要重新编译下内核来开启此功能\n```shell\n### grep CONFIG_QUOTA /boot/config-`uname -r` 来检查下内核是否支持\n\n[root@localhost mnt]# grep CONFIG_QUOTA /boot/config-2.6.32-431.el6.x86_64\nCONFIG_QUOTA=y\nCONFIG_QUOTA_NETLINK_INTERFACE=y\n# CONFIG_QUOTA_DEBUG is not set\nCONFIG_QUOTA_TREE=y\nCONFIG_QUOTACTL=y\n```\n如有以上两条则说明支持，另外quota是针对用户去限制其可使用的block和inode，所以应该是一个基于文件系统的配置，所以我们要在文件系统中开启quota功能，针对已经挂载过的文件系统我们可以使用\n`mount -o  remount,usrquota,grpquota,default    /mnt` \n来重新挂载激活（mnt只是个举例，实际中先df 命令查看当前挂载信息，在选择要开启的文件系统），也可以写在`/etc/fstab`文件中`umount` 后再`mount -a` 或重启系统\n```shell\n[root@localhost mnt]# mount\n/dev/mapper/VolGroup-lv_root on / type ext4 (rw)\nproc on /proc type proc (rw)\nsysfs on /sys type sysfs (rw)\ndevpts on /dev/pts type devpts (rw,gid=5,mode=620)\ntmpfs on /dev/shm type tmpfs (rw,rootcontext=\"system_u:object_r:tmpfs_t:s0\")\n/dev/sda1 on /boot type ext4 (rw)\nnone on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)\n/dev/mapper/vgxiemx-lv1 on /mnt type ext3 (rw)\n\n[root@localhost mnt]# mount -o remount,grpquota,usrquota,defaults /mnt\n\n[root@localhost mnt]# mount\n/dev/mapper/VolGroup-lv_root on / type ext4 (rw)\nproc on /proc type proc (rw)\nsysfs on /sys type sysfs (rw)\ndevpts on /dev/pts type devpts (rw,gid=5,mode=620)\ntmpfs on /dev/shm type tmpfs (rw,rootcontext=\"system_u:object_r:tmpfs_t:s0\")\n/dev/sda1 on /boot type ext4 (rw)\nnone on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)\n/dev/mapper/vgxiemx-lv1 on /mnt type ext3 (rw,usrquota,grpquota)\n[root@localhost mnt]#\n```\n**此时已开启/mnt对应的文件系统的quota功能，我们可针对/mnt目录下进行用户配额限制**\n\n\n1. 检查文件系统的quota信息，生成quota数据库，会在文件系统的根目录下生成quota.user和quota.group二个data类型的文件可用file命令查看\n```shell\n[root@localhost mnt]# quotacheck -avug\nquotacheck: Your kernel probably supports journaled quota but you are not using it. Consider switching to journaled quota to avoid running quotacheck after an unclean shutdown.\nquotacheck: Scanning /dev/mapper/vgxiemx-lv1 [/mnt] done\nquotacheck: Checked 3 directories and 6 files\n[root@localhost mnt]# ls\naquota.group  aquota.user  lost+found\n```\n2. 启动quota\nquotaon -av —a选项启动所有文件系统，也可指定具体某个文件系统。关闭`quotaoff`命令，参数相同。\n```shell\n[root@localhost /]# quotaon -av\n/dev/mapper/vgxiemx-lv1 [/mnt]: group quotas turned on\n/dev/mapper/vgxiemx-lv1 [/mnt]: user quotas turned on\n```\n3. 编辑用户的quota信息\nedquota  -u testuser——会打开一个vi界面\n```shell\nDisk quotas for user testuser (uid 500):\nFilesystem                                    blocks       soft       hard     inodes     soft     hard\n/dev/mapper/vgxiemx-lv1                        0             10        100       0          10       100\n\nfilesystem：目标文件系统\nblock：现在已使用的block数量\ninode：现在已使用的inode数量\nsoft：软限制，可使用的soft数量，0为不限制，具体数值设定后超过数值进行警告，但依旧可以写入，知道宽限时间到达后无法写入默认宽限7天\nhard：硬限制，可使用block/inode数量，0为不限制，具体数值设定后超过数值，立即无法写入。\n```\n例：\n```shell\n[testuser@localhost mnt]$ dd if=/dev/zero of=/mnt/bigfile bs=1M count=20 ——生成一个20M的文件，查过soft block限制\ndm-2: warning, user block quota exceeded.\n1+0 records in\n0+0 records out\n98304 bytes (98 kB) copied, 0.00147599 s, 66.6 MB/s\n\n[testuser@localhost mnt]$ du bigfile ——统计文件大小20M\n20 bigfile\n\n[testuser@localhost mnt]$ dd if=/dev/zero of=/mnt/bigfile bs=1M count=110——生成一个110M的文件，超过hard block限制，最后10M数据无法写入\ndm-2: warning, user block quota exceeded.\ndm-2: write failed, user block limit reached.\ndd: writing `/mnt/bigfile': Disk quota exceeded\n1+0 records in\n0+0 records out\n98304 bytes (98 kB) copied, 0.000795784 s, 124 MB/s\n\n[testuser@localhost mnt]$ du bigfile——统计文件大小为100M\n100 bigfile\n\n[testuser@localhost mnt]$ ll\ntotal 132\n-rw-------. 1 root root 7168 Nov 22 04:01 aquota.group\n-rw-------. 1 root root 7168 Nov 22 03:58 aquota.user\n-rw-rw-r--. 1 testuser testuser 98304 Nov 22 04:02 bigfile\ndrwxrwxrwx. 2 root root 16384 Nov 22 02:31 lost+found\n\n[testuser@localhost mnt]$ touch file{1..10}——创建10个文件，此时已有1个bigfile，总共11个文件超过soft inode限制\ndm-2: warning, user file quota exceeded.\n\n[testuser@localhost mnt]$ ls\naquota.group bigfile file10 file3 file5 file7 file9\naquota.user file1 file2 file4 file6 file8 lost+found\n\n[testuser@localhost mnt]$ touch test{1..100}——创建100个文件，此时已有11个文件，所以最后的11个文件应该创建不成功\ndm-2: write failed, user file limit reached.\ntouch: cannot touch `test90': Disk quota exceeded\ntouch: cannot touch `test91': Disk quota exceeded\ntouch: cannot touch `test92': Disk quota exceeded\ntouch: cannot touch `test93': Disk quota exceeded\ntouch: cannot touch `test94': Disk quota exceeded\ntouch: cannot touch `test95': Disk quota exceeded\ntouch: cannot touch `test96': Disk quota exceeded\ntouch: cannot touch `test97': Disk quota exceeded\ntouch: cannot touch `test98': Disk quota exceeded\ntouch: cannot touch `test99': Disk quota exceeded\ntouch: cannot touch `test100': Disk quota exceeded\n\n[testuser@localhost mnt]$ ls\naquota.group file7 test15 test24 test33 test42 test51 test60 test7 test79 test88\naquota.user file8 test16 test25 test34 test43 test52 test61 test70 test8 test89\nbigfile file9 test17 test26 test35 test44 test53 test62 test71 test80 test9\nfile1 lost+found test18 test27 test36 test45 test54 test63 test72 test81\nfile10 test1 test19 test28 test37 test46 test55 test64 test73 test82\nfile2 test10 test2 test29 test38 test47 test56 test65 test74 test83\nfile3 test11 test20 test3 test39 test48 test57 test66 test75 test84\nfile4 test12 test21 test30 test4 test49 test58 test67 test76 test85\nfile5 test13 test22 test31 test40 test5 test59 test68 test77 test86\nfile6 test14 test23 test32 test41 test50 test6 test69 test78 test87\n```\n\n4.设置宽限时间\n```\nedquota -t  ——默认7天\nGrace period before enforcing soft limits for users:\nTime units may be: days, hours, minutes, or seconds\nFilesystem                                   Block grace period                        Inode grace period\n/dev/mapper/vgxiemx-lv1                      7days                                     7days\n```\n5.查询所有用户的磁盘配额情况，用户也可用quota查询当前用户的quota信息\n```\nrepquota -a\n\n[root@localhost mnt]# repquota -a\n*** Report for user quotas on device /dev/mapper/vgxiemx-lv1\nBlock grace time: 7days; Inode grace time: 7days\nBlock limits File limits\nUser                      used      soft      hard    grace    used      soft      hard       grace\n-------------------------------------------------------------------------------------------\nroot           --        956592     0         0                 6          0        0\ntestuser       ++        100        10        100      6days    100        10       100       6days\n```\n*如果要设置quota，建议在/etc/rc.d/rc.local中写入quotacheck和quotaon 已保证系统启动时quota服务开启且数据都是最新的。分区的quote功能开启建议在/etc/fstab中。*\n\n例：\n```\n/etc/fstab\n# Created by anaconda on Sun May 24 08:16:25 2015\n#\n# Accessible filesystems, by reference, are maintained under '/dev/disk'\n# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info\n#\n/dev/mapper/VolGroup-lv_root / ext4 defaults 1 1\nUUID=3fe6311e-b3fe-4f7f-8558-4f011eab1dde /boot ext4 defaults 1 2\n/dev/mapper/VolGroup-lv_swap swap swap defaults 0 0\ntmpfs /dev/shm tmpfs defaults 0 0\ndevpts /dev/pts devpts gid=5,mode=620 0 0\nsysfs /sys sysfs defaults 0 0\nproc /proc proc defaults 0 0\n/dev/mapper/vgxiemx-lv1 /mnt ext3 defaults,usrquota,grpquota 0 0\n```","tags":["command","linux"],"categories":["linux"]},{"title":"Linux动态磁盘管理（lvm）","url":"//2015/11/29/linux-lvm/","content":"lvm是将底层硬件存储屏蔽整合，通过软件组合在对系统开放，此时系统查看到的就不是底层一块块的物理磁盘了而是我们虚拟的逻辑磁盘，我们可以自由的对这些磁盘进行容量管理。lvm从系统层面到物理层面分别对应“文件系统——lv——vg——pv——磁盘”。所有对磁盘的管理都要遵循这个物理结构的顺序来处理。创建时由下而上，删除时由上而下，调整中间部分是也要结合上下部分分析需求具体在进行处理。\n\npv——物理卷\nvg——卷组\nlv——逻辑卷\npe——最小存储单元，类似与磁盘block\n\nlvm是介于系统与硬件之间的一种磁盘组织方法，下层为上层的基础，下层制约上层。\n\n## lvm管理\n\n1. 磁盘分区\n```\nfdisk ／dev/vda\n```\n2. 同步内核中的分区表信息（如果是新硬盘一般不需要这一部分）\n```\npartx  -a   /dev/vda\n```\n3. 创建pv\n```\npvcreate ／dev/vda1  ——有多个分区时可以依次空格间隔写在后面，也可以针对整块硬盘去直接创建\n\npvs和pvdisplay查看pv缩略和详细信息\n\n[root@localhost ~]# pvs\nPV VG Fmt Attr PSize PFree\n/dev/sda2 VolGroup lvm2 a-- 19.51g 0\n/dev/sdb lvm2 a-- 10.00g 10.00g\n\n[root@localhost ~]# pvdisplay\n--- Physical volume ---\nPV Name               /dev/sda2\nVG Name               VolGroup\nPV Size               19.51 GiB / not usable 3.00 MiB\nAllocatable           yes (but full)\nPE Size               4.00 MiB\nTotal PE              4994\nFree PE               0\nAllocated PE          4994\nPV UUID               fcQLJh-oq2G-adfr-QeEK-sdtl-3XND-m1YtPA\n\n\"/dev/sdb\" is a new physical volume of \"10.00 GiB\"\n--- NEW Physical volume ---\nPV Name               /dev/sdb\nVG Name\nPV Size               10.00 GiB\nAllocatable           NO\nPE Size               0\nTotal PE              0\nFree PE               0\nAllocated PE          0\nPV UUID               WQpkNi-1cN6-QogB-TLB2-R2wT-Agmh-BRBiUN\n```\n4. 创建vg\n```\nvgcreate  -s 8M vgname /dev/vda1  /dev/vda2  —— s选项制定pe的大小默认为4M\n\nvgs和vgdisplay查看vg信息\n\n[root@localhost ~]# vgcreate -s 8M vgxiemx /dev/sdb\nVolume group \"vgxiemx\" successfully created\n[root@localhost ~]# vgs\nVG       #PV #LV #SN Attr   VSize  VFree\nVolGroup   1   2   0 wz--n- 19.51g    0\nvgxiemx    1   0   0 wz--n-  9.99g 9.99g\n[root@localhost ~]# vgdisplay\n--- Volume group ---\nVG Name               vgxiemx\nSystem ID\nFormat                lvm2\nMetadata Areas        1\nMetadata Sequence No  1\nVG Access             read/write\nVG Status             resizable\nMAX LV                0\nCur LV                0\nOpen LV               0\nMax PV                0\nCur PV                1\nAct PV                1\nVG Size               9.99 GiB\nPE Size               8.00 MiB\nTotal PE              1279\nAlloc PE / Size       0 / 0\nFree  PE / Size       1279 / 9.99 GiB\nVG UUID               5WZfct-snfo-aEDl-gHuO-QDbz-v83j-L0lwmR\n```\n5. 创建lv\n```\nlvcreate -l 100 -n lvname vgname——l选项指定分配100个pe给lv ，n指定名称，最后从哪个vg创建。\n\nlvcreate -L1000M -n lvname vgname——L便是直接分配多大空间单位为K M G。L和l参数还有很多种容量表示方法man查阅\n\nlvs和lvdisplay查看lv信息\n\n[root@localhost ~]# lvcreate -n lv1 -l 1279 vgxiemx\nLogical volume \"lv1\" created\n[root@localhost ~]# lvs\nLV VG Attr LSize Pool Origin Data% Move Log Cpy%Sync Convert\nlv_root VolGroup -wi-ao---- 17.51g\nlv_swap VolGroup -wi-ao---- 2.00g\nlv1 vgxiemx -wi-a----- 9.99g\n[root@localhost ~]# lvdisplay\n--- Logical volume ---\nLV Path /dev/vgxiemx/lv1\nLV Name lv1\nVG Name vgxiemx\nLV UUID nViafR-nrnS-REMQ-bGOT-uhDe-Skd0-XUNwmU\nLV Write Access read/write\nLV Creation host, time localhost.localdomain, 2015-11-22 01:50:41 +0800\nLV Status available\n# open 0\nLV Size 9.99 GiB\nCurrent LE 1279\nSegments 1\nAllocation inherit\nRead ahead sectors auto\n- currently set to 256\nBlock device 253:2\n```\n6. 创建文件系统\n```\nmkfs.ext3  /dev/vgname/lvname ——创建文件系统\n\n[root@localhost ~]# mkfs -t ext3 /dev/vgxiemx/lv1\nmke2fs 1.41.12 (17-May-2010)\nFilesystem label=\nOS type: Linux\nBlock size=4096 (log=2)\nFragment size=4096 (log=2)\nStride=0 blocks, Stripe width=0 blocks\n655360 inodes, 2619392 blocks\n130969 blocks (5.00%) reserved for the super user\nFirst data block=0\nMaximum filesystem blocks=2684354560\n80 block groups\n32768 blocks per group, 32768 fragments per group\n8192 inodes per group\nSuperblock backups stored on blocks:\n32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632\n\nWriting inode tables: done\nCreating journal (32768 blocks): done\nWriting superblocks and filesystem accounting information: done\n\nThis filesystem will be automatically checked every 30 mounts or\n180 days, whichever comes first.  Use tune2fs -c or -i to override.\n```\n7. 挂载lv\n```\nmount /dev/vgname/lvname  /mnt\n\n[root@localhost ~]# mount /dev/vgxiemx/lv1 /mnt\n[root@localhost mnt]# df -l\nFilesystem 1K-blocks Used Available Use% Mounted on\n/dev/mapper/VolGroup-lv_root 18069936 6767876 10384148 40% /\ntmpfs 953788 72 953716 1% /dev/shm\n/dev/sda1 495844 39915 430329 9% /boot\n/dev/mapper/vgxiemx-lv1 10313016 154232 9634908 2% /mnt\n```\n## lvm调整\n\n### pv的新增、删除\n\n#### pv的新增\n就是增加新的空闲分区或者新的硬盘创建成新的空闲pv同上pv创建命令一致\n\n#### pv删除\n```\npvremove /dev/vdb\n\n[root@localhost /]# pvremove /dev/sdb\nLabels on physical volume \"/dev/sdb\" successfully wiped\n[root@localhost /]# pvs\nPV VG Fmt Attr PSize PFree\n/dev/sda2 VolGroup lvm2 a-- 19.51g 0\n```\n#### 移动pv中的数据\n\n由下图我们可以看到pv/dev/sdb1中的1G数据已经被我们移动到/dev/sdb2中去了，如果是单硬盘做的pv此时我们就可以撤除/dev/sdb1对应的这块硬盘，而数据没有丢失\n```\npvremove /dev/sdb1 /dev/sdb2\n\n[root@localhost mnt]# pvs\nPV         VG       Fmt  Attr PSize  PFree\n/dev/sda2  VolGroup lvm2 a--  19.51g    0\n/dev/sdb1  vgxiemx  lvm2 a--   1.01g 8.00m\n/dev/sdb2  vgxiemx  lvm2 a--   1.01g 1.01g\n/dev/sdb3  vgxiemx  lvm2 a--   1.01g 1.01g\n/dev/sdb5           lvm2 a--   2.01g 2.01g\n/dev/sdb6           lvm2 a--   4.96g 4.96g\n[root@localhost mnt]# pvmove /dev/sdb1 /dev/sdb2\n/dev/sdb1: Moved: 1.6%\n/dev/sdb1: Moved: 45.3%\n/dev/sdb1: Moved: 89.8%\n/dev/sdb1: Moved: 100.0%\n[root@localhost mnt]# pvs\nPV         VG       Fmt  Attr PSize  PFree\n/dev/sda2  VolGroup lvm2 a--  19.51g    0\n/dev/sdb1  vgxiemx  lvm2 a--   1.01g 1.01g\n/dev/sdb2  vgxiemx  lvm2 a--   1.01g 8.00m\n/dev/sdb3  vgxiemx  lvm2 a--   1.01g 1.01g\n/dev/sdb5           lvm2 a--   2.01g 2.01g\n/dev/sdb6           lvm2 a--   4.96g 4.96g\n```\n### vg扩容、缩小、删除\n\n#### vg的扩容\n\nvg的容量来自于pv，如果vg的容量不够那么vg扩容其实就是新增空闲的pv到vg中去\n```\nvgextend  /dev/sdb5  vgxiemx\n\n[root@localhost /]# pvs\nPV VG Fmt Attr PSize PFree\n/dev/sda2 VolGroup lvm2 a-- 19.51g 0\n/dev/sdb1 vgxiemx lvm2 a-- 1.01g 1.01g\n/dev/sdb2 vgxiemx lvm2 a-- 1.01g 1.01g\n/dev/sdb3 vgxiemx lvm2 a-- 1.01g 1.01g\n/dev/sdb5                 lvm2 a-- 2.01g 2.01g\n/dev/sdb6                 lvm2 a-- 4.96g 4.96g\n[root@localhost /]# vgextend vgxiemx /dev/sdb5\nVolume group \"vgxiemx\" successfully extended\n[root@localhost /]# pvs\nPV VG Fmt Attr PSize PFree\n/dev/sda2 VolGroup lvm2 a-- 19.51g 0\n/dev/sdb1 vgxiemx lvm2 a-- 1.01g 1.01g\n/dev/sdb2 vgxiemx lvm2 a-- 1.01g 1.01g\n/dev/sdb3 vgxiemx lvm2 a-- 1.01g 1.01g\n/dev/sdb5 vgxiemx lvm2 a-- 2.00g 2.00g\n/dev/sdb6                 lvm2 a-- 4.96g 4.96g\n```\n#### vg的减小\n\n也就是将vg中的pv移除，但pv移除是存在pv中的pe都已经被使用存在数据，这是我们就需要移动要删除的数据到新的pv中去，新的pv空间必须要比旧的pv空间大。\n```\nvgreduce vgxiemx /dev/sdb5\n\n[root@localhost /]# pvs\nPV         VG       Fmt  Attr PSize  PFree\n/dev/sda2  VolGroup lvm2 a--  19.51g    0\n/dev/sdb1  vgxiemx  lvm2 a--   1.01g 1.01g\n/dev/sdb2  vgxiemx  lvm2 a--   1.01g 1.01g\n/dev/sdb3  vgxiemx  lvm2 a--   1.01g 1.01g\n/dev/sdb5  vgxiemx  lvm2 a--   2.00g 2.00g\n/dev/sdb6           lvm2 a--   4.96g 4.96g\n[root@localhost /]# vgreduce vgxiemx /dev/sdb5\nRemoved \"/dev/sdb5\" from volume group \"vgxiemx\"\n[root@localhost /]# pvs\nPV         VG       Fmt  Attr PSize  PFree\n/dev/sda2  VolGroup lvm2 a--  19.51g    0\n/dev/sdb1  vgxiemx  lvm2 a--   1.01g 1.01g\n/dev/sdb2  vgxiemx  lvm2 a--   1.01g 1.01g\n/dev/sdb3  vgxiemx  lvm2 a--   1.01g 1.01g\n/dev/sdb5           lvm2 a--   2.01g 2.01g\n/dev/sdb6           lvm2 a--   4.96g 4.96g\n```\n#### vg的删除\n\n```\nvgremove vgxiemx\n\n[root@localhost /]# vgremove vgxiemx\nVolume group \"vgxiemx\" successfully removed\n[root@localhost /]# vgs\nVG #PV #LV #SN Attr VSize VFree\nVolGroup 1 2 0 wz--n- 19.51g 0\n```\n### lv的扩容、缩小、删除\n\n#### lv的缩小\n```\nlvreduce -L 9000M /dev/vgxiemx/lv1\n\n[root@localhost /]# lvreduce -L 9000M /dev/vgxiemx/lv1\nWARNING: Reducing active logical volume to 8.79 GiB\nTHIS MAY DESTROY YOUR DATA (filesystem etc.)\nDo you really want to reduce lv1? [y/n]: y\nReducing logical volume lv1 to 8.79 GiB\nLogical volume lv1 successfully resized\n[root@localhost /]# lvs\nLV VG Attr LSize Pool Origin Data% Move Log Cpy%Sync Convert\nlv_root VolGroup -wi-ao---- 17.51g\nlv_swap VolGroup -wi-ao---- 2.00g\nlv1 vgxiemx -wi-a----- 8.79g\n```\n#### lv的扩容\n```\nlvextend -L 10000M /dev/vgxiemx/lv1\n\n[root@localhost /]# lvextend -L 10000M /dev/vgxiemx/lv1\nExtending logical volume lv1 to 9.77 GiB\nLogical volume lv1 successfully resized\n[root@localhost /]# lvs\nLV VG Attr LSize Pool Origin Data% Move Log Cpy%Sync Convert\nlv_root VolGroup -wi-ao---- 17.51g\nlv_swap VolGroup -wi-ao---- 2.00g\nlv1 vgxiemx -wi-a----- 9.77g\n```\n#### lv的删除\n```\nlvremove /dev/vgxiemx/lv1\n\n[root@localhost /]# lvremove /dev/vgxiemx/lv1\nDo you really want to remove active logical volume lv1? [y/n]: y\nLogical volume \"lv1\" successfully removed\n[root@localhost /]# lvs\nLV VG Attr LSize Pool Origin Data% Move Log Cpy%Sync Convert\nlv_root VolGroup -wi-ao---- 17.51g\nlv_swap VolGroup -wi-ao---- 2.00g\n```\n### 文件系统的扩缩\n\n#### 检测文件系统\n```\ne2fsck /dev/vgxiemx/lv1\n\n[root@localhost /]# e2fsck -f /dev/vgxiemx/lv1\ne2fsck 1.41.12 (17-May-2010)\nPass 1: Checking inodes, blocks, and sizes\nPass 2: Checking directory structure\nPass 3: Checking directory connectivity\nPass 4: Checking reference counts\nPass 5: Checking group summary information\n/dev/vgxiemx/lv1: 11/655360 files (0.0% non-contiguous), 79696/2619392 blocks\n```\n#### 调整文件系统大小\n```\nresize2fs /dev/vgxiemx/lv1 9000M\n\n[root@localhost /]# resize2fs /dev/vgxiemx/lv1 9000M\nresize2fs 1.41.12 (17-May-2010)\nResizing the filesystem on /dev/vgxiemx/lv1 to 2304000 (4k) blocks.\nThe filesystem on /dev/vgxiemx/lv1 is now 2304000 blocks long.\n```\n ","tags":["linux","lvm"],"categories":["linux"]},{"title":"Linux计划任务","url":"//2015/11/28/linux-crontab-and-at/","content":"计划任务种类：\n* 一次性计划任务（at）：由进程atd守护\n* 周期性计划任务（cron）：由进程crond守护\n\n#### 一次性计划任务（at）\n\n生成的文件存放在／var/spool/at/目录下，任务执行一次之后自动删除。一次性计划任务还可以用batch命令去执行。batch命令会在系统空闲的情况下执行任务，用法和at相同但优先级低延时执行。\n\n命令：\n```\n［root@localhost ~]# at  now + 5 minutes\n  at> /bin/mail  root -s \"testing at job\" < /root/.bashrc\n  at> <EOF>         <==输入 [ctrl] + d 结束编辑！此时会在／var/spool/at/目录下生成一个任务文件\n\n[root@localhost ~]# atq\n5 2015-11-25 23:00 a root\n[root@localhost ~]# atrm 5\n```\n一次性计划任务的时间写法支持很多可以查看man page！当我们制定计划任务之后，由于种种原因而要去取消计划任务我们可以使用atq去查询当前系统中有多少为执行的计划，每个计划的编号是多少。在通过atrm＋任务编号来删除这个任务。／etc/at.allow和／etc/at.deny文件规范了哪些用户可以使用at那些用户无法使用at。写在allow文件表明了写在此文件中的用户才可以使用at，deny文件中的用户无法使用。\n\n#### 周期性计划任务（cron）\n\n由进程crond守护，生成的任务文件存放在／var/spool/cron/目录下文件会以用户名命名。也采用了／etc/cron.allow和／etc/cron.deny的授权方式。配置文件为／etc/crontab 。周期性计划任务分为两种，一种是用户级别（通过crontab －e来制定），一种为系统级别（写在／etc/crontab文件中），建议使用crontab来制定。\n\n命令用法\n```\ncrontab   参数\n-u ：只有 root 才能进行这个任务，亦即帮其他使用者创建/移除 crontab 工作排程；\n-e ：编辑 crontab 的工作内容，同vi编辑文件相同（其实就是vi了一个新文件到／var/spool/cron下）\n-l ：查阅 crontab 的工作内容\n-r ：移除所有的 crontab 的工作内容，若仅要移除一项，请用 -e 去编辑。\n\n［root@localhost ～］#crontab  -u  testuser  -e    #为testuser用户制定计划任务，会打开vi的编辑界面，每一行就是一个任务\n```\n例：\n```\n###时间的写法基本就是\",\" \"-\" \"/\"三种符号来间隔,“＊”代表所有都匹配。\n30 * * * *           <==每小时的30分，输出hello\n*/10 ＊ *  *  *      <==每隔10分钟\n30 12 * * 1,3,5      <==每周1 3 5的12时30分\n30 12 1 3-5 *        <==每年的3 4 5月1日12时30分\n```\n\n同样的／etc/crontab 中的时间写法也是类似但`语法不同`cron每分钟都会去读一次计划任务同时也会读一次／etc/crontab文件，crontab文件中设置了4个文件夹（配置文件中的`run-parts`命令部分），系统会在不同的时间去读取运行其中的文件，我们可以将脚本文件放到对应的文件夹下也可以实现脚本的周期性执行。我们的locate数据库同步，logrotate等都是放在这些目录下来实现周期性的工作。\n```\n[root@localhost ~]# cat /etc/crontab\nSHELL=/bin/bash\nPATH=/sbin:/bin:/usr/sbin:/usr/bin\nMAILTO=root\nHOME=/\n\n# run-parts\n01 * * * * root run-parts /etc/cron.hourly <==每小时执行一次\n02 4 * * * root run-parts /etc/cron.daily <==每天执行一次\n22 4 * * 0 root run-parts /etc/cron.weekly <==每周日执行一次\n42 4 1 * * root run-parts /etc/cron.monthly <==每个月 1 号执行一次\n\n###run-parts 我们可以通过which命令来查看其实是/usr/bin/run-parts命令，我们可以man一下这个命令，或者直接打开这个命令的脚本，会发现这个命令会将目录下的脚本全部执行一次。\n```\n","tags":["command","linux"],"categories":["linux"]},{"title":"Linux日志系统","url":"//2015/11/28/linux-log-system/","content":"在Linux系统中，有三个主要的日志子系统：\n\n* 连接日志--由多个程序执行，把纪录写入到/var/log/wtmp和/var/run/utmp，login等程序更新wtmp和utmp文件，使系统管理员能够跟踪谁在何时登录到系统。\n* 进程统计--由系统内核执行。当一个进程终止时，为每个进程往进程统计文件（pacct或acct）中写一个纪录。进程统计的目的是为系统中的基本服务提供命令使用统计。\n* 错误日志--由syslogd（8）执行。各种系统守护进程、用户程序和内核通过syslog（3）向文件/var/log/messages报告值得注意的事件。另外有许多UNIX程序创建日志。像HTTP和FTP这样提供网络服务的服务器也保持详细的日志。\n\n大部分Linux系统都使用syslog管理日志，系统日志默认会写在/var/log目录下，syslog会依据/etc/syslog.conf文件中的配置将不同级别、不同类别的日志分门别类的纪录到不同的日志文件中去。/etc/syslog.conf中依照如下格式纪录日志配置：\n```\n日志对象.级别                  日志文件存放位置\nauthpriv.info                /var/log/secure\n```\n```\n［root@localhost ～］#cat /etc/syslog.conf\n\n//将info或更高级别的消息送到/var/log/messages，除了mail以外。\n//其中*是通配符，代表任何设备；none表示不对任何级别的信息进行记录。\n*.info;mail.none;authpriv.none /var/log/messages\n//将authpirv设备的任何级别的信息记录到/var/log/secure文件中，这主要是一些和认、权限使用相关的信息。\nauthpriv.* /var/log/secure\n//将mail设备中的任何级别的信息记录到/var/log/maillog文件中，这主要是和电子邮件相关的信息。\nmail.* /var/log/maillog\n//将cron设备中的任何级别的信息记录到/var/log/cron文件中，这主要是和系统中定期执行的任务相关的信息。\ncron.* /var/log/cron\n//将任何设备的emerg级别的信息发送给所有正在系统上的用户。\n*.emerg *\n//将uucp和news设备的crit级别的信息记录到/var/log/spooler文件中。\nuucp,news.crit /var/log/spooler\n//将和系统启动相关的信息记录到/var/log/boot.log文件中。\nlocal7.* /var/log/boot.log\n```\n##### 日志对象\n\nkern——内核\nUser——用户程序\nDamon——系统守护进程\nMail——电子邮件系统\nAuth——与安全权限相关的命令\nLpr——打印机\nNews——新闻组信息\nUucp——Uucp程序\nCron——记录当前登录的每个用户信息\nwtmp——一个用户每次登录进入和退出时间的永久记录\nAuthpriv——授权信息\n\n##### 日志的级别\n\nemerg——最高的紧急程度状态\nalert——紧急状态\nCirt——重要信息\nwarning——警告\nerr——错误\nnotice——通知\ninfo——一般性消息\nDebug——调试级信息\nNone——不记录任何日志信息常用的日志文件\n\n##### 常见的日志文件\n\ncron: crontab例行事务的日志\ndmesg: 内核启动时的检测信息,输出同 dmesg 命令\nlastlog: 所有帐号最后一次登录的相关信息，输出同 lastlog 命令\nmaillog: 邮件来往信息\nmessages ： 系统错误信息\nsecure ： 涉及到“输入口令”的操作，都会记录于此\nwtmp与faillog: 登录成功的用户信息(wtmp)和登录失败的用户信息(faillog)\nhttpd, samba等 ： 不同的网络服务用自己的定制的日志文件\nutmp、wtmp和lastlog日志文件是多数重用UNIX日志子系统的关键--保持用户登录进入和退出的纪录。有关当前登录用户的信息记录在文件utmp中；登录进入和退出纪录在文件wtmp中；最后一次登录文件可以用lastlog命令察看。数据交换、关机和重起也记录在wtmp文件中。通常，每次有一个用户登录时，login程序在文件lastlog中察看用户的UID。如果找到了，则把用户上次登录、退出时间和主机名写到标准输出中，然后login程序在lastlog中纪录新的登录时间。在新的lastlog纪录写入后，utmp文件打开并插入用户的utmp纪录。该纪录一直用到用户登录退出时删除。utmp文件被各种命令文件使用，包括who、w、users和finger。 下一步，login程序打开文件wtmp附加用户的utmp纪录。当用户登录退出时，具有更新时间戳的同一utmp纪录附加到文件中。wtmp文件被程序last和ac使用。\n\n### 日志轮替（logrotate）\n\n在系统用户众多的，或系统运行有访问量非常大的程序时，日志文件的增长会非常迅速，严重的会导致系统磁盘被写满数据无法被写入系统进程报错中止等。为了应对这个问题，Linux采取了日志轮替的方式来管理日志文件。logrotate程序会依据/etc/logrotate.conf和/etc/logrotate.d/中定义的轮替规则来裁剪、删除、备份日志文件。logrotate会在/etc/cron.daily目录下生成脚本，每天自动执行。\n```\n[root@localhost ~]# cat /etc/logrotate.conf\n\nweekly                        #默认每周进行一次日志清理\nrotate 4                      #保留的日志文件数量\ncreate                         #创建一个新的来存储\n#compress                #是否需要压缩\ninclude /etc/logrotate.d  #读取/etc/logrotate.d目录下的文件\n/var/log/wtmp {               #针对单个wtmp日志操作\nmonthly                              #每个月一次\nminsize 1M                         #超过1M整理\ncreate 0664 root utmp     #新加文件权限和用户组\nrotate 1                                #保留一个文件\n}\n```\n \n","tags":["linux"],"categories":["linux"]},{"title":"Linux启动过程","url":"//2015/11/15/linux-boot-workflow/","content":"1. 加载自检（post）、BIOS\n  \n当你打开计算机电源，计算机会对硬件设备加电，然后去检查cpu，内存，主板这些硬件设备。加点通过后加载BIOS信息，读取bios中存储的设备启动顺序信息，寻找启动设备一般都是硬盘、光盘、U盘等。\n\n2. 读取MBR\n  \n硬盘上第0磁道第一个扇区被称为MBR，也就是Master Boot Record，即主引导记录，大小是512字节，存放了446字节的boots loader、64字节的分区表信息和2字节的硬盘有效标记。\n  \n系统找到BIOS所指定的硬盘的MBR后，就会将其复制到0×7c00地址所在的物理内存中。其实被复制到物理内存的内容就是Boot Loader，常用的boot loader 有grub、lilo。\n\n3. Boot Loader\n  \n系统读取内存中的grub配置信息（一般为menu.lst或grub.lst），并依照此配置信息来启动不同的操作系统。\n\n4. 加载内核\n  \n依据grub.conf配置文件中的配置文件，系统读取内核，挂载虚拟文件系统initrd。\n\n5. 启动init\n  \n内核被加载后运行第一个进程/sbin/init，读取配置文件/etc/init/rc.Scanf,根据此文件配置在找到/etc/rc.d/rc.sysinit开启系统初始化，在读取/etc/inittab文件，确定系统的启动级别。\n\n6. 运行inittab中规定级别的脚本程序\n  \n根据运行级别的不同，系统会运行rc0.d到rc6.d中的相应的脚本程序，/etc/rc.d/rcX.d/目录。\n\n7. 执行/etc/rc.d/rc.local\n  \n/etc/rc.d/rcX.d/中的脚本按照配置启动完成后，会启动rc.local。\n  \n8. 执行/bin/login\n  \n调用登录程序，启动登陆界面。\n\n以上只是简单的系统启动过程，具体详细的系统层面的启动从第boot loader之后的执行都可以参看系统中的配置文件。可以读取相对应的脚本和配置代码，根据文件中的规定一步步的理解系统的启动，理解系统启动的详细步骤。\n","tags":["linux"],"categories":["linux"]},{"title":"Linux进程管理","url":"//2015/11/15/linux-process-manager/","content":"当我们运行程序时，Linux会为程序创建一个特殊的环境，该环境包含程序运行需要的所有资源，以保证程序能够独立运行，不受其他程序的干扰。\n  \n在系统中我们调用一个指令，Linux 就会创建一个新的进程。例如使用 ls 命令遍历目录中的文件时，就创建了一个进程。进程就是程序的实例。\n  \n系统通过一个五位数字跟踪程序的运行状态，这个数字称为 pid 或进程ID。每个进程都拥有唯一的 pid。两个 pid 一样的进程不能同时存在，Linux用 pid 来跟踪程序的运行状态。\n\n### 进程查看\n  \n#### ps\n```\n[root@localhost ~]# ps aux\nUSER PID  %CPU   %MEM    VSZ    RSS    TTY    STAT    START   TIME   COMMAND\nroot      1       0.0        0.0        19356   1540     ?        Ss         02:09         0:01      /sbin/init\nroot      2      0.0         0.0        0           0           ?        S           02:09         0:00     [kthreadd]\nroot      3      0.0         0.0        0           0           ?        S           02:09         0:00     [migration/0]\n\n[root@localhost ~]# ps -ef\nUID     PID     PPID    C    STIME    TTY    TIME       CMD\nroot      1         0            0     02:09        ?      00:00:01     /sbin/init\nroot     2         0            0     02:09        ?      00:00:00     [kthreadd]\nroot     3         2            0     02:09        ?      00:00:00     [migration/0]\n\n[root@localhost ~]# ps -le\nF  S   UID PID PPID  C   PRI   NI   ADDR   SZ       WCHAN   TTY   TIME         CMD\n4  S     0      1       0       0   80      0       -          4839    poll_s       ?         00:00:01    init\n1  S     0      2       0       0   80      0       -           0          kthrea       ?         00:00:00   kthreadd\n1  S     0      3       2       0   -40      -       -           0          migrat      ?         00:00:00    migration/0\n\nUSER 用户名 \nUID 用户ID（User ID） \nPID 进程ID（Process ID） \nPPID 父进程的进程ID（Parent Process id） \nSID 会话ID（Session id） \n%CPU 进程的cpu占用率 \n%MEM 进程的内存占用率 \nVSZ 进程所使用的虚存的大小（Virtual Size） \nRSS 进程使用的实际内存的大小 \nTTY 与进程关联的终端（tty） \nSTAT 进程的状态：进程状态使用字符表示的（STAT的状态码） \nSTART 进程启动时间和日期 \nTIME 进程使用的总cpu时间 \nCOMMAND 正在执行的命令行命令 \nNI 优先级(Nice) \nPRI 进程优先级编号(Priority)\n进程状态（S字段）： D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程\n\n```\n\n#### top\n```\n[root@localhost ~]# top\ntop - 02:45:58 up 36 min, 4 users, load average: 0.00, 0.00, 0.00\nTasks: 175 total, 1 running, 174 sleeping, 0 stopped, 0 zombie\nCpu(s): 0.3%us, 0.3%sy, 0.0%ni, 99.3%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%st\nMem: 1907580k total, 528968k used, 1378612k free, 26060k buffers\nSwap: 2097144k total, 0k used, 2097144k free, 173020k cached\nPID   USER   PR   NI    VIRT     RES    SHR     S    %CPU %MEM TIME+     COMMAND\n2603  root      20    0      15028   1372    1000    R   0.3       0.1         0:00.21     top\n1          root      20    0     19356    1540    1228    S    0.0      0.1          0:01.04     init\n2         root      20    0      0            0          0           S   0.0       0.0        0:00.00    kthreadd\n\n\n第一行显示系统运行时间，用户数，系统负载1分钟 ，5分钟，15分钟的情况\n第二行显示经常总数，不同stat下的进程数量 \n第三行显示cpu使用情况 第四行显示物理内存使用情况 第五行显示swap交换分区使用情况\n```\n\n#### 信号\n  ```\n[root@localhost ~]# kill -l\n1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP\n6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR1\n11) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM\n16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP\n21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ\n26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR\n31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+3\n38) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+8\n43) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+13\n48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-12\n53) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-7\n58) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-2\n63) SIGRTMAX-1 64) SIGRTMAX\n```\nLinux中信号有64种，通过kill命令我们可以对进程发送这些信号来告知这些进程，做什么操作。常用的以下四种\n  \n1) SIGHUP\n这个信号用于通知它重新读取配置文件但不关闭进程，相当于reload。\n2) SIGINT\n程序终止(interrupt)信号, 在用户键入INTR字符(通常是Ctrl-C)时发出，用于通知前台进程组终止进程。\n9) SIGKILL\n用来立即结束程序的运行. 本信号不能被阻塞、处理和忽略。如果管理员发现某个进程终止不了，可尝试发送这个信号\n15) SIGTERM\n程序结束(terminate)信号, 与SIGKILL不同的是该信号可以被阻塞和处理。通常用来要求程序自己正常退出，shell命令kill缺省产生这个信号。如果进程终止不了，我们才会尝试SIGKILL。\n  例：kill -1 2205 ——告知PID为2205的进程重载下配置文件 kill -9 2205 ——直接杀死2205进程 kill -15 2205 ——通知2205进程关闭，进程会在收到信号后结束工作后自行关闭，有些进程会屏蔽15信号如bash，此时我们只能通过9信号强制结束。\n\n#### 进程优先级\n  \n假设系统中有多个进程同时在排队是，cpu应该有限处理哪个进程哪？这就取决于进程的优先级，优先级高的优先处理。进程在创建是我们可以通过nice命令指定他的优先级，对于已经开始的进程我们可以通过renice来修改他的优先级。\n  \n优先级范围：-20~19 数字越小优先级越高，默认创建的进程优先级为0。\n\n例：\n```\nnice -n 10 vim —— 创建一个优先级为10的vim进程\n[root@localhost ~]# ps -el | grep vim\nF S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD\n0 T 0 6891 2563 0 90 10 - 35930 signal pts/2 00:00:00 vim\n\nrenice -n -10 6891 ——调整6891进程的有限级为-10\n[root@localhost ~]# renice -n -10 6891\n6891: old priority 10, new priority -10\n\nroot用户可以自由调整经常的优先级，普通用户只能调低不能调高。\n  \n[testuser@localhost root]$ renice -n 5 6925\n6925: old priority 0, new priority 5\n  \n[testuser@localhost root]$ renice -n 2 6925\nrenice: 6925: setpriority: Permission denied\n```\n#### 前台进程/后台进程\n  \n由于一个终端在处理工作时只能处理一个，当我们想在处理第二个进程时会由于终端被占用而无法处理。这时我们可以将前台的这个进程放到后台去运行，把前台的终端空出让我们运行新的进程。\n  \n命令：\n* command &让进程在后台运行\n* jobs –l 查看后台运行的进程\n* fg %n 让后台运行的进程n到前台来\n* bg %n 让后台的进程n运行\n  \n以上的%n是通过jobs -l 查看到的工作编号不是pid。\n\n例：\n```\nvim & ——在后台运行vim进程\njobs -l ——查看后台所有进程\nfg %1 ——将后台1号进程移动到前台来运行\nbg %2 —— 让后台的2号进程开始运行\nctrl+z ——暂停当前的进程放到后台，但不运行\n\n[root@localhost ~]# vim &\n[2] 6987\n  \n[root@localhost ~]# vi /etc/resolv.conf\n[3]+ Stopped vi /etc/resolv.conf\n\n[root@localhost ~]# jobs -l\n[1] 6891 Stopped nice -n 10 vim\n[2]- 6987 Stopped (tty output) vim\n[3]+ 6989 Stopped vi /etc/resolv.conf\n\n[root@localhost ~]# bg %2\n[2]- vim &\n\n[root@localhost ~]# fg %3\n```","tags":["linux","tool"],"categories":["linux"]},{"title":"Linux包管理yum仓库配置","url":"//2015/11/09/linux-yum-repo/","content":"Rhel中使用传统的rpm包安装会存在依存关系问题，虽然rpm为我们列出了依存关系，但是并没有为我们自动解决这个问题，依旧需要我们手动的去安装依存包。这样使得软件的安装非常麻烦，由此Linux中出现了yum工具，yum通过预读取软件的依存关系然后生存缓存，以后我们通过yum去安装软件是yum会根据缓存的关系表自动去识别rpm包的依存关系，为我们自动处理依存关系，使得软件的安装的工作变得简单。yum安装软件需要一个软件仓，yum会读取/etc/yum.repos.d/xxxxxxx.repo的仓库配置文件去仓库中提取软件来安装。我们可以通过修改仓配置文件，来修改仓库位置，使用本地或者速度快的仓，加速软件安装。\n\n配置文件内容：\n```\n[base]\nname=CentOS-$releasever - Base - mirrors.aliyun.com\nbaseurl=http://mirrors.aliyun.com/centos/$releasever/os/$basearch/\nenabled=1\ngpgcheck=1\ngpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-6\n```\n以上是阿里云的开源镜像站的yum源配置。\n\n\n配置文件中：\n[ ]内的是仓库的名字\nname是仓库的描述也可以说是名字\nbaseurl 仓库的位置\nenabled 是否启用这个仓库，1为起用，0为禁用\ngpgcheck 是否检查GPG签名（用来验证要安装的包是不是REDHAT官方的）\ngpgkey gpgkey的存放地址我们开启gpgcheck=1才使用这项功能\n\n由以上的配置文件我们可以看出只要修改baseurl我们就可以修改仓的位置，因此我们也可以使用本地的仓库。只要配置baseurl指向相对应的仓即可。如下\n```\n[xiemx] \nname=xiemx \nbaseurl=file:///yumdatebase \nenabled=1 \ngpgcheck=0\n```\n在运行下yum clear all清空下缓存 yum  makecache制作新的缓存。以上的baseurl我们是将系统安装盘的内容复制到本机的/yumdatebase下，由于是光盘的源系统由此也不需要校验程序的安全性所以关闭gpgcheck。也可以使用其它协议来链接仓库如ftp://xxxxxxxxx。\n\n\n","tags":["linux","yum"],"categories":["linux"]},{"title":"Linux目录结构","url":"//2015/11/07/linux-tree/","content":"```\n[root@localhost ~]# ls /\nbin    dev       home        lost+found      mnt       proc         sbin         srv         tmp        var\nboot          etc      lib        media       opt       root       selinux       sys       usr\n\n/bin bin是Binary的缩写。这个目录存放着最经常使用的命令。\n/boot 这里存放的是启动Linux时使用的一些核心文件，包括一些连接文件以及镜像文件。\n/dev dev是Device(设备)的缩写。该目录下存放的是Linux的外部设备，在Linux中访问设备的方式和访问文件的方式是相同的。\n/etc 这个目录用来存放所有的系统管理所需要的配置文件和子目录。\n/home 用户的主目录，在Linux中，每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的。\n/lib 这个目录里存放着系统最基本的动态连接共享库，其作用类似于Windows里的DLL文件。几乎所有的应用程序都需要用到这些共享库。\n/lost+found ext文件系统创建，这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。\n/media Linux系统会自动识别一些设备，例如U盘、光驱等等，当识别后，Linux会把识别的设备挂载到这个目录下。\n/mnt 系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将光驱挂载在/mnt/上，然后进入该目录就可以查看光驱里的内容了。\n/opt 这是给主机额外安装软件所摆放的目录。比如你安装一个ORACLE数据库则就可以放到这个目录下。默认是空的。\n/proc 这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息。\n/root 该目录为系统管理员，也称作超级权限者的用户主目录。\n/sbin 系统管理员使用的系统管理程序。\n/selinux 这个目录是Redhat/CentOS所特有的目录，Selinux是一个安全机制，类似于windows的防火墙，但是这套机制比较复杂，这个目录就是存放selinux相关的文件的。\n/srv 该目录存放一些服务启动之后需要提取的数据。\n/sys 这是linux2.6内核的一个很大的变化。该目录下安装了2.6内核中新出现的一个文件系统 sysfs ，sysfs文件系统集成了下面3种文件系统的信息：针对进程信息的proc文件系统、针对设备的devfs文件系统以及针对伪终端的devpts文件系统。该文件系统是内核设备树的一个直观反映。当一个内核对象被创建的时候，对应的文件和目录也在内核对象子系统种被创建。\n/tmp 这个目录是用来存放一些临时文件的。\n/usr 这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似与windows下的program files目录。\n/usr/bin 系统用户使用的应用程序。\n/usr/sbin 超级用户使用的比较高级的管理程序和系统守护程序。\n/usr/src 内核源代码默认的放置目录。\n/var 经常被修改的目录放在这个目录下。包括各种日志文件。\n```","tags":["linux"],"categories":["linux"]},{"title":"Linux标准输入输出重定向","url":"//2015/11/07/linux-stdin-stdout-stderr/","content":"一个命令执行前，先会准备好所有输入输出，默认分别绑定（stdin,stdout,stderr)，如果这个时候出现错误，命令将终止，不会执行。执行成功会将结果输出到屏幕上，执行错误时也会将错误信息输出到屏幕。这些默认的输出，输入都是linux系统内定的，我们在使用过程中，有时候并不希望执行结果输出到屏幕。我想输出到文件或其它设备。这个时候我们就需要进行输出重定向了。这些标准输入输出对应的文件如下：\n\n```\n[root@localhost ~]# ll /dev/stdin /dev/stdout /dev/stderr\nlrwxrwxrwx. 1 root root 15 Nov 8 01:03 /dev/stderr -> /proc/self/fd/2 \nlrwxrwxrwx. 1 root root 15 Nov 8 01:03 /dev/stdin -> /proc/self/fd/0 \nlrwxrwxrwx. 1 root root 15 Nov 8 01:03 /dev/stdout -> /proc/self/fd/1\n```\n\n由这些管道文件可以看到linux shell下常用输入输出操作符是：\n\n1.  标准输入   (stdin) ：代码为 0 ，使用 < \n2.  标准输出   (stdout)：代码为 1 ，使用 > 或 >> \n3.  标准错误输出(stderr)：代码为 2 ，使用 2> 或 2>> \n\n当我们执行命令时我们不想要程序将结果输出到屏幕时我们就可以调用`>` `>>`重定向操作符来重定向数据流的输出位置\n```shell\nls -l > testfile1  ##查看当前目录下的文件属性，但不输出到屏幕，输出到testfile1的文件中。\n\necho 123qwe321|passwd --stdin testuse\n###输出字符串123qwe321，并将字符串通过标准输入设备传递给passwd程序去修改testuser用户的密码。\n###--stdin代表标准输入设备，也可以用/dev/stdin来代替。(stdin前是双横杠)\n\n重定向时要注意\n## >重定向输出到文件中时会清空文件中的内容\n## >>不会清空文件内容会在文件最后追加输出内容\n###由于>会清空文件在写入输出内容，因此常用来清空文件\n\necho \"\" > /tmp/testfile1   ##清空testfile1文件。\n```","tags":["linux"],"categories":["linux"]},{"title":"Linux特殊权限和acl访问控制列表","url":"//2015/11/05/linux-premissions-and-acl/","content":"### 特殊权限\n\nlinux中除了常见的读（r）、写（w）、执行（x）权限以外，还有3个特殊的权限，分别是setuid、setgid和stick bit。\n\n#### setuid\n只能设置文件，让普通用户拥有可以执行“**只有root权限才能执行**”的特殊权限，一般设置命令文件此属性。\n设置方法：\n```\nchmod u+s /tmp/testfile\nchmod u-s /tmp/testfile\n```\n如：\n```\n-rwsr-xr-x         1       root          root           22984          2015-11-04       /usr/bin/passwd\n```\n#### setgid\n可设置文件和目录，当目录设置此属性时在目录下创建的文件，默认所属组都会变为该目录的所属组。\n\n设置方法:\n```\nchmod g+s /tmp/test/\nchmod g-s /tmp/test/\n```\n如:\n```  \ndrwxr-sr-x         1       testuser         testadmin           22984          2015-11-04       /tmp/test/\n-rw-r--r--           1       root         testadmin          22984          2015-11-04       /tmp/test/file1  ——文件所属组自动继承test的所属组\n```\n#### sticky bit\n在目录下创建的文件，只有文件的拥有者和此目录的拥有者有编辑、删除权限\n\n设置方法：\n```\nchmod +t /tmp\nchmod -t /tmp\n```\n以上权限也可以通过数字的方式来设置特殊权限 setuid=4，setgid=2，sticky bit=1\n```\nchmod 4777 /test/file1 ——setuid\nchmod 2777 /test/file2 ——setgid\nchmod 1777 /test/      ——sticky bit\n```\n\n### attr权限（隐藏权限）\n\n用来设置特殊用户例如：root之类的权限。\n\n\n```\n设置方法：\nchattr  参数   文件\n\n+ ：在原有参数设定基础上，追加参数。\n- ：在原有参数设定基础上，移除参数。\n= ：更新为指定参数设定。\nA：文件或目录的 atime (access time)不可被修改(modified), 避免磁盘I/O读写占用资源。\nS：硬盘I/O同步选项，功能类似sync。\na：即append，设定该参数后，只能向文件中添加数据，而不能删除，多用于服务器日志文件安全，只有root才能设定这个属性。\nc：即compresse，设定文件是否经压缩后再存储。读取时需要经过自动解压操作。\nd：即no dump，设定文件不能成为dump程序的备份目标。\ni：设定文件不能被删除、改名、设定链接关系，同时不能写入或新增内容。i参数对于文件 系统的安全设置有很大帮助。\nj：即journal，设定此参数使得当通过mount参数：data=ordered 或者 data=writeback 挂 载的文件系统，文件在写入时会先被记录(在journal中)。如果filesystem被设定参数为 data=journal，则该参数自动失效。\ns：保密性地删除文件或目录，即文件的block、inode空间被全部清空收回。\nu：与s相反，当设定为u时，数据内容其实还存在磁盘中，可以用于undeletion。\n各参数选项中常用到的是a和i。a选项强制只可添加不可删除，多用于日志系统的安全设定。而i是更为严格的安全设定，只有superuser (root) 或具有CAP_LINUX_IMMUTABLE处理能力（标识）的进程能够施加该选项。\n```\n例：\n```\nchattr +i /tmp/testfile1  ——增加i权限\nchattr +a /tmp/testfile1 ——增加a权限\n\n查看文件的attr权限可使用lsattr命令\nlsattr /tmp/testfile1   ——查看文件隐藏权限\n```\n#### ACL(访问控制列表)\n\nLinux中`-rwxrwxrwx.`这种权限方式只能规范出三种类型的权限，在系统的使用中并不能完全的满足管理者对于用户权限的指定，因此Linux推出了ACL访问控制列表这种权限管理方法，此方法可以针对不同的用户和组来分配不同的权限配置不局限于原始的UGO权限类型。\n\nACL权限的种类\n* R——读权限\n* W——写权限\n* X——执行权限\n\nacl访问控制列表对于权限的规范和UGO类型的权限是一样的，也是通过RWX来组合控制用户权限。但设置方法不同具体如下\n\n设置方法：\n```\nsetfacl   参数    文件\n-m   设置acl规则\n-x   删除用户的acl规则\n-b    清空acl规则\n```\n例：\n```\nsetfacl  -m u:test:rwx /tmp/testfile1 ——赋予用户test对于文件testfile1有RWX权限\nsetfacl  -x u:test /tmp/testfile1     ——删除用户test对于文件testfile1的acl条目\nsetfacl  -b /tmp/testfile1            ——清空testfile1文件的acl条目\n```\n查看文件acl是否开启可以通过ls -l  查看文件权限字段 最后一位是否为+号。如：-rwxrwxrwx+则此文件开启了acl，也可通过命令getfacl  /tmp/testfile1来查看文件具体acl条目信息。","tags":["linux","acl"],"categories":["linux"]},{"title":"Linux链接文件（ln）","url":"//2015/11/05/linux-ln/","content":"Linux系统中,内核为每一个文件分配一个`inode` ，文件属性保存在inode里，在访问文件时，文件系统通过查找到文件的inode中关于文件的存放block，进而去相对应的block中读取文件内容。而链接是一种在共享文件和访问它的用户的若干目录项之间建立联系的一种方法，在系统中有相同的一份文件A和B两个用户都需要这个文件，传统模式下我们会直接复制文件分给2个用户，这样虽然能实现目的。但是两份文件占用了我们双倍的block和inode资源。如果这个文件非常大的话将造成资源的极大浪费。此时Linux引入链接的概念，我们可以通过链接的方式去将文件共享给两个用户。\n\n### 两种链接：\n* 硬链接(Hard Link)\n* 和软链接(Soft Link)，软链接又称为符号链接（Symbolic link）\n\n#### 硬链接\n\n硬链接文件可以理解为一个指针，指向文件的inode，系统不重新分配inode。硬链接文件中记录的是源文件的inode信息，所以硬链接文件的属性和源文件的属性相同只是在硬链接连接数上+1，硬链接文件。\n\n  创建方法：\n```\n  ln    源文件    目标文件\n```\n  例子：\n```\n  ln   /tmp/testdir/testdir2/testfile1   /tmp/testfile1\n  ls  -i   /tmp/testdir/testdir2/testfile1   /tmp/testfile1\n  1491138      -rw-r–r–       1         root     root     48        11-14 14:10       /tmp/testdir/testdir2/testfile1\n  1491138      -rw-r–r–       1         root     root     48        11-14 14:29       /tmp/testfile1\n```\n\n  注意事项：\n  1.硬链接文件无法跨文件系统，因为不同的文件系统inode信息记录的信息不同。\n  2.只能对文件设置硬链接\n  3.删除硬链接文件或源文件，只会使硬链接计数器-1，不会删除文件内容。直到硬链接计数器为1时，在删除文件，此时则会清除文件。\n  4.修改一个文件，其他文件同时被修改\n\n#### 软连接\n\n软连接通过存储源文件的路径来定位源文件。所以软连接文件的大小为路径的长度。软链接文件属性同源文件属性不同。\n\n  创建方法:\n```\n  ln    -s      源文件  目标文件\n```\n  例子：\n```\n  ln  -s    /tmp/testfile    /testfilesoft\n  ls   -i  /tmp/testfile   /testfilesoft\n  1491138      -rw-r–r–       1         root     root     48        11-14 14:17        /tmp/testfile 1491140      lrwxrwxrwx  1         root     root     13          11-14 14:24        /testfilesoft -> /tmp/testfile\n```\n  查看以上文件的输出结果，源文件和链接文件的属性不同。链接文件的大小为13字节，这13字节等于软连接的路径“/tmp/testfile\"的长度。\n\n  注意事项：\n\n  1.软连接文件记录的是文件路径因此可以跨文件系统创建\n  2.可针对目录设置软连接\n  3.删除软连接文件不影响其它链接文件的访问，删除源文件则所有软连接文件都无法访问。\n  4.修改任意文件，所有文件都会变动\n\nLinux系统中软硬链接的方式可使用的环境不同，其中软连接使用的最多","tags":["command","linux"],"categories":["linux"]},{"title":"Linux用户和组","url":"//2015/11/05/linux-user-and-group/","content":"### 用户和组的配置文件\n在linux中，用户帐号，用户密码，用户组信息和用户组密码均是存放在不同的配置文件中的。\n* `/etc/passwd`存放用户配置信息\n* `/etc/shadow`存放用户密码和密码策略\n* `/etc/group`存放组配置信息\n* `/etc/gshadow`存放组配置信息\n\n\n#### /etc/passwd文件中每行存储一个用户信息以：分割分7段具体含义如下\n* **用户帐号:密码占位符:用户ID:用户组ID:描述:用户主目录:用户所使用的shell**\n\n```shell\n[mingxu.xie@cn-aux-cc ~]$ cat /etc/passwd\nroot:x:0:0:root:/root:/bin/bash\nbin:x:1:1:bin:/bin:/sbin/nologin\ndaemon:x:2:2:daemon:/sbin:/sbin/nologin\nadm:x:3:4:adm:/var/adm:/sbin/nologin\nlp:x:4:7:lp:/var/spool/lpd:/sbin/nologin\n```\n#### /etc/shadow文件对应passwd文件分9段，具体信息如下\n* **用户名:密码:最近修改时间:最短有效时间:最长有效时间:过期提醒日期:过期后宽限时间:账户失效时间:保留字段**\n```shell\n[mingxu.xie@cn-aux-cc ~]$ sudo cat /etc/shadow\nroot:*LOCK*:14600::::::\nbin:*:16323:0:99999:7:::\ndaemon:*:16323:0:99999:7:::\nadm:*:16323:0:99999:7:::\nlp:*:16323:0:99999:7:::\ntest:$6$getqHfvX$xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx:18185:0:99999:7:::\n```\n\n#### /etc/group文件保存了所有组的信息。文件中每一行表示一个组由3个冒号分隔成4段，具体信息如下\n\n* **组名:密码占位符:组ID:组成员**\n```shell\n[mingxu.xie@cn-aux-cc ~]$ sudo cat /etc/group\nroot:x:0:\nbin:x:1:bin,daemon\ndaemon:x:2:bin,daemon\nsys:x:3:bin,adm\nadm:x:4:adm,daemon\ntty:x:5:\ndisk:x:6:\nlp:x:7:daemon\nmem:x:8:\nkmem:x:9:\nwheel:x:10:ec2-user,test\n```\n\n#### /etc/gshadow文件对应group文件，分4段，具体信息如下\n\n* **组名:口令:组管理者:组内用户列表**\n```shell\n[mingxu.xie@cn-aux-cc ~]$ sudo cat /etc/gshadow\nroot:::\nbin:::bin,daemon\ndaemon:::bin,daemon\nsys:::bin,adm\nadm:::adm,daemon\n```\n**修改以上文件可以完成对用户和组属性和配置的改变，/sbin/nologin的shell可以使用户不可登陆等等。也可以通过命令来修改用户属性，但实质也是修改此配置文件中的参数。**\n\n### 用户和组的操作\n```shell\n1、添加新用户\n  useradd 选项 用户名 \n\n  其中各选项含义如下： \n  -c comment 指定一段注释性描述。\n  -d 目录 指定用户主目录，如果此目录不存在，则同时使用-m选项，能创建主目录。 \n  -g 指定用户所属的用户组。 \n  -G 指定用户所属的附加组。 \n  -s  指定用户的登录Shell。 \n  -u 指定用户的用户号，如果同时有-o选项，则能重复使用其他用户的标识号。\n\nuseradd  -G bin -g root -s /sbin/noloing -u 101   testuser\n\n2、添加组\n  用法：groupadd    选项    用户名 \n  其中各选项含义如下： \n  -g 指定用户组ID\n\ngroupadd  -g 40000  testgrp\n\n3、修改用户属性和组属性\n  groupmod  ——修改组属性\n  usermod     ——修改用户属性\n  参数、语法和创建时使用的相同。\n\n4、删除用户和组\n  userdel testuser   ——默认删除testuser用户的配置但不删除家目录和邮件需手工删除\n  userdel -r testuser  ——删除用户同时删除家目录文件和邮件\n  groupdel testgrp  ——删除组\n\n5、修改用户密码策略\n\n[mingxu.xie@cn-aux-cc ~]$ chage --help\nUsage: chage [options] [LOGIN]\n\nOptions:\n  -d, --lastday LAST_DAY        set date of last password change to LAST_DAY\n  -E, --expiredate EXPIRE_DATE  set account expiration date to EXPIRE_DATE\n  -h, --help                    display this help message and exit\n  -I, --inactive INACTIVE       set password inactive after expiration\n                                to INACTIVE\n  -l, --list                    show account aging information\n  -m, --mindays MIN_DAYS        set minimum number of days before password\n                                change to MIN_DAYS\n  -M, --maxdays MAX_DAYS        set maximim number of days before password\n                                change to MAX_DAYS\n  -W, --warndays WARN_DAYS      set expiration warning days to WARN_DAYS\n系统添加用户时如果没有规定用户的详细属性，则系统默认参照/etc/login.defs  和/etc/default/useradd来配置用户的属性！修改此处文档可以设置默认用户添加时的属性。\n\n```","tags":["linux","commands"],"categories":["linux"]},{"title":"Linux查找","url":"//2015/10/16/linux-find-commands/","content":"1. find\n\n功能强大，用法自行google\n\n2. locate\n\nlocate命令其实是\"find -name\"的另一种写法，但是要比后者快得多，原因在于它不搜索具体目录，而是搜索一个数据库（/var/lib/mlocate/mlocate.db），这个数据库中含 有本地所有文件信息。Linux系统自动创建这个数据库，并且每天自动更新一次，所以使用locate命令查不到最新变动过的文件。为了避免这种情况，可 以在使用locate之前，先使用updatedb命令，手动更新数据库。\n\n例：\n\nlocate /etc/sh* ——搜索etc目录下所有以sh开头的文件。\n\nlocate -i ~/m*——搜索用户主目录下，所有以m开头的文件，并且忽略大小写。\n\n3. whereis\n\nwhereis命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息。\n\n4. which\n\nwhich命令的作用是，在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。\n","tags":["command","linux"],"categories":["linux"]},{"title":"PHP 调用date()函数时区报错","url":"//2015/10/11/php-data-timezone/","content":"### 现象：\n```\n  “PHP Warning: date() [function.date]: It is not safe to rely on the system’s timezone settings.\n  You are *required* to use the date.timezone setting or the date_default_timezone_set() function.\n  In case you used any of those methods and you are still getting this warning, you most likely\n  misspelled the timezone identifier. We selected ‘UTC’ for ’8.0/no DST’ instead in”\n```\nphp 有些版本默认的时区为格林威治标准时间，我们需要调整时区为+0800 “Asia/shanghai”\n### 解决\n以下是三种方法(任选一种都行)：\n```\n1. 在页头使用date_default_timezone_set()设置 date_default_timezone_set('PRC');\n2. 在页头使用 ini_set('date.timezone','Asia/Shanghai');\n3. 修改php.ini。打开php.ini查找date.timezone 去掉前面的分号修改成为：date.timezone = PRC\n```","tags":["debug","php"],"categories":["php"]},{"title":"window系统文件或目录无法删除解决方法","url":"//2015/10/08/window-force-delete-file/","content":"1. 新建.BAT批处理文件输入如下命令，然后将要删除的文件拖放到批处理文件图标上即可删除。\n\n```powershell\nDEL /F /A /Q\nRD /S /Q\n```\n\n2. 可利用软件来解锁占用文件的进程推荐unlock\n\n","tags":["windows","debug"],"categories":["windows"]},{"title":"session_start()函数报错","url":"//2015/09/30/session_start-error/","content":"### 现象\n\nWarning: session_start() [function.session-start]: Cannot send session cache limiter - headers already sent\n\n### 解决\n\n(PHP 4, PHP 5)\n\nsession_start — 启动新会话或者重用现有会话\n\n在调用此函数是不能向浏览器输出内容，如出现上面报错可以将函数放到 ` <？php `最上方顶句如下\n\n```php\n<?php\nsession_start();\ninclude xxxxxxxx;\n?>\n```","tags":["debug","php"],"categories":["php","debug"]},{"title":"2003系统控制面板打不开解决办法","url":"//2015/09/29/windows-cant-open-contral/","content":"```\n运行regedit打开注册表\n把 HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\Nls\\Locale下的两个项修改为\n\"(Default)\"=\"00000409\"\n\"00000804\"=\"1\"\n```","tags":["windows","debug"],"categories":["windows"]},{"title":"应用程序池0x80大量报错解决办法","url":"//2015/09/29/apppool-0x80-error/","content":"为应用程序池提供服务的进程意外终止。进程ID是。进程退出代码是'0x80':\n```\nHKEY_LOCAL_MACHINE\\System\\CurrentControlSet\\Services\\W3SVC\\Parameters键下新建一个DWORD项，名字为：UseSharedWPDesktop值为1 重启IIS\n\n```\n","tags":["iis","windows","debug"],"categories":["iis","windows"]},{"title":"网站支持.apk文件下载的设置方法","url":"//2015/09/29/website-download-apk/","content":"### iis支持.apk文件下载的设置\n\nIIS服务器不能下载.apk文件的原因：iis的默认MIME类型中没有.apk文件，所以无法下载。\n\nIIS服务器不能下载.apk文件的解决办法：既然.apk无法下载是因为没有MIME，那么添加一个MIME类型就可以了。\n\nIIS服务器不能下载.apk文件的解决步骤：\n```\n打开IIS服务管理器，找到服务器，右键-属性，打开IIS服务属性；\n单击MIME类型下的“MIME类型”按钮，打开MIME类型设置窗口；\n单击“新建”，建立新的MIME类型；\n扩展名中填写“.apk”,\nMIME类型中填写apk的MIME类型`application/vnd.android.package-archive`\n\n```\n重启IIS，使设置生效。\n\n### apache支持.apk文件下载的设置\n在Apache安装目录下的`mime.types`加上以下一行语句\n```\napplication/vnd.android.package-archive     apk;\n```\n重启apache即可\n\n### nginx支持.apk文件下载的设置\n\napk 和 .ipa分别是android应用和ios应用的扩展名。\n\n如果在浏览器下载这些文件为后缀的文件时，会自动重命名为zip文件。\n\n当然可以下载后手动修改后缀，依然可以安装。\n\n如果想下载后缀直接就是apk ipa的，可以修改 `/usr/local/nginx/conf`目录下的mime.types\n\n```\napplication/vnd.android.package-archive apk;\napplication/iphone          pxl ipa;\n```\n增加上述配置，重启nginx生效\n","tags":["iis","debug","nginx"],"categories":["iis","nginx"]},{"title":"PHPCMS V9 “密码重试次数太多，请过-xxx分钟后重新登录！”的解决办法","url":"//2015/09/29/phpcms-v9-password-retry-too-more/","content":"找到文件 /phpcms/modules/admin/index.php\n\n将如下代码注释掉：\n```php\nif($rtime['times'] >= $maxloginfailedtimes) {\n$minute = 60-floor((SYS_TIME-$rtime['logintime'])/60);\nshowmessage(L('wait_1_hour',array('minute'=>$minute)));\n}\n```\n注意哦，一共是4行。","tags":["debug","php","CMS"],"categories":["php"]},{"title":"2003系统8G内存条只能识别3G解决办法","url":"//2015/09/29/windows-server-memory-limit/","content":"#### 现象\n32位的2003系统的机器，8G内存条只能识别3G，这种情况需要进系统进行修改。具体修改如下：\nwin2003 32位企业版理论上能支持最大32G内存，但有些机器系统只认3.25G\n#### 解决办法：\n```\nC：\\boot.ini 修改里面的命令（\\boot.ini有可能被隐藏）\ndefault=multi(0)disk(0)rdisk(0)partition(2)\\WINDOWS\n[operating systems]\n(0)disk(0)rdisk(0)partition(2)\\WINDOWS=Windows Server 2003, Enterprise&quot; /fastdetect /PAE\n```","tags":["windows","debug"],"categories":["windows"]},{"title":"访问网站要求输入密码","url":"//2015/08/24/iis-auth/","content":"访问网站要求输入密码：\n![image-20191021135643975](/images/image-20191021135643975.png)\n\n### 方法1\n\n1. Internet—域名属性—目录安全性—编辑身份认证和访问控制——修改密码\n![image-20191021135659882](/images/image-20191021135659882.png)\n\n2. 网站应用程序池—属性—标识——修改密码(密码和上面相同)\n![image-20191021135714997](/images/image-20191021135714997.png)\n\n### 方法2：\n\n1. 删除权限中不统一的用户\n![image-20191021135724279](/images/image-20191021135724279.png)\n\n","tags":["iis","windows"],"categories":["windows","iis"]},{"title":"phpmyadmin上传文件大小修改限制","url":"//2015/08/24/phpmyadmin-upload-limit-size/","content":"修改phpmyadmin上传文件大小限制主要分修改php.ini配置文件和phpmyadmin配置文件两个步骤。\n\n#### 第一步：修改php.ini配置文件中文件上传大小配置**\n\n此步骤与一般的php.ini 配置文件上传功能方法一致，需要修改php.ini配置文件中`upload_max_filesize`和`post_max_size`两个选项值\n\n#### 第二步：修改php执行时间及内存限制实现phpmyadmin上传大文件功能\n\n如果想要phpmyadmin上传大文件，还需修改php.ini配置文件中的`max_execution_time`（php页面执行最大时间）、 `max_input_time`（php页面接受数据最大时间）、`memory_limit`（php页面占用的最大内存）三个配置选项，这是因为 phpmyadmin上传大文件时，php页面的执行时间、内存占用也势必变得更长更大，其需要php运行环境的配合，光修改上传文件大小限制是不够的。\n\n#### 第三步：修改phpmyadmin配置文件\n\n在完成php.ini的相关配置后，还需要修改phpmyadmin配置。\n\n1、修改phpmyadmin config配置文件中的`$cfg［'ExecTimeLimit'］`配置选项，默认值是300，需要修改为0，即没有时间限制。\n\n2、修改phpmyadmin安装根目录下的import页面中的`$memory_limit`\n\n说明：首选读取php.ini配置文件中的内存配置选项memory_limit，如果为空则默认内存大小限制为2M，如果没有限制则内存大小限制为10M，你可以结合你php.ini配置文件中的相关信息修改这段代码。\n\n至此，经过修改php.ini配置文件中的文件上传配置选项以及phpmyadmin配置文件后，即可解决phpmyadmin上传文件大小限制问题，从而实现phpmyadmin上传大文件功能。\n","tags":["mysql","database","php","phpmyadmin"],"categories":["mysql"]},{"title":"MySQL数据库mysqlcheck的使用方法详解","url":"//2015/08/24/mysqlcheck/","content":"mysqlcheck，是mysql自带的可以检查和修复`MyISAM`表，并且它还可以优化和分析表，mysqlcheck的功能类似myisamchk，但其工作不同。主要差别是当mysqld服务器在运行时必须使用mysqlcheck，而myisamchk应用于服务器没有运行时。myisamchk修复失败是不可逆的。\n```shell\n1.如果需要检查并修复所有的数据库的数据表\nmysqlcheck -A -o -r -p\nEnter password:\ndatabase1 OK\ndatabase2 OK\n\n2.如果需要修复指定的数据库用\nmysqlcheck -A -o -r database1 -p\ndatabase1 OK\n```","tags":["mysql","tool","mysqlcheck"],"categories":["mysql"]},{"title":"URLRewriter设置Config和IIS配置做伪静态","url":"//2015/08/21/iis-set-urlrewrite/","content":"\n一、首先检查config文件里面是否包含这两个节点\n\n```xml\n <configSections>\n\n   <section name=\"RewriterConfig\" requirePermission=\"false\" type=\"URLRewriter.Config.RewriterConfigSerializerSectionHandler, URLRewriter\"/>\n\n </configSections>\n\n <system.web>\n\n   <httpModules>\n     <add type=\"URLRewriter.ModuleRewriter, URLRewriter\" name=\"ModuleRewriter\"/>\n\n   </httpModules>\n\n </system.web>\n\n```\n\n二、配置IIS。\n\n 1.打开IIS，选择网站。右键，点击属性\n\n 2.选择主目录，点击配置按钮\n\n 3.在弹出的应用程序配置中，点击添加\n\n 4.在弹出的添加/编辑应用程序扩展名映射中，填写可执行文件\n\n​\t点击浏览，选择aspnet_isapi.dll这个DLL，它一般在：C:\\WINDOWS\\Microsoft.NET\\Framework\\v2.0.50727文件夹中(也就是你的系统盘中）\n\n 5.在扩展名中填写.html\n\n 6.在动作里选择限制为，他的框就可以填写了！请填写：GET,POST\n\n 7.不要勾选把确认文件是否存在，然后点击确定\n\n![image-20190917165314461](/images/image-20190917165314461.png)","tags":["iis","windows"],"categories":["iis","windows"]},{"title":"MSSQL数据库 阻止保存要求重新创建表的更改","url":"//2015/08/21/mssql-deny-save/","content":"\nMSSQL2008以上\n\n工具菜单----选项----Designers(设计器)----阻止保存要求重新创建表的更改 取消勾选即可\n\n[![1](/images/1.png)](1.png)\n\nMSSql2005下 添加excute权限\n\n[![clipboard](/images/clipboard.png)](clipboard.png)\n\n","tags":["debug","database","mssql"],"categories":["mssql"]},{"title":"常见MIME类型设置方法","url":"//2015/08/21/mime-type/","content":"### 常见MIME类型和设置方法\n当前列举了常用的MIME类型，查询详细：[MIME Type](https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Basics_of_HTTP/MIME_types/Complete_list_of_MIME_types)\n\n#### MIME类型\n\n| 扩展名 | 文档类型 | MIME 类型 |\n| :----- | :------ | ----: |\n| .aac | AAC audio | audio/aac |\n| .abw | AbiWord document | application/x-abiword |\n| .arc | Archive document (multiple files embedded) | application/x-freearc |\n| .avi | AVI: Audio Video Interleave | video/x-msvideo |\n| .azw | Amazon Kindle eBook format | application/vnd.amazon.ebook |\n| .bin | Any kind of binary data | application/octet-stream |\n| .bmp | Windows OS/2 Bitmap Graphics | image/bmp |\n| .bz | BZip archive | application/x-bzip |\n| .bz2 | BZip2 archive | application/x-bzip2 |\n| .csh | C-Shell script | application/x-csh |\n| .css | Cascading Style Sheets (CSS) | text/css |\n| .csv | Comma-separated values (CSV) | text/csv |\n| .doc | Microsoft Word | application/msword |\n| .epub | Electronic publication (EPUB) | application/epub+zip |\n| .gif | Graphics Interchange Format (GIF) | image/gif |\n| .html | HyperText Markup Language (HTML) | text/html |\n| .ico | Icon format | image/vnd.microsoft.icon |\n| .ics | iCalendar format | text/calendar |\n| .jar | Java Archive (JAR) | application/java-archive |\n| .jpeg | JPEG images | image/jpeg |\n| .js | JavaScript | text/javascript |\n| .json | JSON format | application/json |\n| .mjs | JavaScript module | text/javascript |\n| .mp3 | MP3 audio | audio/mpeg |\n| .mpeg | MPEG Video | video/mpeg |\n| .oga | OGG audio | audio/ogg |\n| .ogv | OGG video | video/ogg |\n| .ogx | OGG | application/ogg |\n| .otf | OpenType font | font/otf |\n| .png | Portable Network Graphics | image/png |\n| .pdf | Adobe Portable Document Format (PDF) | application/pdf |\n| .rar | RAR archive | application/x-rar-compressed |\n| .rtf | Rich Text Format (RTF) | application/rtf |\n| .sh | Bourne shell script | application/x-sh |\n| .svg | Scalable Vector Graphics (SVG) | image/svg+xml |\n| .tar | Tape Archive (TAR) | application/x-tar |\n| .tiff | Tagged Image File Format (TIFF) | image/tiff |\n| .ttf | TrueType Font | font/ttf |\n| .txt | Text, (generally ASCII or ISO 8859-n) | text/plain |\n| .vsd | Microsoft Visio | application/vnd.visio |\n| .wav | Waveform Audio Format | audio/wav |\n| .woff | Web Open Font Format (WOFF) | font/woff |\n| .woff2 | Web Open Font Format (WOFF) | font/woff2 |\n| .xhtml | XHTML | application/xhtml+xml |\n| .xls | Microsoft Excel | application/vnd.ms-excel |\n| .xml | XML | application/xml|\n| .zip | ZIP archive | application/zip |\n| .3gp | 3GPP audio/video container | video/3gpp |\n| .3g2 | 3GPP2 audio/video container | video/3gpp2 |\n| .7z | 7-zip archive | application/x-7z-compressed |\n\n#### 设置方法\n```markdown\n# IIS\n默认网站属性-->http 头-->MIME映射-->文件类型-->新类型\n\n# nginx\nconf/mime.types\n```","tags":["http","mime-type"],"categories":["http","mime-type"]}]